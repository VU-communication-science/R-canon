---
title: "Using the pipe notation"
subtitle: "How to make your code more readable"
order: 6
---


# Working with Pipes

In the previous sections you learned several data management techniques.
In practise, you will often apply multiple of these techniques in succession to clean and prepare data.
For example, the following code reads a csv file, filters rows, computes a new variable, selects columns, and sorts the data:

```{r}
library(tidyverse)

practice_data = read_csv("https://tinyurl.com/R-practice-data")
practice_data = select(practice_data, age, experiment_group, trust_t1)
practice_data = filter(practice_data, age >= 18)
practice_data = arrange(practice_data, trust_t1)
```

This code works fine, but it is a bit cumbersome to read.
For each step, we need to provide `practice_data` as input, and also assign the output to `practice_data` to overwrite it.



If syou look at the code above, you notice that the result of each function is stored as an object, 
and that this object is used as the first argument for the next function.
Moreover, we don't really care about this temporary object, we only care about the final summary table. 

This is a very common usage pattern, and it can be seen as a *pipeline* of functions, where the output of each function is the input for the next function.
Because this is so common, tidyverse offers a more convenient way of writing the code above using the pipeline operator `%>%`.
In sort, whenever you write `f(a, x)` you can replace it by `a %>% f(x)`. If you then want to use the output of `f(a, x)` for a second function,
you can just add it to the pipe: `a %>% f(x) %>% f2(y)` is equivalent to `f2(f(a,x), y)`, or more readable, `b=f(a,x); f2(b, y)`

Put simply, pipes take the output of a function, and directly use that output as the input for the `.data` argument in the next function. As you have seen, all the `dplyr` functions that we discussed have in common that the first argument is a *tibble*, and all functions return a *tibble*. This is intentional, and allows us to pipe all the functions together. 

This seems a bit abstract, but consider the code below, which is a collection of statements from above:

```{r}
# d <- read_csv(url)
# d <- filter(d, Question == 'age-21')
# d <- mutate(d, party_diff = abs(`Republican Support` - `Democratic Support`))
# d <- select(d, Question, Pollster, party_diff)
# arrange(d, -party_diff)
```

To recap, this reads the csv, filters by question, computes the difference, drops other variables, and sorts.
Since the output of each function is the input of the next, we can also write this as a single pipeline:

```{r}
# read_csv(url) %>% 
#   filter(Question == 'age-21') %>% 
#   mutate(party_diff = abs(`Republican Support` - `Democratic Support`)) %>%
#   select(Question, Pollster, party_diff) %>% 
#   arrange(-party_diff)
```

The nice thing about pipes is that it makes it really clear what you are doing. Also, it doesn't require making many intermediate objects (such as `ds`). If applied right, piping allows you to make nicely contained pieces of code to perform specific parts of your analysis from raw input straight to results, including statistical modeling or visualization. It usually makes sense to have each "step" in the pipeline in its own line. This way, we can easily read the code line by line

Of course, you probably don't want to replace your whole script with a single pipe, and often it is nice to store intermediate values.
For example, you might want to download, clean, and subset a data set before doing multiple analyses with it.
In that case, you probably want to store the result of downloading, cleaning, and subsetting as a variable, and use that in your analyses.


