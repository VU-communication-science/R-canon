---
title: "Multiple regression"
subtitle: "Controlling for other variables"
format:
  html:
    df-print: paged
---


# What is multiple regression?

Multiple regression means that we are using more than one predictor variable to predict the dependent variable.
In practice, it is very similar to simple linear regression, and if you already know how to use and report simple linear regression, you can easily extend this to multiple regression. 
If you are not yet familiar with simple linear regression, make sure to check out the [tutorial on linear regression](../tests/linear-regression.qmd) first.

Multiple regression is a critical tool in the social sciences, because it allows us to control for confounding variables.
As explained in the section on [correlation](../tests/correlation.qmd), we need to be careful when interpreting the correlation between two variables, because it might be a **spurious correlation**.
The same is true for the effects in a regression model.
It is possible (and common) that the effect of the independent variable on the dependent variable is not because of an actual causal relation. 
Instead, it could be due to a third variable that influences both the independent and dependent variable.
This third variable is then called a **confounding variable**, and the effect that we observe between the independent and dependent variable is a **spurious effect**.

*Multiple* regression allows us to control for these confounding variables by including *multiple* independent variables in the model.
When we do, the model estimates the effect of each independent variable on the dependent variable *while controlling for the other variables*.

# How to use

To use multiple regression in R, you can use the `lm()` function, just like in simple linear regression.
Let's use the same example data as in the linear regression tutorial for using a `dummy` variable, where we looked at the effect of `np_subscription` (yes/no) on `trust_t1`. 
But this time, we also run a second model where we also add the `age` of the participants as a control variable.
<!-- Demographic variables like `age`, `gender` and `education` are often used as control variables, because they are known to influence many other variables, making them likely to lead to spurious effects. -->

When running multiple regression, it is good practice to report the results of multiple models side by side. 
This way you can see what happens to the effects when you control for other variables.
With the `tab_model()` function from the `sjPlot` package, you can pass multiple models to the function and it will show you the results side by side.

```{r, message=F}
library(tidyverse)
library(sjPlot)

## read and clean the practice data
d <- read_csv("https://tinyurl.com/R-practice-data") |>
        mutate(age = if_else(age > 100, 2024 - age, age))

m1 <- lm(trust_t1 ~ np_subscription, data = d)
m2 <- lm(trust_t1 ~ np_subscription + age, data = d)
tab_model(m1, m2)
```

In the first model, we see that people that have a subscription to a newspaper have a higher trust in the news media compared to people that do not have a subscription ($b = 0.37$, p < 0.001).
However, in the second model, where we control for age, this effect is no longer significant ($b = -0.04$, p = 0.612).
This suggests that the effect of `np_subscription` on `trust_t1` was spurious, and that the real reason for the difference in trust between subscribers and non-subscribers is the age of the participants.
That is, older people are more likely to have a newspaper subscription, and older people also have a higher trust in the news media.

Being able to control for confounding variables enables us to better explore possible [causal relationships](../concepts/causality.qmd) between variables.
Based on the first model, we might have concluded that having a newspaper subscription causes people to trust the news media more.
If we were a newspaper, this finding might have led us to invest in subscription campaigns to increase trust in the news media.
However, our second model indicates that this strategy would *probably*[^1] not be effective.

[^1]: We say *probably*, because we cannot say with certainty that a newspaper subscription really has no effect.
The only thing we can say is that based on this data, we do not find evidence for this effect.
But this *is* already a huge win, because it prevents us from making bad decisions based on a spurious effect.


# Example of controlling for confounding variables

There is a [famous example](http://www.brixtonhealth.com/storksBabies.pdf) of confounding, which points at the surprisingly high correlation between the number of storks and the number of newborn babies across European countries ($\rho = 0.62$)
A naive interpretation would then be that this is evidence for the folk theory that storks deliver babies.
The real reason however, as explained in the article, is that the size of the country is a confounding variable: larger countries simply have more storks and more babies.

So let's use this example to demonstrate how we can control for confounding variables in a regression model!
Here we create a tibble using the data from **table 1** in the article.

```{r, message=F}
library(tidyverse)
library(sjPlot)

storks <- tibble(
 storks = c(100,300,1,5000,9,140,3300,2500,4,5000,5,30000,1500,5000,8000,150,25000),
 birth_rate = c(83,87,118,117,59,774,901,106,188,124,551,610,120,367,439,82,1576),
 area_km2 = c(29,84,31,111,43,544,357,132,42,93,301,312,92,237,505,41,779) 
)
```

The data we have here is about the number of `storks`, the `birth_rate`, and the `area_km2` of the country.
Let's first have a look at the correlations.

```{r}
tab_corr(storks)
```

As reported in the article, the correlation between the number of storks and the birth rate is high at $r = 0.62$.
But we also see that the area of the country is correlated with both the number of storks and the birth rate ($r = 0.58$ and $r = 0.92$ respectively).
This is a good indication that the area of the country is likely a confounding variable.

Now let's see what happens when we put the number of storks into a regression model.
Our dependent variable is the `birth_rate`, our independent variable is the `storks`, and we'll control for the `area_km2`.
The variables are standardized first, so that you can see the relation to the correlation coefficients.
We'll run two models: without and with the control variable.


```{r}
##  standardize all variables
storks_z <- storks |> scale() |> as_tibble()

m1 <- lm(birth_rate ~ storks,            data = storks_z)
m2 <- lm(birth_rate ~ storks + area_km2, data = storks_z)
tab_model(m1, m2)
```

In the first model we see a positive effect of the number of storks on the birth rate ($\beta = 0.62$).
The effect is significant (p < 0.01) and the standardized coefficient is the same as the correlation coefficient.
So here you see firsthand a spurious effect.

In the second model, where we control for the area of the country, the effect of the number of storks on the birth rate is no longer significant (p = 0.304).
Instead, we see a very strong effect of the area of the country on the birth rate ($\beta = 0.85$, p < 0.001).
So here you see the power of multiple regression in action: by controlling for `area_km2`, the spurious effect of `storks` on `birth_rate` disappears. 