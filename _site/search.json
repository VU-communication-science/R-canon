[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Communication Science R Canon",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "2_statistics/2_tests/index.html",
    "href": "2_statistics/2_tests/index.html",
    "title": "Tests",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "2_statistics/2_tests/regression.html",
    "href": "2_statistics/2_tests/regression.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Statistics",
      "Tests",
      "Regression"
    ]
  },
  {
    "objectID": "2_statistics/2_tests/chi2.html",
    "href": "2_statistics/2_tests/chi2.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Statistics",
      "Tests",
      "Chi2"
    ]
  },
  {
    "objectID": "2_statistics/0_general-concepts/index.html",
    "href": "2_statistics/0_general-concepts/index.html",
    "title": "General Concepts",
    "section": "",
    "text": "In this folder we can put explanations of general concepts, like p-values, distributions, etc.\n\n\n\n Back to top",
    "crumbs": [
      "Statistics",
      "General Concepts"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/3_visualization.html",
    "href": "1_data-management/0_tidyverse/3_visualization.html",
    "title": "Visualization basics",
    "section": "",
    "text": "This tutorial teaches the basics of data visualization using the ggplot2 package (included in tidyverse). For more information, see R4DS Chapter 3: Da`ta Visualization and R4DS Chapter 7: Exploratory Data Analysis.\nFor many cool visualization examples using gplot2 (with R code included!) see the R Graph Gallery. For inspiration (but unfortunately no R code), there is also a 538 blog post on data visualization from 2016. Finally, see the article on ‘the grammar of graphics’ published by Hadley Wickham for more insight into the ideas behind ggplot.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Visualization basics"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/3_visualization.html#important-note-on-ggplot-command-syntax",
    "href": "1_data-management/0_tidyverse/3_visualization.html#important-note-on-ggplot-command-syntax",
    "title": "Visualization basics",
    "section": "Important note on ggplot command syntax",
    "text": "Important note on ggplot command syntax\nFor the plot to work, R needs to execute the whole ggplot call and all layers as a single statement. Practically, that means that if you combine a plot over multiple lines, the plus sign needs to be at the end of the line, so R knows more is coming. The general syntax is always:\n\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n\nSo, the following is good:\n\nggplot(data = facts_state) + \n  geom_point(mapping = aes(x = college, y = income))\n\nBut this is not:\n\nggplot(data = facts_state) \n  + geom_point(mapping = aes(x = college, y = income))\n\nAlso note that the data and mapping arguments are the first arguments the functions expect, so you can also leave them out:\n\nggplot(facts_state) + \n  geom_point(aes(x = college, y = income))",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Visualization basics"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/3_visualization.html#other-aesthetics",
    "href": "1_data-management/0_tidyverse/3_visualization.html#other-aesthetics",
    "title": "Visualization basics",
    "section": "Other aesthetics",
    "text": "Other aesthetics\nTo find out which visual elements can be used in a layer, use e.g. ?geom_point. According to the help file, we can (among others) set the colour, alpha (transparency), and size of points. Let’s first set the size of points to the (log) population of each state, creating a bubble plot:\n\nggplot(data = facts_state) + \n  geom_point(aes(x = college, y = income, size = population))\n\nSince it is difficult to see overlapping points, let’s make all points somewhat transparent. Note: Since we want to set the alpha of all points to a single value, this is not a mapping (as it is not mapped to a column from the data frame), but a constant. These are set outside the mapping argument:\n\nggplot(data = facts_state) + \n  geom_point(aes(x = college, y = income, size = population), \n             alpha = .5, \n             colour = \"red\")\n\nInstead of setting colour to a constant value, we can also let it vary with the data. For example, we can colour the states by percentage of population that is identified as ‘white’:\n\nggplot(data = facts_state) + \n  geom_point(aes(x=college, y=income, size=population, colour=white), \n             alpha=.9)\n\nFinally, you can map to a categorical value as well. Let’s categorize states into whether population is growing (at least 1%) or stable or declining. We use the if_else(condition, iftrue, iffalse) function, which assigns the iftrue value if the condition is true, and iffalse otherwise:\n\nfacts_state &lt;- facts_state %&gt;% \n  mutate(growth = ifelse(pop_change &gt; 1, \"Growing\", \"Stable\"))\n\nggplot(data=facts_state) + \n  geom_point(aes(x = college, y = income, size = population, colour = growth), \n             alpha=.9)\n\nAs you can see in these examples, ggplot tries to be smart about the mapping you ask. It automatically sets the x and y ranges to the values in your data. It mapped the size such that there are small and large points, but not e.g. a point so large that it would dominate the graph. For the colour, for interval variables it created a colour scale, while for a categorical variable it automatically assigned a colour to each group.\nOf course, each of those choices can be customized, and sometimes it makes a lot of sense to do so. For example, you might wish to use red for republicans and blue for democrats, if your audience is used to those colors; or you may wish to use grayscale for an old-fashioned paper publication. We’ll explore more options in a later tutorial, but for now let’s be happy that ggplot does a lot of work for us!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Visualization basics"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/3_visualization.html#setting-graph-options",
    "href": "1_data-management/0_tidyverse/3_visualization.html#setting-graph-options",
    "title": "Visualization basics",
    "section": "Setting graph options",
    "text": "Setting graph options\nSome options, like labels, legends, and the coordinate system are graph-wide rather than per layer. You add these options to the graph by adding extra functions to the call. For example, we can use coord_flip() to swap the x and y axes:\n\nggplot(nh_gop) + \n  geom_col(aes(x=candidate, y=votes)) +\n  coord_flip()\n\nYou can also reorder categories with the fct_reorder function, for example to sort by number of votes. Also, let’s add some colour (just because we can!):\n\nggplot(nh_gop) + \n  geom_bar(aes(x=fct_reorder(candidate, votes), y=votes, fill=candidate), \n           stat='identity') + \n  coord_flip()\n\n(Note: this works because ggplot assumes all labels are factors, which have an ordering; you can use other functions from the forcats package (generally starting with fct_) to do other things such as reversing the order, manually specifying the order, etc).\nThis is getting somewhere, but the y-axis label is not very pretty and we don’t need guides for the fill mapping. This can be remedied by more graph-level options. Also, we can use a theme to alter the appearance of the graph, for example using the minimal theme:\n\nggplot(nh_gop) + \n  geom_bar(aes(x=reorder(candidate, votes), y=votes, fill=candidate), \n           stat='identity') + \n  coord_flip() + \n  xlab(\"Candidate\") + \n  guides(fill=\"none\") + \n  theme_minimal()",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Visualization basics"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/3_visualization.html#grouped-bar-plots",
    "href": "1_data-management/0_tidyverse/3_visualization.html#grouped-bar-plots",
    "title": "Visualization basics",
    "section": "Grouped bar plots",
    "text": "Grouped bar plots\nWe can also add groups to bar plots. For example, we can set the x category to state (taking only NH and IA to keep the plot readable), and then group by candidate:\n\ngop2 &lt;- results_state %&gt;% \n  filter(party == \"Republican\" & (state == \"New Hampshire\" | state == \"Iowa\")) \nggplot(gop2) + geom_col(aes(x=state, y=votes, fill=candidate))\n\nBy default, the groups are stacked. This can be controlled with the position parameter, which can be dodge (for grouped bars) or fill (stacking to 100%): (note that the position is a constant, not an aesthetic mapping, so it goes outside the aes argument)\n\nggplot(gop2) + geom_col(aes(x=state, y=votes, fill=candidate), position='dodge')\nggplot(gop2) + geom_col(aes(x=state, y=votes, fill=candidate), position='fill')\n\nOf course, you can also make the grouped bars add up to 100% by computing the proportion manually, which can give you a bit more control over the process.\nNote that the example below pipes the preprocessing output directly into the ggplot command, that is, it doesn’t create a new temporary data set like gop2 above. This is entirely a stylistic choice, but can be useful for operations that are only intended for a single visualization.\n\ngop2 %&gt;% \n  group_by(state) %&gt;% \n  mutate(vote_prop=votes/sum(votes)) %&gt;%\n  ggplot() + \n    geom_col(aes(x=state, y=vote_prop, fill=candidate), position='dodge') + \n    ylab(\"Votes (%)\")\n\nNote that where group_by %&gt;% summarize replaces the data frame by a summarization, group_by %&gt;% mutate adds a column to the existing data frame, using the grouped values for e.g. sums. See our tutorial on Data Summarization for more details.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Visualization basics"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/4_reshaping.html",
    "href": "1_data-management/0_tidyverse/4_reshaping.html",
    "title": "Long and Wide data",
    "section": "",
    "text": "This tutorial discusses how to reshape data, particularly from long to wide format and vice versa. It mostly follows Chapter 12 of the R4DS book, but uses the pivot_longer and pivot_wider functions that replace gather and spread1. At the time of writing these functions are not yet in the book, but the writers explain the change and the new functions here.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/4_reshaping.html#pivot-longer-wide-to-long",
    "href": "1_data-management/0_tidyverse/4_reshaping.html#pivot-longer-wide-to-long",
    "title": "Long and Wide data",
    "section": "Pivot longer (wide to long)",
    "text": "Pivot longer (wide to long)\nWe will tidy this data in three steps. First, we pivot_longer the columns into a single column with all measurements. Then, we separate the country from the measurement level. Finally, we pivot_wider the measurement levels to columns again (since they are measurements on the same observation).\nThe first step is the same as above: we gather all columns except for the year column into a single column:\n\nwealth = pivot_longer(wealth_raw, -Year, names_to=\"key\", values_to=\"value\")\nwealth",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/4_reshaping.html#separating-columns-splitting-one-column-into-two",
    "href": "1_data-management/0_tidyverse/4_reshaping.html#separating-columns-splitting-one-column-into-two",
    "title": "Long and Wide data",
    "section": "Separating columns (splitting one column into two)",
    "text": "Separating columns (splitting one column into two)\nThe next step is to split the ‘key’ column into two columns, for country and for measurement. This can be done using the separate command, for which you specify the column to split, the new column names, and what separator to split on:\n\nwealth = separate(wealth, key, into = c(\"country\",\"measurement\"), sep=\":\")\nwealth\n\nThe measurement column is quoted in the output because it stars with a space. We could resolve this by specifying sep=\": \" (i.e. adding the space to the separator). We can also solve this by changing the column after the split with mutate. The code below removes the space using the trimws (trim white space) function:\n\nwealth %&gt;% mutate(measurement = trimws(measurement))\n\nWe can also use sub to search and replace (substitute) within a column, in this case changing ” top ” into “capital_top_”:\n\nwealth = wealth %&gt;% mutate(measurement = sub(\" top \", \"capital_top_\", measurement))\nwealth",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/4_reshaping.html#pivot-wider-long-to-wide",
    "href": "1_data-management/0_tidyverse/4_reshaping.html#pivot-wider-long-to-wide",
    "title": "Long and Wide data",
    "section": "Pivot wider (long to wide)",
    "text": "Pivot wider (long to wide)\nThe wealth data above is now ‘too long’ to be tidy: the measurement for each country is spread over multiple rows, listing the three different measurement levels (decile, percentile, promille). In effect, we want to undo one level of gathering, by spreading the column over multiple columns.\nThs syntax for the spread call is similar to that for pivot_longer: pivot_wider(data, names_from=key_column, values_from=value_column). Before we had the arguments names_to and values_to, to specify the column names of the new stacked (i.e. long format) columns. This time, we have the names_from and values_from arguments to reverse the process. For each unique value in the names_from column a new column will be created, with the corresponding value in the values_from column in the cell.\n\nwealth = pivot_wider(wealth, names_from=measurement, values_from=value)\nwealth\n\nSo now each row contains three measurements (columns, variables) relating to each observation (country x year).",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/4_reshaping.html#tidyness-as-a-matter-of-perception",
    "href": "1_data-management/0_tidyverse/4_reshaping.html#tidyness-as-a-matter-of-perception",
    "title": "Long and Wide data",
    "section": "Tidyness as a matter of perception",
    "text": "Tidyness as a matter of perception\nAs a last exercise, suppose we would like to plot wealth and capital inequality in the same figure as separate lines. You can do this with two separate geom_line commands, and e.g. use a dashed line for income inequality:\n\nggplot(inequality) + geom_line(aes(x=Year, y=capital_top_decile, colour=country)) + \n  geom_line(aes(x=Year, y=income_topdecile, colour=country), linetype=\"dashed\")\n\nThis works, but it would be nice if we could specify the measurement as colour (or type) and have ggplot automatically make the legend. To do this, the different measurements need to be in rows rather than in columns. In other words, data that is tidy from one perspective can be ‘too wide’ for another.\nLet’s gather the data into a single column, and plot the result for the US:\n\ninequality2 = pivot_longer(inequality, income_topdecile:capital_top_promille, names_to=\"measurement\", values_to=\"value\")\n\ninequality2 %&gt;% \n  filter(country==\"US\") %&gt;% \n  ggplot() + geom_line(aes(x=Year, y=value, linetype=measurement))\n\nWe can also plot only top-decile capital and income in a paneled plot. Note the use of extra options to set legend location and title, vertical label, and main title text and location (horizontal justification):\n\ninequality2 %&gt;% \n  filter(measurement %in% c(\"income_topdecile\", \"capital_top_decile\") & country != \"Europe\") %&gt;% \n  ggplot() + geom_line(aes(x=Year, y=value, linetype=measurement)) + facet_wrap(~ country, nrow = 2) +\n  scale_linetype_discrete(name=\"Variable:\", labels=c(\"Capital\", \"Income\")) +\n  theme(legend.position=\"bottom\", plot.title = element_text(hjust = 0.5)) +\n  ylab(\"Inequality\") + \n  ggtitle(\"Capital and income inequality over time\")",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/4_reshaping.html#footnotes",
    "href": "1_data-management/0_tidyverse/4_reshaping.html#footnotes",
    "title": "Long and Wide data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe replacement of spread and gather with pivot_wider and pivot_longer is a recent change, so you might still see spread and gather used in code from other. As such, it is still usefull to have a look at how spread and gather work (which is very similar to pivot_wider and pivot_longer). However, make sure to use the new pivot_ functions in your own code, because spread and gather are on their way out.↩︎",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/5_joining.html",
    "href": "1_data-management/0_tidyverse/5_joining.html",
    "title": "Joining data",
    "section": "",
    "text": "In many cases, you need to combine data from multiple data sources. For example, you can combine a sentiment analysis of tweets with metadata about the tweets; or data on election results with data about the candidates ideological positions or details on the races.\nThis tutorial will teach you the inner_join and other _join commands used to combine two data sets on shared columns. See R4DS Chapter 13: Relational Data for more information and examples.\n\n\nFor this tutorial, we will look at data describing the US presidential primaries. These data can be downloaded from the Houston Data Visualisation github page, who in turn got it from Kaggle.\nIn the CSV folder on the github, you can find (among others)\n\nprimary_results.csv Number of votes in the primary per county per candidate\nprimary_schedule.csv Dates of each primary per state and per party\ncounty_facts.csv Information about the counties and states, including population, ethnicity, age, etc.\n\nFor many research questions, we need to be able to combine the data from these files. For example, we might want to know if Clinton did better in counties or states with more women (needing results and facts), or how Trump’s performance evolved over time (requiring results and calendar).\n\n\n\nBefore we start, let’s download the three data files:\n\nlibrary(tidyverse)\ncsv_folder_url &lt;- \"https://raw.githubusercontent.com/houstondatavis/data-jam-august-2016/master/csv\"\nresults &lt;- read_csv(paste(csv_folder_url, \"primary_results.csv\", sep = \"/\"))\nfacts &lt;- read_csv(paste(csv_folder_url, \"county_facts.csv\", sep = \"/\"))\nschedule  &lt;- read_csv(paste(csv_folder_url, \"primary_schedule.csv\", sep = \"/\"))\n\nNote: I use paste to join the base url with the filenames, using a / as a separator.\nHave a look at all three data sets. Before we proceed, there are some things we want to do. First, the facts data frame is really large, with 54 columns. Let’s select a couple interesting ones to work with:\n\nfacts_subset &lt;- facts %&gt;% \n  select(area_name, \n         population = Pop_2014_count, \n         pop_change = Pop_change_pct, \n         over65 = Age_over_65_pct, \n         female = Sex_female_pct, \n         white = Race_white_pct, \n         college = Pop_college_grad_pct, \n         income = Income_per_capita)\n\nNext, the schedule dates are now a character (textual) field rather than date, so let’s fix that using the as.Date function, specifying the dates to be formatted as month/day/year:\n\nschedule &lt;- schedule %&gt;% \n  mutate(date = as.Date(date, format=\"%m/%d/%y\"))\n\nLast, let’s create a data set with per-state (rather than per-country) election results using group_by and summarize:\n\nresults_state &lt;- results %&gt;% \n  group_by(state, party, candidate) %&gt;% \n  summarize(votes = sum(votes))\nresults_state\n\nNote: see R-tidy-5-transformations if you are unsure about the transformations above!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Joining data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/5_joining.html#data",
    "href": "1_data-management/0_tidyverse/5_joining.html#data",
    "title": "Joining data",
    "section": "",
    "text": "For this tutorial, we will look at data describing the US presidential primaries. These data can be downloaded from the Houston Data Visualisation github page, who in turn got it from Kaggle.\nIn the CSV folder on the github, you can find (among others)\n\nprimary_results.csv Number of votes in the primary per county per candidate\nprimary_schedule.csv Dates of each primary per state and per party\ncounty_facts.csv Information about the counties and states, including population, ethnicity, age, etc.\n\nFor many research questions, we need to be able to combine the data from these files. For example, we might want to know if Clinton did better in counties or states with more women (needing results and facts), or how Trump’s performance evolved over time (requiring results and calendar).",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Joining data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/5_joining.html#downloading-and-preparing-the-data",
    "href": "1_data-management/0_tidyverse/5_joining.html#downloading-and-preparing-the-data",
    "title": "Joining data",
    "section": "",
    "text": "Before we start, let’s download the three data files:\n\nlibrary(tidyverse)\ncsv_folder_url &lt;- \"https://raw.githubusercontent.com/houstondatavis/data-jam-august-2016/master/csv\"\nresults &lt;- read_csv(paste(csv_folder_url, \"primary_results.csv\", sep = \"/\"))\nfacts &lt;- read_csv(paste(csv_folder_url, \"county_facts.csv\", sep = \"/\"))\nschedule  &lt;- read_csv(paste(csv_folder_url, \"primary_schedule.csv\", sep = \"/\"))\n\nNote: I use paste to join the base url with the filenames, using a / as a separator.\nHave a look at all three data sets. Before we proceed, there are some things we want to do. First, the facts data frame is really large, with 54 columns. Let’s select a couple interesting ones to work with:\n\nfacts_subset &lt;- facts %&gt;% \n  select(area_name, \n         population = Pop_2014_count, \n         pop_change = Pop_change_pct, \n         over65 = Age_over_65_pct, \n         female = Sex_female_pct, \n         white = Race_white_pct, \n         college = Pop_college_grad_pct, \n         income = Income_per_capita)\n\nNext, the schedule dates are now a character (textual) field rather than date, so let’s fix that using the as.Date function, specifying the dates to be formatted as month/day/year:\n\nschedule &lt;- schedule %&gt;% \n  mutate(date = as.Date(date, format=\"%m/%d/%y\"))\n\nLast, let’s create a data set with per-state (rather than per-country) election results using group_by and summarize:\n\nresults_state &lt;- results %&gt;% \n  group_by(state, party, candidate) %&gt;% \n  summarize(votes = sum(votes))\nresults_state\n\nNote: see R-tidy-5-transformations if you are unsure about the transformations above!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Joining data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/2_summarizing.html",
    "href": "1_data-management/0_tidyverse/2_summarizing.html",
    "title": "Summarizing",
    "section": "",
    "text": "The functions used in the earlier part on data preparation worked on individual rows. Sometimes, you need to compute properties of groups of rows (cases). This is called aggregation (or summarization) and in tidyverse uses the group_by function followed by either summarize or mutate.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Summarizing"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/2_summarizing.html#grouping-rows",
    "href": "1_data-management/0_tidyverse/2_summarizing.html#grouping-rows",
    "title": "Summarizing",
    "section": "Grouping rows",
    "text": "Grouping rows\nNow, we can use the group_by function to group by, for example, pollster:\n\nd %&gt;% \n  group_by(Question)\n\nAs you can see, the data itself didn’t actually change yet, it merely recorded (at the top) that we are now grouping by Question, and that there are 8 groups (different questions) in total.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Summarizing"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/2_summarizing.html#summarizing",
    "href": "1_data-management/0_tidyverse/2_summarizing.html#summarizing",
    "title": "Summarizing",
    "section": "Summarizing",
    "text": "Summarizing\nTo summarize, you follow the group_by with a call to summarize. Summarize has a syntax that is similar to mutate: summarize(column = calculation, ...). The crucial difference, however, is that you always need to use a function in the calculation, and that function needs to compute a single summary value given a vector of values. Very common summarization functions are sum, mean, and sd (standard deviation).\nFor example, the following computes the average support per question (and sorts by descending support):\n\nd %&gt;% \n  group_by(Question) %&gt;%                    # group by \"Questions\"\n  summarize(Support = mean(Support)) %&gt;%    # average \"Support\" per group\n  arrange(-Support)                         # sort based on \"Support\"\n\nAs you can see, summarize drastically changes the shape of the data. There are now rows equal to the number of groups (8), and the only columns left are the grouping variables and the summarized values.\nYou can also compute summaries of multiple values, and even do ad hoc calculations:\n\nd %&gt;% \n  group_by(Question) %&gt;% \n  summarize(Dem = mean(Dem), \n            Rep = mean(Rep), \n            diff = mean(Dem-Rep)) %&gt;% \n  arrange(-diff)\n\nSo, Democrats are more in favor of all proposed gun laws except arming teachers.\nYou can also compute multiple summaries of a single value. Another useful function is n() (without arguments), which simply counts the values in each group. For example, the following gives the count, mean, and standard deviation of the support:\n\nd %&gt;% \n  group_by(Question) %&gt;% \n  summarize(n = n(),\n            mean = mean(Support), \n            sd = sd(Support))\n\nNote: As you can see, one of the values has a missing value (NA) for standard deviation. Why?",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Summarizing"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/2_summarizing.html#using-mutate-with-group_by",
    "href": "1_data-management/0_tidyverse/2_summarizing.html#using-mutate-with-group_by",
    "title": "Summarizing",
    "section": "Using mutate with group_by",
    "text": "Using mutate with group_by\nThe examples above all reduce the number of cases to the number of groups. Another option is to use mutate after a group_by, which allows you to add summary values to the rows themselves.\nFor example, suppose we wish to see whether a certain poll has a different prediction from the average polling of that question. We can group_by question and then use mutate to calculate the average support:\n\nd2 &lt;- d %&gt;% \n  group_by(Question) %&gt;%\n  mutate(avg_support = mean(Support), \n         diff = Support - avg_support)\nd2\n\nAs you can see, where summarize reduces the rows and columns to the groups and summaries, mutate adds a new column which is identical for all rows within a group.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Summarizing"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/2_summarizing.html#ungrouping",
    "href": "1_data-management/0_tidyverse/2_summarizing.html#ungrouping",
    "title": "Summarizing",
    "section": "Ungrouping",
    "text": "Ungrouping\nFinally, you can use ungroup to get rid of any groupings.\nFor example, the data produced by the example above is still grouped by Question as mutate does not remove grouping information. So, if we want to compute the overall standard deviation of the difference we could ungroup and then summarize:\n\nd2 %&gt;% \n  ungroup() %&gt;% \n  summarize(diff = sd(diff))\n\n(of course, running sd(d2$diff)) would yield the same result.)\nIf you run the same command without the ungroup, what would the result be? Why?",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Summarizing"
    ]
  },
  {
    "objectID": "0_getting-started/index.html",
    "href": "0_getting-started/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "In the VU Communication Science track, you’ll be using the R statistical software as a key tool for learning quantitative methods and data analysis techniques. R is a powerful, open-source programming language that is widely used in academia and industry for statistical analysis, visualization, text analysis, and more.\nThrough our courses, you’ll get hands-on experience with R, allowing you to analyze real-world data, perform statistical tests, create insightful visualizations, and develop reproducible research workflows. These skills will be invaluable as you move forward in your studies and future career, where data-driven decision-making plays a crucial role.\nR is also open-source, and therefore free to use for both academic and commercial purposes. This means you can continue to use R beyond your studies without any licensing costs, giving you the flexibility to apply your skills in various professional settings. The open-source nature of R also fosters a vibrant community of users and contributors who regularly share packages, tutorials, and resources. This community support makes it easier to find help, learn new techniques, and stay updated with the latest developments in data science and statistical analysis.\nIn this Getting started section, we’ll walk you through your first steps. We will cover how to install R, how to install and use the RStudio software for working with R, and the basics of the R syntax.\n\n\n\n Back to top",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "0_getting-started/0_install-r-and-rstudio.html",
    "href": "0_getting-started/0_install-r-and-rstudio.html",
    "title": "Install R and RStudio",
    "section": "",
    "text": "To work with R, you will need to install two pieces of software.\nBoth programs can be downloaded for free, and are available for all main operating systems (Windows, macOS and Linux).",
    "crumbs": [
      "Getting Started",
      "Install R and RStudio"
    ]
  },
  {
    "objectID": "0_getting-started/0_install-r-and-rstudio.html#installing-r",
    "href": "0_getting-started/0_install-r-and-rstudio.html#installing-r",
    "title": "Install R and RStudio",
    "section": "Installing R",
    "text": "Installing R\nTo install R, you can download it from the CRAN (comprehensive R Archive Network) website. Do not be alarmed by the website’s 90’s aesthetics. The website is legit.",
    "crumbs": [
      "Getting Started",
      "Install R and RStudio"
    ]
  },
  {
    "objectID": "0_getting-started/0_install-r-and-rstudio.html#installing-rstudio",
    "href": "0_getting-started/0_install-r-and-rstudio.html#installing-rstudio",
    "title": "Install R and RStudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nRStudio can be downloaded from the posit.co website, which is the developer of RStudio. Make sure to pick the latest version available for your operating system.",
    "crumbs": [
      "Getting Started",
      "Install R and RStudio"
    ]
  },
  {
    "objectID": "0_getting-started/5_packages.html",
    "href": "0_getting-started/5_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Tutorial about how to install and load packages\n\n\n\n Back to top",
    "crumbs": [
      "Getting Started",
      "Packages"
    ]
  },
  {
    "objectID": "0_getting-started/4_functions.html",
    "href": "0_getting-started/4_functions.html",
    "title": "Functions",
    "section": "",
    "text": "99% of what you do in R will involve using functions. A function in R is like a mini-program that you can use to perform specific tasks. It takes input, processes it, and gives you an output. For example, there are functions for:\nA function in R has the form: output = function_name(argument1, argument2, ...)\nFor example, the function c combines multiple values into a vector.\nx = c(1,2,3,4)\nNow, we can use the mean function to calculate the mean of these numbers:\nm = mean(x)\nThe calculated mean, 2.5, is now assigned to the name m:\nm\n\n[1] 2.5",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "0_getting-started/4_functions.html#optional-arguments",
    "href": "0_getting-started/4_functions.html#optional-arguments",
    "title": "Functions",
    "section": "Optional arguments",
    "text": "Optional arguments\nIn the c and mean functions above, all the arguments were required. To combine numbers into a vector, we needed to provide a list of numbers. To calculate a mean, we needed to provide a numeric vector.\nIn addition to the required arguments, a function can also have optional arguments, that give you more control over what a function does. For example, suppose we have a range of numbers that also contains a missing value. In R a missing value is called NA, which stands for Not Available:\n\nx_with_missing = c(1, 2, 3, NA, 4)\n\nNow, if we call the mean function, R will say that the mean is unknown, since the third value is unknown:\n\nmean(x_with_missing)\n\n[1] NA\n\n\nThis is statistically a very correct answer. But often, if some values happen to be missing in your data, you want to be able to calculate the mean just for the numbers that are not missing. Fortunately, the mean function has an optional argument na.rm (remove NAs) that you can set to TRUE (or to T, which is short for TRUE) to ignore the NAs:\n\nmean(x, na.rm=TRUE)\n\n[1] 2.5\n\n\nNotice that for the required argument, we directly provide the input x, but for the optional argument we include the argument name na.rm = TRUE. The reason is simply that there are other optional arguments, so we need to specify which one we’re using.\n\n\n\n\n\n\nHow do I know what arguments a function has?\n\n\n\n\n\nTo learn more about what a function does and what arguments it has, you can look it up in the ‘Help’ pane in the bottom right, or run ?function_name in R.\n\n?mean\n\nHere you can learn about the na.rm argument that we just used!\nIf you are just getting to know R, we recommend first finishing the rest of the Getting Started section. Then once you get the hang of things, have a look at the Use ?function help page tutorial.",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "0_getting-started/4_functions.html#using-pipes",
    "href": "0_getting-started/4_functions.html#using-pipes",
    "title": "Functions",
    "section": "Using pipes",
    "text": "Using pipes\nThere are two ways for using functions.\n\nThe first is the one shown above, where we put all the arguments between the parentheses: function_name(argument1, argument2, ...).\nThe second way is to pipe the first argument into the function: argument1 |&gt; function_name(argument2, ...)\n\nIf this is your first time seeing pipes, you’re probably wondering why you would want to do this? Why bother having two ways to do the exact same thing? The reason is that when writing code, you shouldn’t just think about what the code does, but also about how easy the code is to read. This not only helps you prevent mistakes, but also makes your analysis transparent.\nAs you’ll see later, you’ll encounter many cases where your analysis requires you to string together multiple functions. In these cases, pipes make your code much easier to read. let’s rewrite our code from above using the pipe notation:\n\nx_with_missing |&gt; mean(na.rm=T)\n\nNotice how our first argument, the required argument x_with_missing, is piped into the mean function. Inside the mean function, we only specify the second argument, the optional argument na.rm.\nNow imagine we would want to round the result (2.5) up to a round number (3). We can do this without pipe notation, but it would be quite ugly and hard to read:\n\nround(mean(x_with_missing, na.rm=T))\n\nThe pipe notation allows us to break this down into a nice pipeline:\n\nx_with_missing |&gt; \n  mean(na.rm=T) |&gt; \n  round()",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "tips-and-best-practices/0_use-help-pages.html",
    "href": "tips-and-best-practices/0_use-help-pages.html",
    "title": "Function Help Pages",
    "section": "",
    "text": "The help page tells you how you can use the function. For mean, it shows you that the main form is\n\nmean(x, trim = 0, na.rm = FALSE, ...)\n\nWhat this means is that\n\n\n\n Back to top",
    "crumbs": [
      "Tips and Best Practices",
      "Function Help Pages"
    ]
  },
  {
    "objectID": "0_getting-started/3_names-and-values.html",
    "href": "0_getting-started/3_names-and-values.html",
    "title": "Names and Values",
    "section": "",
    "text": "In R, and in computer programming in general, the most essential operation is to assign values to names. By value, we then simply mean a piece of information. This can be a anything: a single number, a string (i.e. piece of text), a list of numbers, and even an entire data set. Assigning such values to names is essential, because it allows us to more easily refer to the values.\nIn plain terms, assignment is how you make R remember things by assigning them to a name. To assign a value to a name, we use the arrow notation: name &lt;- value. For example:\nx &lt;- 2\nBy running the code x &lt;- 2, you are saying: Assign the value 2 to the name x. Any values that you assigned to names are stored in your Environment. You can see this environment in the top-right window, under the Environment tab. If you assigned 2 to x, you should see a table called Values, with in the left column the names (x) and in the right column the values (2).\nFrom hereon, when you use the name x in your code, it will refer to the value 2. So when we run the code x * 5 (x times 5) it will print the number 10\nx * 5\n\n[1] 10",
    "crumbs": [
      "Getting Started",
      "Names and Values"
    ]
  },
  {
    "objectID": "0_getting-started/3_names-and-values.html#assigning-different-types-of-values",
    "href": "0_getting-started/3_names-and-values.html#assigning-different-types-of-values",
    "title": "Names and Values",
    "section": "Assigning different types of values",
    "text": "Assigning different types of values\nYou can assign any type of value to a name, and you can use any name, as long as it starts with a letter and doesn’t contain spaces or symbols (but underscores are OK)\n\na_number = 5\nmy_cats_name = \"Hobbes\"\n\nIf you run this code and check you Environment (top-right), you should now see these name-value pairs added.",
    "crumbs": [
      "Getting Started",
      "Names and Values"
    ]
  },
  {
    "objectID": "0_getting-started/3_names-and-values.html#assigning-results",
    "href": "0_getting-started/3_names-and-values.html#assigning-results",
    "title": "Names and Values",
    "section": "Assigning results",
    "text": "Assigning results\nTill now we only directly assigned values to names. This is convenient, but the power of assignment really shines when you use it to store results. For example, we can also do this.\n\nx = 5 + 10\n\nThis a very simple example, but just think for a second what this allows us to do. Since we can assign anything to a name, we can break down any complicated procedure into multiple steps! For now, the key lesson is just to wrap your head around the syntax for assigning values to names. This is fundamental to everything you will be doing in R (and in programming in general).",
    "crumbs": [
      "Getting Started",
      "Names and Values"
    ]
  },
  {
    "objectID": "0_getting-started/2_r-in-action.html",
    "href": "0_getting-started/2_r-in-action.html",
    "title": "R in Action (demo)",
    "section": "",
    "text": "R is a very powerful tool, but it takes some time to learn how to use it before you get to fully appreciate what you can use it for. On this page we show you a quick example of some of the things you will learn here.\nAll the code that you see here will be explained in the rest of this online book. For now, don’t worry about understanding the code, and focus on thinking how the techniques we’ll be using fit into your tool belt as a communication scientist.",
    "crumbs": [
      "Getting Started",
      "R in Action (demo)"
    ]
  },
  {
    "objectID": "0_getting-started/2_r-in-action.html#loading-the-packages-well-use",
    "href": "0_getting-started/2_r-in-action.html#loading-the-packages-well-use",
    "title": "R in Action (demo)",
    "section": "Loading the packages we’ll use",
    "text": "Loading the packages we’ll use\nOne of the things that makes R so versatile, is that anyone can extend it by writing new packages. You can think of packages kind of like apps in an app-store. For this demo, we’ll need two packages, that you’ll first need to install.\n\ninstall.packages('tidyverse')\ninstall.packages('sjPlot')\n\nYou only need to install packages once, just like apps on you mobile phone. Once downloaded, they are stored in your R library. When you then use the packages in an R script, you just open them like this:\n\nlibrary(tidyverse)\nlibrary(sjPlot)",
    "crumbs": [
      "Getting Started",
      "R in Action (demo)"
    ]
  },
  {
    "objectID": "0_getting-started/2_r-in-action.html#import-data",
    "href": "0_getting-started/2_r-in-action.html#import-data",
    "title": "R in Action (demo)",
    "section": "Import data",
    "text": "Import data\nThe first step for any analysis is to import your data. Using the read_csv function from the tidyverse package, we can directly download this information from the internet and import it into R.",
    "crumbs": [
      "Getting Started",
      "R in Action (demo)"
    ]
  },
  {
    "objectID": "0_getting-started/1_using-rstudio.html",
    "href": "0_getting-started/1_using-rstudio.html",
    "title": "How to use RStudio",
    "section": "",
    "text": "Once you have installed R and RStudio, you can start by launching RStudio. If everything was installed correctly, RStudio will automatically launch R as well.\nThe first time you open RStudio, you will likely see three separate windows. The first thing you want to do is open an R Script (!!) to work in. To do so, go to the toolbar and select File -&gt; New File -&gt; R Script.\nYou will now see four windows split evenly over the four corners of your screen:\nWhile you can directly enter code into your console (bottom-left), you should always work with R scripts (top-left). This allows you to keep track of what you are doing and save every step.",
    "crumbs": [
      "Getting Started",
      "How to use RStudio"
    ]
  },
  {
    "objectID": "0_getting-started/1_using-rstudio.html#running-code-from-the-r-script",
    "href": "0_getting-started/1_using-rstudio.html#running-code-from-the-r-script",
    "title": "How to use RStudio",
    "section": "Running code from the R script",
    "text": "Running code from the R script\nCopy and paste the following example code into your R Script. For now, don’t bother understanding the syntax itself. Just focus on running it.\n\n3 + 3\n2 * 5\n6 / 2\n\"some text\"\n\"some more text\"\nsum(1,2,3,4,5)\n\nYou can run code by selecting the code and clicking on the Run button in the toolbar. However, we highly recommend getting used to using the keyboard shortcut, because this will greatly speed up your process. On Windows (and Linux) the shortcut is Ctrl + Enter. On Mac it’s Command + Enter.\nThere are two ways to run code:\n\nIf you select a specific piece of code (so that it is highlighted) you can run this specific selection. For example, select the first three lines (the three mathematical operations) and press Ctrl + Enter. This should then print the results for these three mathematical expressions. Note that you can also select a specific part on a line. Try selecting just the second 3 on the first line. This should just print the number 3.\nIf you haven’t made a selection, but your text cursor is somewhere on a line in your editor, you can press Ctrl + Enter to run the line where the cursor is at. This will also move the cursor to the next line, so you can walk through the code from top to bottom, running each line. Try starting on the first line, and pressing Ctrl + Enter six times, to run each line separately.",
    "crumbs": [
      "Getting Started",
      "How to use RStudio"
    ]
  },
  {
    "objectID": "0_getting-started/1_using-rstudio.html#using-rstudio-projects",
    "href": "0_getting-started/1_using-rstudio.html#using-rstudio-projects",
    "title": "How to use RStudio",
    "section": "Using RStudio projects",
    "text": "Using RStudio projects\nIt is best to put all your code in an RStudio project. This is essentially a folder on your computer in which you can store the R files and data for a project that you are working on. While you do not necessarily need a project to work with R, they are very convenient, and we strongly recommend using them.\nTo create a new project, go to the top-right corner of your RStudio window. Look for the button labeled Project: (None). Click on this button, and select New Project. Follow the instructions to create a new directory with a new project. Name the project “R introduction”.\nNow, open a new R script and immediately save it (select File -&gt; Save in the toolbar, or press ctrl-s). Name the file my_first_r_script.r. In the bottom-right corner, under the Files tab, you’ll now see the file added to the project. The extension .r indicates that the file is an R script.",
    "crumbs": [
      "Getting Started",
      "How to use RStudio"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/0_read-data.html",
    "href": "1_data-management/0_tidyverse/0_read-data.html",
    "title": "Read data into R",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Read data into R"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html",
    "href": "1_data-management/0_tidyverse/6_strings.html",
    "title": "Textual data",
    "section": "",
    "text": "The goal of this tutorial is to get you acquainted with basic string handling in R. A large part of this uses the stringr included in the Tidyverse. See also chapter 14 of R for Data Science and the stringr cheat sheet\nNote that ‘string’ is not an official word in R (which uses character to denote textual data), but since it’s the word used in most documentations I will also use strings to refer to objects containing textual data. (AFAIK, the name originates from seeing a text as a string or sequence of characters)",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html#combining-strings",
    "href": "1_data-management/0_tidyverse/6_strings.html#combining-strings",
    "title": "Textual data",
    "section": "Combining strings",
    "text": "Combining strings\nTo combine two strings, you can use str_c (which is equivalent to built-in paste0):\n\nstr_c(\"john\", \"mary\")\nstr_c(\"john\", \"mary\", sep = \" & \")\n\nIt can also work of longer vectors, where shorter vectors are repeated as needed:\n\nnames = c(\"john\", \"mary\")\nstr_c(\"Hello, \", names)\n\nFinally, you can also ask it to collapse longer vectors after the initial pasting:\n\nstr_c(names, collapse=\" & \")\nstr_c(\"Hello, \", names, collapse=\" and \")",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html#subsetting-strings",
    "href": "1_data-management/0_tidyverse/6_strings.html#subsetting-strings",
    "title": "Textual data",
    "section": "Subsetting strings",
    "text": "Subsetting strings\nTo take a fixed subset of a string, you can use str_sub. This can be useful, for example, to strip the time part off dates:\n\ndates = c(\"2019-04-01T12:00\", \"2012-07-29 01:12\")\nstr_sub(dates, start = 1, end = 10)\n\nYou can also replace a substring, for example to make sure the ‘T’ notation is used in the dates:\n\nstr_sub(dates, start=11, end=11) = \"T\"\ndates",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html#finding-patterns",
    "href": "1_data-management/0_tidyverse/6_strings.html#finding-patterns",
    "title": "Textual data",
    "section": "Finding patterns",
    "text": "Finding patterns\nRegular expressions can be used e.g. to find rows containing a specific pattern. For example, if we had a data frame containing the earlier texts, we can filter for rows containing an email address:\n\nt = tibble(id=1:3, text=txt)\nt\nt |&gt; filter(str_detect(text, regex_email))\n\nYou can also str_count to count how many matches of a pattern are found in each text:\n\nt |&gt; mutate(n_hashtags = str_count(text, \"#\\\\w+\"))",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html#replacing-patterns",
    "href": "1_data-management/0_tidyverse/6_strings.html#replacing-patterns",
    "title": "Textual data",
    "section": "Replacing patterns",
    "text": "Replacing patterns\nYou can also use regular expressions to do find-an-replace. For example, you can remove all punctionation, normalize whitespace, or redact email addresses:\n\nt |&gt; mutate(nopunct = str_replace_all(text, \"[^\\\\w ]\", \"\"),\n             normalized = str_replace_all(text, \"\\\\s+\", \" \"),\n             redacted = str_replace_all(text, \"\\\\w+@\", \"****@\"),\n             text = NULL)\n\nNote the use of setting text to NULL as an alternative way to drop a column. In textr, most functions have a _all variant which replaces/finds/extracts all matches, rather than just the first.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html#extracting-patterns",
    "href": "1_data-management/0_tidyverse/6_strings.html#extracting-patterns",
    "title": "Textual data",
    "section": "Extracting patterns",
    "text": "Extracting patterns\nBesides replacing patterns, it can also be useful to extract elements from a string, for example the email or hashtag:\n\nregex_email = regex(\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+\")\nt |&gt; mutate(email = str_extract(text, regex_email),\n             hashtag = str_extract(text, \"#\\\\w+\"))\n\nNote that for the hashtag, it only extracted the first hit. You can use the str_extract_all function, but since it can match zero, once, or more often per text, it returns a list containing all matches per row (or more correctly, per element of the input vector):\n\nstr_extract_all(t$text, \"#\\\\w+\")\n\nThe best way to deal with this in the context of a data frame is to use unnest_longer to turn the list into a long format. First, use mutate to create a column containing the lists (so this is a column which itself contains complex data)\n\nhashtags = mutate(t, tags = str_extract_all(t$text, \"#\\\\w+\"))\nhashtags\n\nAs you can see, the tags column has ‘list’ as its type, with the last row containing two elements. To turn this into a more usable dataframe, we ‘unnest’ it into a ‘long’ format using unnest_longer:\n\nhashtags |&gt; unnest_longer(\"tags\")\n\nAs you can see, this creates a new data frame with one row per hash tag.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html#using-separate",
    "href": "1_data-management/0_tidyverse/6_strings.html#using-separate",
    "title": "Textual data",
    "section": "Using separate",
    "text": "Using separate\nFirst, if the number of elements is known and you can use multiple you can use separate to separate the column into seperate columns:\n\nd |&gt; separate(person, into=c(\"firstname\", \"lastname\"), sep=\" \")\n\nAs said, this is mostly useful if a column always contains a fixed number of data points that each have a distinct meaning, e.g. first and last name or city and state.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html#using-str_split",
    "href": "1_data-management/0_tidyverse/6_strings.html#using-str_split",
    "title": "Textual data",
    "section": "Using str_split",
    "text": "Using str_split\nIf there is variable number of data points such as the list of books in the data set above, you can use str_split, which takes a regular expression argument to split the column:\n\nd |&gt; mutate(books = str_split(books, pattern=\",\"))\n\nJust like above, this produces a column of type ‘list’ which contains multiple values per row. To normalize this, use unnest_longer as above:\n\nd |&gt; mutate(books = str_split(books, pattern=\",\")) |&gt; unnest_longer(books)\n\nAs you can also see, the spaces around the book titles are not removed. You can fix this in two ways, either by changing the pattern to incluide optional whitespace (\\\\w is white space, * means the preceding element is optional and can be repeated); or by adding a trimws call afterwards:\n\nd |&gt; mutate(books = str_split(books, pattern=\"\\\\s*,\\\\s*\")) |&gt; unnest_longer(books)\nd |&gt; mutate(books = str_split(books, pattern=\",\")) |&gt; unnest_longer(books) |&gt; mutate(books=trimws(books))",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html#background",
    "href": "1_data-management/0_tidyverse/6_strings.html#background",
    "title": "Textual data",
    "section": "Background",
    "text": "Background\nA fairly short version of the story is as follows: when computers were mostly dealing with English text, life was easy, as there are not a lot of different letters and they could easily assign each letter and some punctuation marks to a number below 128, so it could be stored as 7 bits. For example, A is number 65. This encoding was called ‘ASCII’.\nIt turned out, however, that many people needed more than 26 letters, for example to write accented letters. For this reason, the 7 bits were expanded to 8, and many accented latin letters were added. This representation is called latin-1, also known as ISO-8859-1.\nOf course, many languages don’t use the latin script, so other 8-bit encodings were invented to deal with Cyrillic, Arabic, and other scripts. Most of these are based on ASCII, meaning that 65 still refers to ‘A’ in e.g. the Hebrew encoding. However, character 228 could refer to greek δ, cyrillic ф, or hebrew ה. Things get even more complex if you consider Chinese, where you can’t fit all characters in 256 numbers, so several larger (multi-byte) encodings were used.\nThis can cause a lot of confusion if you read a text that was encoding in e.g. greek as if it were encoded in Hebrew. A famous example of this confusion is that Microsoft Exchange used the WingDings font and encoding for rendering symbols in emails, amongst others using character 74 as a smiley. For non-exchange users (who didn’t have that font), however, it renders as the ASCII character nr 74: “J”. So, if you see an email from a microsoft user with a J where you expected a smiley, now you know :).\nTo end this confusion, unicode was invented, which assigns a unique number (called a code point) to each letter. A is still 65 (or “1” in hexadecimal R notataion), but δ is now uniquely “3B4”, and ф is uniquely “444”. There are over 1M possible unicode characters, of which about 100 thousand have been currently assigned. This gives enough room for Chinese, old Nordic runes, and even Klingon to be encoded.\nYou can directly use these in an R string:\n\n\"Some Unicode letters: \\u41 \\u03B4 \\u0444\"\n\nNow, to be able to write all 1M characters to string, one would need almost 24 bits per character, tripling the storage and memory needed to handle most text. So, more efficient encodings were invented that would normally take only 8 or 16 bits per character, but can take more bits if needed. So, while the problem of defining characters is solved, unfortunately you still need to know the actual encoding of a text. Fortunately, UTF-8 (which uses 1 byte for latin characters, but more for non-western text) is emerging as a de facto standard for most texts. This is a compromise which is most efficient for latin alphabeters, but is still able to unambiguously express all languages.\nIt is still quite common, however, to encounter text in other encodings, so it can be good to understand what problems you can face and how to deal with them",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html#text-encoding-in-r",
    "href": "1_data-management/0_tidyverse/6_strings.html#text-encoding-in-r",
    "title": "Textual data",
    "section": "Text encoding in R",
    "text": "Text encoding in R\nTo show how this works in R, we can use the charToRaw function to see how a character is encoded in R:\n\ncharToRaw('A')\n\n[1] 41\n\n\nNote that the output of this function depends on your regional settings (called ‘locale’). On most computers, this should produce 41 however, as most encodings are based on ASCII.\nFor other alphabets it can be more tricky. The Chinese character “蘭” (unicode “62d”) on my computer is expressed in UTF-8, where it takes 3 bytes:",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/6_strings.html#dealing-with-encodings",
    "href": "1_data-management/0_tidyverse/6_strings.html#dealing-with-encodings",
    "title": "Textual data",
    "section": "Dealing with encodings",
    "text": "Dealing with encodings\nTo convert between encodings, you can use the iconv function. For example, to express the Chinese character above in GB2312 (Chinese national standard) encoding:\n\ncharToRaw(iconv('蘭', to='GB2312'))\n\nThe most common way of dealing with encodings is to ignore the problem and hope it goes away. However, outside the English world this is often not an option. Also, due to general unicode ignorance many people will use the wrong encoding, and you will even see things like double-utf8-encoded text.\nThe sane way to deal with encodings is to make sure that all text stored inside your program is encoded in a standard encoding, presumably UTF-8. This means that whenever you read text from an external source, you need to convert it to UTF-8 if it isn’t already in that form.\nThis means that when you use read_csv (on text data) or readtext, you should ideally always specify which encoding the text is encoded in:\n\nreadtext::readtext(\"file.txt\", encoding = \"utf-8\")\nread_csv(\"file.csv\", locale=locale(encoding='utf-8'))\n\nIf you don’t know what encoding a text is in, you can try utf-8 and the most common local encodings (e.g. latin-1 in many western countries), you can inspect the raw bytes, or you can use the guessEncoding function from readr:\n\nguess_encoding(\"file.txt\")",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/index.html",
    "href": "1_data-management/0_tidyverse/index.html",
    "title": "The Tidyverse Toolkit",
    "section": "",
    "text": "The tidyverse versus base R\n\n\n\n\n\nMany of the things that the tidyverse allows you to do are also possible in base R (i.e. the basic installation of R). Base R also provides functions for importing, managing and visualizing data. So why do we need the tidyverse?\nThe tidyverse is an opinionated framework, which means that it doesn’t just enable you to do things, but also suggests how you should do things. The authors have thought long and hard about how to make data management easy, effective and intuitive (they have even written papers about it). This not only makes the tidyverse much easier and intuitive to learn, but also makes sure everyone writes their code in the same way, which improves transparency and shareability.\nThis is different from base R, which is designed to be a highly flexible programming language, that allows you to do almost anything. Accordingly, it is still worthwhile to learn base R at some point if you want to specialize more in computational research methods. But for our Communication Science program, and for many data science applications in general, you can do all your data management in the tidyverse.\nThe tidyverse is collection of R packages that makes it much easier to import, manage and visualize data. To use the tidyverse, you only need to open the tidyverse package, and it will automatically open all of the tidyverse R packages.\nLike any normal package, you need to first install it once:\ninstall.packages('tidyverse')\nThen in every script that you use the package, you open it with library:\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/index.html#what-packages-does-the-tidyverse-contain",
    "href": "1_data-management/0_tidyverse/index.html#what-packages-does-the-tidyverse-contain",
    "title": "The Tidyverse Toolkit",
    "section": "What packages does the tidyverse contain?",
    "text": "What packages does the tidyverse contain?\nNotice above that when you run library(tidyverse), R prints all the tidyverse packages that it opened for you. Some of the most important ones that we’ll we using are:\n\ntibble. An optimized way for structuring rectangular data (basically: a spreadsheet of rows and columns)\ndplyr. Functions for manipulating tibbles: select and rename columns, filter rows, mutate values, etc.\nreadr. Read data into R.\nggplot2. One of the best visualization tools out there. Check out the gallery\n\n\n\n\n\n\n\nWhat about the ‘Conflicts’?\n\n\n\n\n\nWhen opening the tidyverse, and when opening packages in general, you can get a Conflicts warning. A very common warning for the tidyverse is:\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nSo what does this mean, and should we be worried?\nSince anyone can write new packages for R, it can happen that two packages provide functions with the same name. In this example, we see that the filter function exists in both the dplyr package (which we opened by opening the tidyverse), and in the stats package (which is included in base R). So now R needs to decide which version of the function to use when you type filter(). In this case, it says that the dplyr::filter() masks stats::filter(), meaning that it will now use the dplyr version.\nIn practice, this will rarely be a problem, because you seldom need two versions of a function in the same script. But if you ever do, there is a simple solution. Instead of just using filter(), you can type dplyr::filter() to specifically use this version. In the following code, we use this notation to specifically open the help page for dplyr::filter and stats::filter.\n\n?dplyr::filter()\n?stats::filter()",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html",
    "href": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html",
    "title": "Data transformation",
    "section": "",
    "text": "The goal of this tutorial is to get you acquainted with the Tidyverse. Tidyverse is a collection of packages that have been designed around a singular and clearly defined set of principles about what data should look like and how we should work with it. It comes with a nice introduction in the R for Data Science book, for which the digital version is available for free. This tutorial deals with most of the material in chapter 5 of that book.\nIn this part of the tutorial, we’ll focus on working with data using the tidyverse package. This package includes the dplyr (data-pliers) packages, which contains most of the tools we’re using below, but it also contains functions for reading, analysing and visualising data that will be explained later.\n\n\nAs before, install.packages() is used to download and install the package (you only need to do this once on your computer) and library() is used to make the functions from this package available for use (required each session that you use the package).\n\ninstall.packages('tidyverse') # only needed once\n\n\nlibrary(tidyverse)\n\nNote: don’t be scared if you see a red message after calling library. RStudio doesn’t see the difference between messages, warnings, and errors, so it displays all three in red. You need to read the message, and it will contain the word ‘error’ if there is an error, such as a misspelled package:\n\nlibrary(tidyvers) # this will cause an error!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#installing-tidyverse",
    "href": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#installing-tidyverse",
    "title": "Data transformation",
    "section": "",
    "text": "As before, install.packages() is used to download and install the package (you only need to do this once on your computer) and library() is used to make the functions from this package available for use (required each session that you use the package).\n\ninstall.packages('tidyverse') # only needed once\n\n\nlibrary(tidyverse)\n\nNote: don’t be scared if you see a red message after calling library. RStudio doesn’t see the difference between messages, warnings, and errors, so it displays all three in red. You need to read the message, and it will contain the word ‘error’ if there is an error, such as a misspelled package:\n\nlibrary(tidyvers) # this will cause an error!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#reading-data-read_csv",
    "href": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#reading-data-read_csv",
    "title": "Data transformation",
    "section": "Reading data: read_csv",
    "text": "Reading data: read_csv\nThe example above manually created a data set, but in most cases you will start with data that you get from elsewhere, such as a csv file (e.g. downloaded from an online dataset or exported from excel) or an SPSS or Stata data file.\nTidyverse contains a function read_csv that allows you to read a csv file directly into a data frame. You specify the location of the file, either on your local drive or directly from the Internet!\nThe example below downloads an overview of gun polls from the data analytics site 538, and reads it into a tibble using the read_csv function:\n\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/poll-quiz-guns/guns-polls.csv\"\nd &lt;- read_csv(url)\nd\n\n(Note that you can safely ignore the (red) message, they simply tell you how each column was parsed)\nThe shows the first ten rows of the data set, and if the columns don’t fit they are not printed. The remaining rows and columns are printed at the bottom. For each column the data type is also mentioned ( stands for integer, which is a numeric value;  is textual or character data). If you want to browse through your data, you can also click on the name of the data.frame (d) in the top-right window “Environment” tab or call View(d).",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#subsetting-with-filter",
    "href": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#subsetting-with-filter",
    "title": "Data transformation",
    "section": "Subsetting with filter()",
    "text": "Subsetting with filter()\nThe filter function can be used to select a subset of rows. In the guns data, the Question column specifies which question was asked. We can select only those rows (polls) that asked whether the minimum purchage age for guns should be raised to 21:\n\nage21 &lt;- filter(d, Question == 'age-21')\nage21\n\nThis call is typical for a tidyverse function: the first argument is the data to be used (d), and the remaining argument(s) contain information on what should be done to the data.\nNote the use of == for comparison: In R, = means assingment and == means equals. Other comparisons are e.g. &gt; (greather than), &lt;= (less than or equal) and != (not equal). You can also combine multiple conditions with logical (boolean) operators: & (and), | or, and ! (not), and you can use parentheses like in mathematics.\nSo, we can find all surveys where support for raising the gun age was at least 80%:\n\nfilter(d, Question == 'age-21' & Support &gt;= 80)\n\nNote that this command did not assign the result to an object, so the result is only displayed on the screen but not remembered. This can be a great way to quickly inspect your data, but if you want to continue analysing this subset you need to assign it to an object as above.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#aside-getting-help-on-tidy-function",
    "href": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#aside-getting-help-on-tidy-function",
    "title": "Data transformation",
    "section": "Aside: getting help on (tidy) function",
    "text": "Aside: getting help on (tidy) function\nAs explained earlier, to get help on a function you can type ?filter in the console or search for filter in the help pane. In both cases, you need to specify that you mean filter from the dplyr package, as there is also a filter function in other packages.\nIf you look at the help page, you will first see the general description. This is followed by Usage, which shows how the function should be called. In this case, it lists filter(.data, ...). The first argument (.data) makes sense, but the ... is confusing. What is means is that you can give an arbitrary number of extra arguments, that will (in this case) all be used as filters. This is explained in the Arguments: the ... arguments are ‘Logical predicates defined in terms of the variables in .data’.\nThe remainder give extra information on what exactly the function does (Details), the output it produces (Value), and links to other useful packages, functions, and finally a number examples.\nAlthough it may seem intimidating at first, it is important to get used to style of the R documentation as it is the primary source of information on most functions and packages you will be using!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#selecting-certain-columns",
    "href": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#selecting-certain-columns",
    "title": "Data transformation",
    "section": "Selecting certain columns",
    "text": "Selecting certain columns\nWhere filter selects specific rows, select allows you to select specific columns. Most simply, we can simply name the columns that we want to retrieve them in that particular order.\n\nselect(age21, Population, Support, Pollster)\n\nYou can also specify a range of columns, for example all columns from Support to Democratic Support:\n\nselect(age21, Support:`Democratic Support`)\n\nNote the use of ‘backticks’ (reverse quotes) to specify the column name, as R does not normally allow spaces in names.\nSelect can also be used to rename columns when selecting them, for example to get rid of the spaces:\n\nselect(age21, Pollster, rep = `Republican Support`, dem = `Democratic Support`)\n\nNote that select drops all columns not selected. If you only want to rename columns, you can use the rename function:\n\nrename(age21, start_date = Start, end_date = End)\n\nFinally, you can drop a variable by adding a minus sign in front of a name:\n\nselect(age21, -Question, -URL)",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#sorting-with-arrange",
    "href": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#sorting-with-arrange",
    "title": "Data transformation",
    "section": "Sorting with arrange()",
    "text": "Sorting with arrange()\nYou can easily sort a data set with arrange: you first specify the data, and then the column(s) to sort on. To sort in descending order, put a minus in front of a variable. For example, the following orders by population and then by support (descending):\n\nage21 &lt;- arrange(age21, Population, -Support)\nage21\n\nNote that I assigned the result of arranging to the age21 object again, i.e. I replace the object by its sorted version. If I wouldn’t assign it to anything, it would display it on screen but not remember the sorting. Assigning a result to the same name means I don’t create a new object, preventing the environment from being cluttered (and saving me from the bother of thinking up yet another object name). For sorting, this should generally be fine as the sorted data should contain the same data as before. For subsetting, this means that the rows or columns are actually deleted from the dataset (in memory), so you will have to read the file again (or start from an earlier object) if you need those rows or columns later.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#adding-or-transforming-variables-with-mutate",
    "href": "1_data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#adding-or-transforming-variables-with-mutate",
    "title": "Data transformation",
    "section": "Adding or transforming variables with mutate()",
    "text": "Adding or transforming variables with mutate()\nThe mutate function makes it easy to create new variables or to modify existing ones. For those more familiar with SPSS, this is what you would do with compute and recode.\nIf you look at the documentation page, you see that mutate works similarly to filter() and select(), in the sense that the first argument is the tibble, and then any number of additional arguments can be given to perform mutations. The mutations themselves are named arguments, in which you can provide any calculations using the existing columns.\nHere we’ll first create some variables and then look at the variables (using the select function to focus on the changes). Specifically, we’ll make a column for the absolute difference between the support scores for republicans and democrats, as a measure of how much they disagree.\n\nage21 &lt;- mutate(age21, party_diff = abs(`Republican Support` - `Democratic Support`))\nselect(age21, Question, Pollster, party_diff)\nage21 &lt;- arrange(age21, Population, -Support)\n\nTo transform (recode) a variable in the same column, you can simply use an existing name in mutate() to overwrite it.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "1_data-management/index.html",
    "href": "1_data-management/index.html",
    "title": "Data management",
    "section": "",
    "text": "Here we’ll add tutorials for\n\nimporting data as a tibble\nselecting and renaming columns\nfiltering rows\nmutating columns\njoining data\nreshaping data (long - wide)\n\n\n\n\n Back to top",
    "crumbs": [
      "Data management"
    ]
  },
  {
    "objectID": "2_statistics/2_tests/0_selecting-the-right-test.html",
    "href": "2_statistics/2_tests/0_selecting-the-right-test.html",
    "title": "Selecting the right test",
    "section": "",
    "text": "On this page we can explain when to use what test, and link to the right page.\n\n\n\n Back to top",
    "crumbs": [
      "Statistics",
      "Tests",
      "Selecting the right test"
    ]
  },
  {
    "objectID": "2_statistics/2_tests/t-test.html",
    "href": "2_statistics/2_tests/t-test.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Statistics",
      "Tests",
      "T Test"
    ]
  },
  {
    "objectID": "2_statistics/2_tests/anova.html",
    "href": "2_statistics/2_tests/anova.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Statistics",
      "Tests",
      "Anova"
    ]
  },
  {
    "objectID": "2_statistics/index.html",
    "href": "2_statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "2_statistics/0_causal_models.html",
    "href": "2_statistics/0_causal_models.html",
    "title": "Causal models",
    "section": "",
    "text": "In explanatory research, we are often concerned with causal relations. Why do certain social phenomena occur? What might have been their causes?\nHere are some examples of communication science theories about causal relations\nSo how do we know whether these theories are correct? The core of the scientific method lies in systematically testing these theories by *deriving hypotheses, and using statistical tests to confirm or challenge them based on empirical data.",
    "crumbs": [
      "Statistics",
      "Causal models"
    ]
  },
  {
    "objectID": "2_statistics/0_causal_models.html#deriving-hypotheses-from-theory",
    "href": "2_statistics/0_causal_models.html#deriving-hypotheses-from-theory",
    "title": "Causal models",
    "section": "Deriving hypotheses from theory",
    "text": "Deriving hypotheses from theory\nThe first step in testing a theory is to derive a hypothesis from it. A hypothesis is a testable statement that predicts the relationship between two or more variables. for example,\nLet’s consider the theory that increased social media use leads to lower life satisfaction. This theory suggests that the more time individuals spend on social media, the more likely they are to experience negative emotions, such as envy or loneliness, which can ultimately reduce their overall life satisfaction.\nTo explore this theory, we could derive the following hypothesis: “Individuals who spend more time on social media platforms will report lower levels of life satisfaction compared to those who spend less time on social media.”\nOnce we have this hypothesis, the next step is to statistically test it. This could involve collecting data on the amount of time participants spend on social media and their self-reported life satisfaction levels. By analyzing the data, we can assess whether there is a statistically significant relationship between social media use and life satisfaction. If the results confirm the hypothesis, it would provide evidence in support of the theory; if not, the theory may need further examination or adjustment.\nDeriving and testing such hypotheses allows us to empirically evaluate the validity of communication theories, offering insights into the potential impacts of social media on well-being.",
    "crumbs": [
      "Statistics",
      "Causal models"
    ]
  },
  {
    "objectID": "2_statistics/0_deriving_hypotheses.html",
    "href": "2_statistics/0_deriving_hypotheses.html",
    "title": "Deriving hypotheses",
    "section": "",
    "text": "In order to use statistical methods to test theories, we first need to derive hypotheses from them. A hypothesis is a testable statement, that when proven true or false, provides evidence for or against the theory.\nFor example, consider the theory that playing violent video games leads to increased aggression in children. There is much disagreement about whether this theory is correct (Griffiths 1999), so it’s important to test it empirically. In order to do so, we need to derive hypotheses that we can conduct to rigorous statistical tests. Some examples of hypotheses are:\n\nThere is a relation “between arcade game use and teachers’ ratings of impulsiveness and aggresiveness”. (Lin and Lepper 1987, 81)\nPlaying violent video games leads to higher levels of heart rate and blood pressure compared to playing non-violent video games. (Lynch 1994)\nThere will be a “linear increase in aggressive affect after playing nonaggressive, moderately aggresive, and highly aggressive games”. (Scott 1995, 125)\n\nNotice that these hypotheses are more specific than the theory they are derived from. There are multiple ways to define and measure “playing violent video games” and “increased aggression”, and it could also be that the relation only holds for certain groups of people, or only for short periods of time. Hypotheses help us make (different aspects of) the theory more explicit, so that we can conduct empirical tests.\n\n\nIf an hypothesis is confirmed, it provides evidence in support of the theory. However, note that it does not necessarily prove the theory. For example, if we find that children who play violent video games are more aggressive, it does not prove that the games caused the aggression. Perhaps people who are already aggressive are more likely to play violent video games, or there is a confounding variable that causes both aggression and video game playing (e.g., gender, socioeconomic status, or parental involvement).\nHypotheses are thus the bridge between theory and statistical methods, and are essential for conducting rigorous research in communication science. They need to be specific enough to be testable, but close enough to the theory they are derived from to allow for meaningful conclusions.",
    "crumbs": [
      "Statistics",
      "Deriving hypotheses"
    ]
  },
  {
    "objectID": "2_statistics/1_causal_models.html",
    "href": "2_statistics/1_causal_models.html",
    "title": "Causal relations",
    "section": "",
    "text": "In communication science we are often concerned with causal relations. Consider the following two examples:\n\nWhen news media focus heavily on certain issues, it enhances the public’s perception of their importance (Mccombs and Shaw 1972).\nMore screen time among children and adolescents is associated with lower psychological well-being (Twenge and Campbell 2018).\n\nResearch into such causal relations is important, because by understanding causal mechanisms, we can develop strategies to change the outcomes for the better. If more screen time is indeed associated with lower psychological well-being, we should advice parents to limit children’s screen time.\nBut before we incurr the wrath of our children by trying to take away their smartphones, we should first make sure that our theory is correct. As usual, we can derive hypotheses that predict relationships between variables, and then test these hypotheses with statistical methods.\nHowever, when testing causal relations, we need to be particularly careful, because correlation does not imply causation!\n\n\nCausation implies a cause-and-effect relationship, where a change in one variable (the cause) leads to a change in the other (the effect). If we were to somehow manipulate the cause, we would expect to see a corresponding change in the effect.\nFor instance, if we could somehow make the news media talk more about climate change, we would expect the public to think and talk about it more, too. Basically, this is why PR exists, and why politicians try to get media to talk about the issues they care about.\nCorrelation, on the other hand, indicates an association or relationship between two variables without establishing a direct cause-and-effect link. Perhaps there is a causal effect, but we don’t know which variable is the cause and which is the effect. But it could also be that there is a third variable that causes both of them to change. This is known as a confounding variable.\nIn the case of screen time and psychological well-being study, the authors did observe an “association”, but were not able to establish which variable causes the other. Based on their data, “it is not possible to determine if screen time leads to low well-being, low well-being leads to screen time, or both” (Twenge and Campbell 2018, 281). Indeed, it could be that children who are already unhappy end up spending more time on screens, which would mean that taking away their phone might not help (and might even make things worse). Furthermore, possible confounders could be that parents who are more involved in their children’s lives might both limit screen time and help develop better psychological well-being.\n\n\n\nSo when we observe a correlation between two variables, how can we determine whether there is a causal relationship between them? The gold standard for establishing causation is the controlled experiment, in which the researcher manipulates the cause and observes the effect, while keeping all other variables constant.\nSadly, it’s often not possible to conduct a controlled experiment in communication science.\nWhen it is not possible to conduct a controlled experiment, there is still hope. Using statistical methods, we can try to control for confounding variables, and establish whether the relationship between the variables is consistent with a causal model. This does not immediately prove causation, because there can always be other confounders, but it can support our confidence in the theory. In fact, many important causal relations in communication science have only been ‘supported’ this way, such as the relationship between media coverage and public opinion (Mccombs and Shaw 1972). And even common knowledge like “smoking causes cancer” relies heavily on evidence from statistical methods that control for confounders.\nSome researchers argue that causation should not be treated as a binary concept, but rather a matter of degree (pearl09?).\nk\nSo how can we test causal relations in communication science? The answer is that it’s very difficult to establish causation with certainty.",
    "crumbs": [
      "Statistics",
      "Causal relations"
    ]
  },
  {
    "objectID": "2_statistics/1_causal_models.html#correlation-versus-causation",
    "href": "2_statistics/1_causal_models.html#correlation-versus-causation",
    "title": "Causal relations",
    "section": "",
    "text": "Causation implies a cause-and-effect relationship, where a change in one variable (the cause) leads to a change in the other (the effect). If we were to somehow manipulate the cause, we would expect to see a corresponding change in the effect.\nFor instance, if we could somehow make the news media talk more about climate change, we would expect the public to think and talk about it more, too. Basically, this is why PR exists, and why politicians try to get media to talk about the issues they care about.\nCorrelation, on the other hand, indicates an association or relationship between two variables without establishing a direct cause-and-effect link. Perhaps there is a causal effect, but we don’t know which variable is the cause and which is the effect. But it could also be that there is a third variable that causes both of them to change. This is known as a confounding variable.\nIn the case of screen time and psychological well-being study, the authors did observe an “association”, but were not able to establish which variable causes the other. Based on their data, “it is not possible to determine if screen time leads to low well-being, low well-being leads to screen time, or both” (Twenge and Campbell 2018, 281). Indeed, it could be that children who are already unhappy end up spending more time on screens, which would mean that taking away their phone might not help (and might even make things worse). Furthermore, possible confounders could be that parents who are more involved in their children’s lives might both limit screen time and help develop better psychological well-being.",
    "crumbs": [
      "Statistics",
      "Causal relations"
    ]
  },
  {
    "objectID": "2_statistics/1_causal_models.html#deriving-hypotheses-from-theory",
    "href": "2_statistics/1_causal_models.html#deriving-hypotheses-from-theory",
    "title": "Testing causal relations",
    "section": "Deriving hypotheses from theory",
    "text": "Deriving hypotheses from theory\nThe first step in testing a theory is to derive a hypothesis from it. A hypothesis is a testable statement that predicts the relationship between two or more variables. for example,\nLet’s consider the theory that increased social media use leads to lower life satisfaction. This theory suggests that the more time individuals spend on social media, the more likely they are to experience negative emotions, such as envy or loneliness, which can ultimately reduce their overall life satisfaction.\nTo explore this theory, we could derive the following hypothesis: “Individuals who spend more time on social media platforms will report lower levels of life satisfaction compared to those who spend less time on social media.”\nOnce we have this hypothesis, the next step is to statistically test it. This could involve collecting data on the amount of time participants spend on social media and their self-reported life satisfaction levels. By analyzing the data, we can assess whether there is a statistically significant relationship between social media use and life satisfaction. If the results confirm the hypothesis, it would provide evidence in support of the theory; if not, the theory may need further examination or adjustment.\nDeriving and testing such hypotheses allows us to empirically evaluate the validity of communication theories, offering insights into the potential impacts of social media on well-being.",
    "crumbs": [
      "Statistics",
      "Testing causal relations"
    ]
  },
  {
    "objectID": "2_statistics/1_causal_models.html#correlation-versus-causation-1",
    "href": "2_statistics/1_causal_models.html#correlation-versus-causation-1",
    "title": "Testing causal relations",
    "section": "Correlation versus causation",
    "text": "Correlation versus causation\nWhen examining the relationship between variables, it is essential to distinguish between correlation and causation.",
    "crumbs": [
      "Statistics",
      "Testing causal relations"
    ]
  },
  {
    "objectID": "2_statistics/0_deriving_hypotheses.html#evidence-proof",
    "href": "2_statistics/0_deriving_hypotheses.html#evidence-proof",
    "title": "Deriving hypotheses",
    "section": "Evidence != proof",
    "text": "Evidence != proof\nIf an hypothesis is confirmed, it provides evidence in support of the theory. However, note that it does not necessarily prove the theory. For example, if we find that children who play violent video games are more aggressive, it does not prove that the games caused the aggression. Perhaps people who are already aggressive are more likely to play violent video games, or there is a confounding variable that causes both aggression and video game playing (e.g., gender, socioeconomic status, or parental involvement).\nHypotheses are thus the bridge between theory and statistical methods, and are essential for conducting rigorous research in communication science. They need to be specific enough to be testable, but close enough to the theory they are derived from to allow for meaningful conclusions.",
    "crumbs": [
      "Statistics",
      "Deriving hypotheses"
    ]
  },
  {
    "objectID": "2_statistics/0_deriving_hypotheses.html#evidence-neq-proof",
    "href": "2_statistics/0_deriving_hypotheses.html#evidence-neq-proof",
    "title": "Deriving hypotheses",
    "section": "",
    "text": "If an hypothesis is confirmed, it provides evidence in support of the theory. However, note that it does not necessarily prove the theory. For example, if we find that children who play violent video games are more aggressive, it does not prove that the games caused the aggression. Perhaps people who are already aggressive are more likely to play violent video games, or there is a confounding variable that causes both aggression and video game playing (e.g., gender, socioeconomic status, or parental involvement).\nHypotheses are thus the bridge between theory and statistical methods, and are essential for conducting rigorous research in communication science. They need to be specific enough to be testable, but close enough to the theory they are derived from to allow for meaningful conclusions.",
    "crumbs": [
      "Statistics",
      "Deriving hypotheses"
    ]
  },
  {
    "objectID": "2_statistics/1_causal_models.html#testing",
    "href": "2_statistics/1_causal_models.html#testing",
    "title": "Communication Science R Book",
    "section": "",
    "text": "In communication science we are often concerned with causal relations. Consider the following two examples:\n\nWhen news media focus heavily on certain issues, it enhances the public’s perception of their importance (Mccombs and Shaw 1972).\nMore screen time among children and adolescents is associated with lower psychological well-being (Twenge and Campbell 2018).\n\nResearch into such causal relations is important, because by understanding causal mechanisms, we can develop strategies to change the outcomes for the better. If more screen time is indeed associated with lower psychological well-being, we should advice parents to limit children’s screen time.\nBut before we incurr the wrath of our children by trying to take away their smartphones, we should first make sure that our theory is correct. As usual, we can derive hypotheses that predict relationships between variables, and then test these hypotheses with statistical methods.\nHowever, when testing causal relations, we need to be particularly careful, because correlation does not imply causation!",
    "crumbs": [
      "Statistics",
      "Testing"
    ]
  },
  {
    "objectID": "2_statistics/1_causal_models.html#testing-causal-relations",
    "href": "2_statistics/1_causal_models.html#testing-causal-relations",
    "title": "Testing",
    "section": "",
    "text": "So when we observe a correlation between two variables, how can we determine whether there is a causal relationship between them? The gold standard for establishing causation is the controlled experiment, in which the researcher manipulates the cause and observes the effect, while keeping all other variables constant.\nSadly, it’s often not possible to conduct a controlled experiment in communication science.\nWhen it is not possible to conduct a controlled experiment, there is still hope. Using statistical methods, we can try to control for confounding variables, and establish whether the relationship between the variables is consistent with a causal model. This does not immediately prove causation, because there can always be other confounders, but it can support our confidence in the theory. In fact, many important causal relations in communication science have only been ‘supported’ this way, such as the relationship between media coverage and public opinion (Mccombs and Shaw 1972). And even common knowledge like “smoking causes cancer” relies heavily on evidence from statistical methods that control for confounders.\nSome researchers argue that causation should not be treated as a binary concept, but rather a matter of degree (pearl09?).\nk\nSo how can we test causal relations in communication science? The answer is that it’s very difficult to establish causation with certainty.",
    "crumbs": [
      "Statistics",
      "Testing"
    ]
  },
  {
    "objectID": "2_statistics/0_hypotheses.html",
    "href": "2_statistics/0_hypotheses.html",
    "title": "Hypotheses",
    "section": "",
    "text": "In order to use statistical methods to test theories, we first need to derive hypotheses from them. A hypothesis is a testable statement, that when proven true or false, provides evidence for or against the theory.\nFor example, consider the theory that playing violent video games leads to increased aggression in children. There is much disagreement about whether this theory is correct (Griffiths 1999), so it’s important to test it empirically. In order to do so, we need to derive hypotheses that we can conduct to rigorous statistical tests. Some examples of hypotheses are:\n\nThere is a relation “between arcade game use and teachers’ ratings of impulsiveness and aggresiveness”. (Lin and Lepper 1987, 81)\nPlaying violent video games leads to higher levels of heart rate and blood pressure compared to playing non-violent video games. (Lynch 1994)\nThere will be a “linear increase in aggressive affect after playing nonaggressive, moderately aggresive, and highly aggressive games”. (Scott 1995, 125)\n\nNotice that these hypotheses are more specific than the theory they are derived from. There are multiple ways to define and measure “playing violent video games” and “increased aggression”, and it could also be that the relation only holds for certain groups of people, or only for short periods of time. Hypotheses help us make (different aspects of) the theory more explicit, so that we can conduct empirical tests.\n\n\nIf an hypothesis is confirmed, it provides evidence in support of the theory. However, note that it does not necessarily prove the theory. For example, if we find that children who play violent video games are more aggressive, it does not prove that the games caused the aggression. Perhaps people who are already aggressive are more likely to play violent video games, or there is a confounding variable that causes both aggression and video game playing (e.g., gender, socioeconomic status, or parental involvement).\nHypotheses are thus the bridge between theory and statistical methods, and are essential for conducting rigorous research in communication science. They need to be specific enough to be testable, but close enough to the theory they are derived from to allow for meaningful conclusions.",
    "crumbs": [
      "Statistics",
      "Hypotheses"
    ]
  },
  {
    "objectID": "2_statistics/0_hypotheses.html#evidence-neq-proof",
    "href": "2_statistics/0_hypotheses.html#evidence-neq-proof",
    "title": "Hypotheses",
    "section": "",
    "text": "If an hypothesis is confirmed, it provides evidence in support of the theory. However, note that it does not necessarily prove the theory. For example, if we find that children who play violent video games are more aggressive, it does not prove that the games caused the aggression. Perhaps people who are already aggressive are more likely to play violent video games, or there is a confounding variable that causes both aggression and video game playing (e.g., gender, socioeconomic status, or parental involvement).\nHypotheses are thus the bridge between theory and statistical methods, and are essential for conducting rigorous research in communication science. They need to be specific enough to be testable, but close enough to the theory they are derived from to allow for meaningful conclusions.",
    "crumbs": [
      "Statistics",
      "Hypotheses"
    ]
  },
  {
    "objectID": "2_statistics/1_causal_models.html#testing-causal-relations-1",
    "href": "2_statistics/1_causal_models.html#testing-causal-relations-1",
    "title": "Causal relations",
    "section": "",
    "text": "So when we observe a correlation between two variables, how can we determine whether there is a causal relationship between them? The gold standard for establishing causation is the controlled experiment, in which the researcher manipulates the cause and observes the effect, while keeping all other variables constant.\nSadly, it’s often not possible to conduct a controlled experiment in communication science.\nWhen it is not possible to conduct a controlled experiment, there is still hope. Using statistical methods, we can try to control for confounding variables, and establish whether the relationship between the variables is consistent with a causal model. This does not immediately prove causation, because there can always be other confounders, but it can support our confidence in the theory. In fact, many important causal relations in communication science have only been ‘supported’ this way, such as the relationship between media coverage and public opinion (Mccombs and Shaw 1972). And even common knowledge like “smoking causes cancer” relies heavily on evidence from statistical methods that control for confounders.\nSome researchers argue that causation should not be treated as a binary concept, but rather a matter of degree (pearl09?).\nk\nSo how can we test causal relations in communication science? The answer is that it’s very difficult to establish causation with certainty.",
    "crumbs": [
      "Statistics",
      "Causal relations"
    ]
  },
  {
    "objectID": "2_statistics/1_causality.html",
    "href": "2_statistics/1_causality.html",
    "title": "Causality",
    "section": "",
    "text": "xkcd: correlation",
    "crumbs": [
      "Statistics",
      "Causality"
    ]
  },
  {
    "objectID": "2_statistics/1_causality.html#correlation-versus-causation",
    "href": "2_statistics/1_causality.html#correlation-versus-causation",
    "title": "Causality",
    "section": "Correlation versus causation",
    "text": "Correlation versus causation\n\nCausation\nCausation implies a cause-and-effect relationship, where a change in one variable (the cause) leads to a change in the other (the effect). If we were to somehow manipulate the cause (which we could do in an experiment), we would expect to see a corresponding change in the effect.\nFor instance, if we could somehow make the news media talk more about climate change, we would expect the public to think and talk about it more, too. Basically, this is why PR exists, and why politicians try to get media to talk about the issues they care about.\n\n\nCorrelation\nCorrelation, on the other hand, merely indicates an association or relationship between two variables. When two variables are correlated, they tend to change together. This can be due to a causal relationship in either or both directions, but it could also be due to a third variable that causes both of them to change. This is known as a confounding variable, and a correlation that results from a confounding variable is called a spurious correlation.\nA famous example is that across European countries the number of storks is quite strongly correlated (\\(\\r = 0.62\\)) with the number of newborn babies. The confounder in this case is the size of the country: larger countries have more storks and more babies. So the correlation is not evidence for the folk theory that storks deliver babies.\nIn the case of screen time and psychological well-being study, the authors did observe an “association”, but were not able to establish which variable causes the other. Based on their data, “it is not possible to determine if screen time leads to low well-being, low well-being leads to screen time, or both” (Twenge and Campbell 2018, 281). Indeed, it could be that children who are already unhappy end up spending more time on screens, which would mean that taking away their phone might not help (and might even make things worse). Furthermore, possible confounders could be that parents who are more involved in their children’s lives might both limit screen time and help develop better psychological well-being.",
    "crumbs": [
      "Statistics",
      "Causality"
    ]
  },
  {
    "objectID": "2_statistics/1_causality.html#testing-causal-relations-1",
    "href": "2_statistics/1_causality.html#testing-causal-relations-1",
    "title": "Causality",
    "section": "",
    "text": "So when we observe a correlation between two variables, how can we determine whether there is a causal relationship between them? The gold standard for establishing causation is the controlled experiment, in which the researcher manipulates the cause and observes the effect, while keeping all other variables constant.\nSadly, it’s often not possible to conduct a controlled experiment in communication science.\nWhen it is not possible to conduct a controlled experiment, there is still hope. Using statistical methods, we can try to control for confounding variables, and establish whether the relationship between the variables is consistent with a causal model. This does not immediately prove causation, because there can always be other confounders, but it can support our confidence in the theory. In fact, many important causal relations in communication science have only been ‘supported’ this way, such as the relationship between media coverage and public opinion (Mccombs and Shaw 1972). And even common knowledge like “smoking causes cancer” relies heavily on evidence from statistical methods that control for confounders.\nSome researchers argue that causation should not be treated as a binary concept, but rather a matter of degree (pearl09?).\nk\nSo how can we test causal relations in communication science? The answer is that it’s very difficult to establish causation with certainty.",
    "crumbs": [
      "Statistics",
      "Causality"
    ]
  },
  {
    "objectID": "2_statistics/1_causality.html#how-to-establish-causality",
    "href": "2_statistics/1_causality.html#how-to-establish-causality",
    "title": "Causality",
    "section": "How to establish causality",
    "text": "How to establish causality\nSo when we observe a correlation between two variables, how can we determine whether there is a causal relationship between them?\n\nThe gold standard: controlled experiments\nThe gold standard for establishing causation is the randomized controlled experiment, in which the researcher manipulates the cause and observes the effect, while keeping all other variables constant. Since the treatment group and the control group are statistically identical before the treatment is applied, any difference in the outcome can be attributed to the treatment. Where possible, this is the best way to establish causation.\nHowever, this is often not possible or feasible in communication science. In those cases, where we only have observational data (e.g., surveys, interviews, content analysis), we therefore have to rely on special statistical methods to control for confounding variables and establish causation.\n\n\nThe next best thing: controlling for confounders\nUsing statistical methods, we can try to control for confounding variables. This is not perfect, because there can always be other confounders, but it does allow us to build evidence to support or refute our theories. In fact, many well supported causal theories in communication science have only been supported this way, such as the relationship between media coverage and public opinion (Mccombs and Shaw 1972). And even common knowledge like “smoking causes cancer” relies heavily on evidence from statistical methods that control for confounders. This makes statistical modeling of confounding variables an essential part of the communication scientist’s toolkit.",
    "crumbs": [
      "Statistics",
      "Causality"
    ]
  },
  {
    "objectID": "getting-started/index.html",
    "href": "getting-started/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "In the VU Communication Science track, you’ll be using the R statistical software as a key tool for learning quantitative methods and data analysis techniques. R is a powerful, open-source programming language that is widely used in academia and industry for statistical analysis, visualization, text analysis, and more.\nThrough our courses, you’ll get hands-on experience with R, allowing you to analyze real-world data, perform statistical tests, create insightful visualizations, and develop reproducible research workflows. These skills will be invaluable as you move forward in your studies and future career, where data-driven decision-making plays a crucial role.\nR is also open-source, and therefore free to use for both academic and commercial purposes. This means you can continue to use R beyond your studies without any licensing costs, giving you the flexibility to apply your skills in various professional settings. The open-source nature of R also fosters a vibrant community of users and contributors who regularly share packages, tutorials, and resources. This community support makes it easier to find help, learn new techniques, and stay updated with the latest developments in data science and statistical analysis.\nIn this Getting started section, we’ll walk you through your first steps. We will cover how to install R, how to install and use the RStudio software for working with R, and the basics of the R syntax.\n\n\n\n Back to top",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "data-management/index.html",
    "href": "data-management/index.html",
    "title": "Data management",
    "section": "",
    "text": "In this chapter we will introduce you to the basics of data management in R, using the powerful tidyverse framework. Data management is the process of importing, cleaning, transforming, and exporting data. This is an essential part of any data analysis project, where you will often have to deal with data that is messy, incomplete, or in the wrong format. Knowing your way around data management will save you a lot of time and frustration, and opens up new possibilities for analysis and visualization.\nOne of the greatest benefits of learning how to manage data in R is that it allows you to work with data in a structured and reproducible way. All the steps from importing the raw data to analyzing the final results are written in your script. If you made a mistake anywhere in the process, you can easily go back to fix it, and then rerun the script from the start. It is also common that you will need to rerun your analysis at a later time, for example when you get new data, or when someone (e.g., a client, a reviewer) asks you to make some changes. By the end of this chapter, you will be able to manage your data in a way that is transparent, reproducible, and efficient.",
    "crumbs": [
      "Data management"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html",
    "href": "data-management/0_tidyverse/6_strings.html",
    "title": "Textual data",
    "section": "",
    "text": "The goal of this tutorial is to get you acquainted with basic string handling in R. A large part of this uses the stringr included in the Tidyverse. See also chapter 14 of R for Data Science and the stringr cheat sheet\nNote that ‘string’ is not an official word in R (which uses character to denote textual data), but since it’s the word used in most documentations I will also use strings to refer to objects containing textual data. (AFAIK, the name originates from seeing a text as a string or sequence of characters)",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html#combining-strings",
    "href": "data-management/0_tidyverse/6_strings.html#combining-strings",
    "title": "Textual data",
    "section": "Combining strings",
    "text": "Combining strings\nTo combine two strings, you can use str_c (which is equivalent to built-in paste0):\n\nstr_c(\"john\", \"mary\")\n\n[1] \"johnmary\"\n\nstr_c(\"john\", \"mary\", sep = \" & \")\n\n[1] \"john & mary\"\n\n\nIt can also work of longer vectors, where shorter vectors are repeated as needed:\n\nnames = c(\"john\", \"mary\")\nstr_c(\"Hello, \", names)\n\n[1] \"Hello, john\" \"Hello, mary\"\n\n\nFinally, you can also ask it to collapse longer vectors after the initial pasting:\n\nstr_c(names, collapse=\" & \")\n\n[1] \"john & mary\"\n\nstr_c(\"Hello, \", names, collapse=\" and \")\n\n[1] \"Hello, john and Hello, mary\"",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html#subsetting-strings",
    "href": "data-management/0_tidyverse/6_strings.html#subsetting-strings",
    "title": "Textual data",
    "section": "Subsetting strings",
    "text": "Subsetting strings\nTo take a fixed subset of a string, you can use str_sub. This can be useful, for example, to strip the time part off dates:\n\ndates = c(\"2019-04-01T12:00\", \"2012-07-29 01:12\")\nstr_sub(dates, start = 1, end = 10)\n\n[1] \"2019-04-01\" \"2012-07-29\"\n\n\nYou can also replace a substring, for example to make sure the ‘T’ notation is used in the dates:\n\nstr_sub(dates, start=11, end=11) = \"T\"\ndates\n\n[1] \"2019-04-01T12:00\" \"2012-07-29T01:12\"",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html#finding-patterns",
    "href": "data-management/0_tidyverse/6_strings.html#finding-patterns",
    "title": "Textual data",
    "section": "Finding patterns",
    "text": "Finding patterns\nRegular expressions can be used e.g. to find rows containing a specific pattern. For example, if we had a data frame containing the earlier texts, we can filter for rows containing an email address:\n\nt = tibble(id=1:3, text=txt)\nt\n\n# A tibble: 3 × 2\n     id text                                 \n  &lt;int&gt; &lt;chr&gt;                                \n1     1 Hi, I'm Bob                          \n2     2 my email address  is  Bob@example.com\n3     3 A #hashtag for the #millenials       \n\nt |&gt; filter(str_detect(text, regex_email))\n\n# A tibble: 1 × 2\n     id text                                 \n  &lt;int&gt; &lt;chr&gt;                                \n1     2 my email address  is  Bob@example.com\n\n\nYou can also str_count to count how many matches of a pattern are found in each text:\n\nt |&gt; mutate(n_hashtags = str_count(text, \"#\\\\w+\"))\n\n# A tibble: 3 × 3\n     id text                                  n_hashtags\n  &lt;int&gt; &lt;chr&gt;                                      &lt;int&gt;\n1     1 Hi, I'm Bob                                    0\n2     2 my email address  is  Bob@example.com          0\n3     3 A #hashtag for the #millenials                 2",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html#replacing-patterns",
    "href": "data-management/0_tidyverse/6_strings.html#replacing-patterns",
    "title": "Textual data",
    "section": "Replacing patterns",
    "text": "Replacing patterns\nYou can also use regular expressions to do find-an-replace. For example, you can remove all punctionation, normalize whitespace, or redact email addresses:\n\nt |&gt; mutate(nopunct = str_replace_all(text, \"[^\\\\w ]\", \"\"),\n             normalized = str_replace_all(text, \"\\\\s+\", \" \"),\n             redacted = str_replace_all(text, \"\\\\w+@\", \"****@\"),\n             text = NULL)\n\n# A tibble: 3 × 4\n     id nopunct                             normalized                  redacted\n  &lt;int&gt; &lt;chr&gt;                               &lt;chr&gt;                       &lt;chr&gt;   \n1     1 Hi Im Bob                           Hi, I'm Bob                 Hi, I'm…\n2     2 my email address  is  Bobexamplecom my email address is Bob@ex… my emai…\n3     3 A hashtag for the millenials        A #hashtag for the #millen… A #hash…\n\n\nNote the use of setting text to NULL as an alternative way to drop a column. In textr, most functions have a _all variant which replaces/finds/extracts all matches, rather than just the first.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html#extracting-patterns",
    "href": "data-management/0_tidyverse/6_strings.html#extracting-patterns",
    "title": "Textual data",
    "section": "Extracting patterns",
    "text": "Extracting patterns\nBesides replacing patterns, it can also be useful to extract elements from a string, for example the email or hashtag:\n\nregex_email = regex(\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+\")\nt |&gt; mutate(email = str_extract(text, regex_email),\n             hashtag = str_extract(text, \"#\\\\w+\"))\n\n# A tibble: 3 × 4\n     id text                                  email           hashtag \n  &lt;int&gt; &lt;chr&gt;                                 &lt;chr&gt;           &lt;chr&gt;   \n1     1 Hi, I'm Bob                           &lt;NA&gt;            &lt;NA&gt;    \n2     2 my email address  is  Bob@example.com Bob@example.com &lt;NA&gt;    \n3     3 A #hashtag for the #millenials        &lt;NA&gt;            #hashtag\n\n\nNote that for the hashtag, it only extracted the first hit. You can use the str_extract_all function, but since it can match zero, once, or more often per text, it returns a list containing all matches per row (or more correctly, per element of the input vector):\n\nstr_extract_all(t$text, \"#\\\\w+\")\n\n[[1]]\ncharacter(0)\n\n[[2]]\ncharacter(0)\n\n[[3]]\n[1] \"#hashtag\"    \"#millenials\"\n\n\nThe best way to deal with this in the context of a data frame is to use unnest_longer to turn the list into a long format. First, use mutate to create a column containing the lists (so this is a column which itself contains complex data)\n\nhashtags = mutate(t, tags = str_extract_all(t$text, \"#\\\\w+\"))\nhashtags\n\n# A tibble: 3 × 3\n     id text                                  tags     \n  &lt;int&gt; &lt;chr&gt;                                 &lt;list&gt;   \n1     1 Hi, I'm Bob                           &lt;chr [0]&gt;\n2     2 my email address  is  Bob@example.com &lt;chr [0]&gt;\n3     3 A #hashtag for the #millenials        &lt;chr [2]&gt;\n\n\nAs you can see, the tags column has ‘list’ as its type, with the last row containing two elements. To turn this into a more usable dataframe, we ‘unnest’ it into a ‘long’ format using unnest_longer:\n\nhashtags |&gt; unnest_longer(\"tags\")\n\n# A tibble: 2 × 3\n     id text                           tags       \n  &lt;int&gt; &lt;chr&gt;                          &lt;chr&gt;      \n1     3 A #hashtag for the #millenials #hashtag   \n2     3 A #hashtag for the #millenials #millenials\n\n\nAs you can see, this creates a new data frame with one row per hash tag.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html#using-separate",
    "href": "data-management/0_tidyverse/6_strings.html#using-separate",
    "title": "Textual data",
    "section": "Using separate",
    "text": "Using separate\nFirst, if the number of elements is known and you can use multiple you can use separate to separate the column into seperate columns:\n\nd |&gt; separate(person, into=c(\"firstname\", \"lastname\"), sep=\" \")\n\n# A tibble: 2 × 3\n  firstname lastname books                                  \n  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                                  \n1 Liam      Jones    The great Gatsby, To kill a Mockingbird\n2 Olivia    Smith    Pride and Prejudice, 1984, Moby Dick   \n\n\nAs said, this is mostly useful if a column always contains a fixed number of data points that each have a distinct meaning, e.g. first and last name or city and state.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html#using-str_split",
    "href": "data-management/0_tidyverse/6_strings.html#using-str_split",
    "title": "Textual data",
    "section": "Using str_split",
    "text": "Using str_split\nIf there is variable number of data points such as the list of books in the data set above, you can use str_split, which takes a regular expression argument to split the column:\n\nd |&gt; mutate(books = str_split(books, pattern=\",\"))\n\n# A tibble: 2 × 2\n  person       books    \n  &lt;chr&gt;        &lt;list&gt;   \n1 Liam Jones   &lt;chr [2]&gt;\n2 Olivia Smith &lt;chr [3]&gt;\n\n\nJust like above, this produces a column of type ‘list’ which contains multiple values per row. To normalize this, use unnest_longer as above:\n\nd |&gt; mutate(books = str_split(books, pattern=\",\")) |&gt; unnest_longer(books)\n\n# A tibble: 5 × 2\n  person       books                   \n  &lt;chr&gt;        &lt;chr&gt;                   \n1 Liam Jones   \"The great Gatsby\"      \n2 Liam Jones   \" To kill a Mockingbird\"\n3 Olivia Smith \"Pride and Prejudice\"   \n4 Olivia Smith \" 1984\"                 \n5 Olivia Smith \" Moby Dick\"            \n\n\nAs you can also see, the spaces around the book titles are not removed. You can fix this in two ways, either by changing the pattern to incluide optional whitespace (\\\\w is white space, * means the preceding element is optional and can be repeated); or by adding a trimws call afterwards:\n\nd |&gt; mutate(books = str_split(books, pattern=\"\\\\s*,\\\\s*\")) |&gt; unnest_longer(books)\n\n# A tibble: 5 × 2\n  person       books                \n  &lt;chr&gt;        &lt;chr&gt;                \n1 Liam Jones   The great Gatsby     \n2 Liam Jones   To kill a Mockingbird\n3 Olivia Smith Pride and Prejudice  \n4 Olivia Smith 1984                 \n5 Olivia Smith Moby Dick            \n\nd |&gt; mutate(books = str_split(books, pattern=\",\")) |&gt; unnest_longer(books) |&gt; mutate(books=trimws(books))\n\n# A tibble: 5 × 2\n  person       books                \n  &lt;chr&gt;        &lt;chr&gt;                \n1 Liam Jones   The great Gatsby     \n2 Liam Jones   To kill a Mockingbird\n3 Olivia Smith Pride and Prejudice  \n4 Olivia Smith 1984                 \n5 Olivia Smith Moby Dick",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html#background",
    "href": "data-management/0_tidyverse/6_strings.html#background",
    "title": "Textual data",
    "section": "Background",
    "text": "Background\nA fairly short version of the story is as follows: when computers were mostly dealing with English text, life was easy, as there are not a lot of different letters and they could easily assign each letter and some punctuation marks to a number below 128, so it could be stored as 7 bits. For example, A is number 65. This encoding was called ‘ASCII’.\nIt turned out, however, that many people needed more than 26 letters, for example to write accented letters. For this reason, the 7 bits were expanded to 8, and many accented latin letters were added. This representation is called latin-1, also known as ISO-8859-1.\nOf course, many languages don’t use the latin script, so other 8-bit encodings were invented to deal with Cyrillic, Arabic, and other scripts. Most of these are based on ASCII, meaning that 65 still refers to ‘A’ in e.g. the Hebrew encoding. However, character 228 could refer to greek δ, cyrillic ф, or hebrew ה. Things get even more complex if you consider Chinese, where you can’t fit all characters in 256 numbers, so several larger (multi-byte) encodings were used.\nThis can cause a lot of confusion if you read a text that was encoding in e.g. greek as if it were encoded in Hebrew. A famous example of this confusion is that Microsoft Exchange used the WingDings font and encoding for rendering symbols in emails, amongst others using character 74 as a smiley. For non-exchange users (who didn’t have that font), however, it renders as the ASCII character nr 74: “J”. So, if you see an email from a microsoft user with a J where you expected a smiley, now you know :).\nTo end this confusion, unicode was invented, which assigns a unique number (called a code point) to each letter. A is still 65 (or “1” in hexadecimal R notataion), but δ is now uniquely “3B4”, and ф is uniquely “444”. There are over 1M possible unicode characters, of which about 100 thousand have been currently assigned. This gives enough room for Chinese, old Nordic runes, and even Klingon to be encoded.\nYou can directly use these in an R string:\n\n\"Some Unicode letters: \\u41 \\u03B4 \\u0444\"\n\n[1] \"Some Unicode letters: A δ ф\"\n\n\nNow, to be able to write all 1M characters to string, one would need almost 24 bits per character, tripling the storage and memory needed to handle most text. So, more efficient encodings were invented that would normally take only 8 or 16 bits per character, but can take more bits if needed. So, while the problem of defining characters is solved, unfortunately you still need to know the actual encoding of a text. Fortunately, UTF-8 (which uses 1 byte for latin characters, but more for non-western text) is emerging as a de facto standard for most texts. This is a compromise which is most efficient for latin alphabeters, but is still able to unambiguously express all languages.\nIt is still quite common, however, to encounter text in other encodings, so it can be good to understand what problems you can face and how to deal with them",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html#text-encoding-in-r",
    "href": "data-management/0_tidyverse/6_strings.html#text-encoding-in-r",
    "title": "Textual data",
    "section": "Text encoding in R",
    "text": "Text encoding in R\nTo show how this works in R, we can use the charToRaw function to see how a character is encoded in R:\n\ncharToRaw('A')\n\n[1] 41\n\n\nNote that the output of this function depends on your regional settings (called ‘locale’). On most computers, this should produce 41 however, as most encodings are based on ASCII.\nFor other alphabets it can be more tricky. The Chinese character “蘭” (unicode “62d”) on my computer is expressed in UTF-8, where it takes 3 bytes:",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/6_strings.html#dealing-with-encodings",
    "href": "data-management/0_tidyverse/6_strings.html#dealing-with-encodings",
    "title": "Textual data",
    "section": "Dealing with encodings",
    "text": "Dealing with encodings\nTo convert between encodings, you can use the iconv function. For example, to express the Chinese character above in GB2312 (Chinese national standard) encoding:\n\ncharToRaw(iconv('蘭', to='GB2312'))\n\n[1] 4e 41\n\n\nThe most common way of dealing with encodings is to ignore the problem and hope it goes away. However, outside the English world this is often not an option. Also, due to general unicode ignorance many people will use the wrong encoding, and you will even see things like double-utf8-encoded text.\nThe sane way to deal with encodings is to make sure that all text stored inside your program is encoded in a standard encoding, presumably UTF-8. This means that whenever you read text from an external source, you need to convert it to UTF-8 if it isn’t already in that form.\nThis means that when you use read_csv (on text data) or readtext, you should ideally always specify which encoding the text is encoded in:\n\nreadtext::readtext(\"file.txt\", encoding = \"utf-8\")\nread_csv(\"file.csv\", locale=locale(encoding='utf-8'))\n\nIf you don’t know what encoding a text is in, you can try utf-8 and the most common local encodings (e.g. latin-1 in many western countries), you can inspect the raw bytes, or you can use the guessEncoding function from readr:\n\nguess_encoding(\"file.txt\")",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/3_visualization.html",
    "href": "data-management/0_tidyverse/3_visualization.html",
    "title": "Visualization basics",
    "section": "",
    "text": "This tutorial teaches the basics of data visualization using the ggplot2 package (included in tidyverse). For more information, see R4DS Chapter 3: Da`ta Visualization and R4DS Chapter 7: Exploratory Data Analysis.\nFor many cool visualization examples using gplot2 (with R code included!) see the R Graph Gallery. For inspiration (but unfortunately no R code), there is also a 538 blog post on data visualization from 2016. Finally, see the article on ‘the grammar of graphics’ published by Hadley Wickham for more insight into the ideas behind ggplot.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Visualization basics"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/3_visualization.html#important-note-on-ggplot-command-syntax",
    "href": "data-management/0_tidyverse/3_visualization.html#important-note-on-ggplot-command-syntax",
    "title": "Visualization basics",
    "section": "Important note on ggplot command syntax",
    "text": "Important note on ggplot command syntax\nFor the plot to work, R needs to execute the whole ggplot call and all layers as a single statement. Practically, that means that if you combine a plot over multiple lines, the plus sign needs to be at the end of the line, so R knows more is coming. The general syntax is always:\n\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n\nSo, the following is good:\n\nggplot(data = facts_state) + \n  geom_point(mapping = aes(x = college, y = income))\n\n\n\n\n\n\n\n\nBut this is not:\n\nggplot(data = facts_state) \n  + geom_point(mapping = aes(x = college, y = income))\n\nAlso note that the data and mapping arguments are the first arguments the functions expect, so you can also leave them out:\n\nggplot(facts_state) + \n  geom_point(aes(x = college, y = income))",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Visualization basics"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/3_visualization.html#other-aesthetics",
    "href": "data-management/0_tidyverse/3_visualization.html#other-aesthetics",
    "title": "Visualization basics",
    "section": "Other aesthetics",
    "text": "Other aesthetics\nTo find out which visual elements can be used in a layer, use e.g. ?geom_point. According to the help file, we can (among others) set the colour, alpha (transparency), and size of points. Let’s first set the size of points to the (log) population of each state, creating a bubble plot:\n\nggplot(data = facts_state) + \n  geom_point(aes(x = college, y = income, size = population))\n\n\n\n\n\n\n\n\nSince it is difficult to see overlapping points, let’s make all points somewhat transparent. Note: Since we want to set the alpha of all points to a single value, this is not a mapping (as it is not mapped to a column from the data frame), but a constant. These are set outside the mapping argument:\n\nggplot(data = facts_state) + \n  geom_point(aes(x = college, y = income, size = population), \n             alpha = .5, \n             colour = \"red\")\n\n\n\n\n\n\n\n\nInstead of setting colour to a constant value, we can also let it vary with the data. For example, we can colour the states by percentage of population that is identified as ‘white’:\n\nggplot(data = facts_state) + \n  geom_point(aes(x=college, y=income, size=population, colour=white), \n             alpha=.9)\n\n\n\n\n\n\n\n\nFinally, you can map to a categorical value as well. Let’s categorize states into whether population is growing (at least 1%) or stable or declining. We use the if_else(condition, iftrue, iffalse) function, which assigns the iftrue value if the condition is true, and iffalse otherwise:\n\nfacts_state &lt;- facts_state %&gt;% \n  mutate(growth = ifelse(pop_change &gt; 1, \"Growing\", \"Stable\"))\n\nggplot(data=facts_state) + \n  geom_point(aes(x = college, y = income, size = population, colour = growth), \n             alpha=.9)\n\n\n\n\n\n\n\n\nAs you can see in these examples, ggplot tries to be smart about the mapping you ask. It automatically sets the x and y ranges to the values in your data. It mapped the size such that there are small and large points, but not e.g. a point so large that it would dominate the graph. For the colour, for interval variables it created a colour scale, while for a categorical variable it automatically assigned a colour to each group.\nOf course, each of those choices can be customized, and sometimes it makes a lot of sense to do so. For example, you might wish to use red for republicans and blue for democrats, if your audience is used to those colors; or you may wish to use grayscale for an old-fashioned paper publication. We’ll explore more options in a later tutorial, but for now let’s be happy that ggplot does a lot of work for us!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Visualization basics"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/3_visualization.html#setting-graph-options",
    "href": "data-management/0_tidyverse/3_visualization.html#setting-graph-options",
    "title": "Visualization basics",
    "section": "Setting graph options",
    "text": "Setting graph options\nSome options, like labels, legends, and the coordinate system are graph-wide rather than per layer. You add these options to the graph by adding extra functions to the call. For example, we can use coord_flip() to swap the x and y axes:\n\nggplot(nh_gop) + \n  geom_col(aes(x=candidate, y=votes)) +\n  coord_flip()\n\n\n\n\n\n\n\n\nYou can also reorder categories with the fct_reorder function, for example to sort by number of votes. Also, let’s add some colour (just because we can!):\n\nggplot(nh_gop) + \n  geom_bar(aes(x=fct_reorder(candidate, votes), y=votes, fill=candidate), \n           stat='identity') + \n  coord_flip()\n\n\n\n\n\n\n\n\n(Note: this works because ggplot assumes all labels are factors, which have an ordering; you can use other functions from the forcats package (generally starting with fct_) to do other things such as reversing the order, manually specifying the order, etc).\nThis is getting somewhere, but the y-axis label is not very pretty and we don’t need guides for the fill mapping. This can be remedied by more graph-level options. Also, we can use a theme to alter the appearance of the graph, for example using the minimal theme:\n\nggplot(nh_gop) + \n  geom_bar(aes(x=reorder(candidate, votes), y=votes, fill=candidate), \n           stat='identity') + \n  coord_flip() + \n  xlab(\"Candidate\") + \n  guides(fill=\"none\") + \n  theme_minimal()",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Visualization basics"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/3_visualization.html#grouped-bar-plots",
    "href": "data-management/0_tidyverse/3_visualization.html#grouped-bar-plots",
    "title": "Visualization basics",
    "section": "Grouped bar plots",
    "text": "Grouped bar plots\nWe can also add groups to bar plots. For example, we can set the x category to state (taking only NH and IA to keep the plot readable), and then group by candidate:\n\ngop2 &lt;- results_state %&gt;% \n  filter(party == \"Republican\" & (state == \"New Hampshire\" | state == \"Iowa\")) \nggplot(gop2) + geom_col(aes(x=state, y=votes, fill=candidate))\n\n\n\n\n\n\n\n\nBy default, the groups are stacked. This can be controlled with the position parameter, which can be dodge (for grouped bars) or fill (stacking to 100%): (note that the position is a constant, not an aesthetic mapping, so it goes outside the aes argument)\n\nggplot(gop2) + geom_col(aes(x=state, y=votes, fill=candidate), position='dodge')\n\n\n\n\n\n\n\nggplot(gop2) + geom_col(aes(x=state, y=votes, fill=candidate), position='fill')\n\n\n\n\n\n\n\n\nOf course, you can also make the grouped bars add up to 100% by computing the proportion manually, which can give you a bit more control over the process.\nNote that the example below pipes the preprocessing output directly into the ggplot command, that is, it doesn’t create a new temporary data set like gop2 above. This is entirely a stylistic choice, but can be useful for operations that are only intended for a single visualization.\n\ngop2 %&gt;% \n  group_by(state) %&gt;% \n  mutate(vote_prop=votes/sum(votes)) %&gt;%\n  ggplot() + \n    geom_col(aes(x=state, y=vote_prop, fill=candidate), position='dodge') + \n    ylab(\"Votes (%)\")\n\n\n\n\n\n\n\n\nNote that where group_by %&gt;% summarize replaces the data frame by a summarization, group_by %&gt;% mutate adds a column to the existing data frame, using the grouped values for e.g. sums. See our tutorial on Data Summarization for more details.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Visualization basics"
    ]
  },
  {
    "objectID": "getting-started/packages.html",
    "href": "getting-started/packages.html",
    "title": "Packages",
    "section": "",
    "text": "In R, a package is a collection of functions, data, and documentation that extends the capabilities of R. You can think of packages kind of like apps on your phone: they provide additional functionality that you can use to perform specific tasks. Also, like apps, you can install and uninstall packages as needed, directly from within R.\nThere are thousands of R packages available, which enables you to use almost any existing data analysis technique. R is not just a tool for statistical analysis, but also for data visualization, data collection, articifial intelligence, and much more. If there is anything you need to do, there is a good chance that someone has already written a package for it.",
    "crumbs": [
      "Getting Started",
      "Packages"
    ]
  },
  {
    "objectID": "getting-started/using-rstudio.html",
    "href": "getting-started/using-rstudio.html",
    "title": "How to use RStudio",
    "section": "",
    "text": "Once you have installed R and RStudio, you can start by launching RStudio. If everything was installed correctly, RStudio will automatically launch R as well.\nThe first time you open RStudio, you will likely see three separate windows. The first thing you want to do is open an R Script (!!) to work in. To do so, go to the toolbar and select File -&gt; New File -&gt; R Script.\nYou will now see four windows split evenly over the four corners of your screen:\nWhile you can directly enter code into your console (bottom-left), you should always work with R scripts (top-left). This allows you to keep track of what you are doing and save every step.",
    "crumbs": [
      "Getting Started",
      "How to use RStudio"
    ]
  },
  {
    "objectID": "getting-started/using-rstudio.html#running-code-from-the-r-script",
    "href": "getting-started/using-rstudio.html#running-code-from-the-r-script",
    "title": "How to use RStudio",
    "section": "Running code from the R script",
    "text": "Running code from the R script\nCopy and paste the following example code into your R Script. For now, don’t bother understanding the syntax itself. Just focus on running it.\n\n3 + 3\n2 * 5\n6 / 2\n\"some text\"\n\"some more text\"\nsum(1,2,3,4,5)\n\nYou can run code by selecting the code and clicking on the Run button in the toolbar. However, we highly recommend getting used to using the keyboard shortcut, because this will greatly speed up your process. On Windows (and Linux) the shortcut is Ctrl + Enter. On Mac it’s Command + Enter.\nThere are two ways to run code:\n\nIf you select a specific piece of code (so that it is highlighted) you can run this specific selection. For example, select the first three lines (the three mathematical operations) and press Ctrl + Enter. This should then print the results for these three mathematical expressions. Note that you can also select a specific part on a line. Try selecting just the second 3 on the first line. This should just print the number 3.\nIf you haven’t made a selection, but your text cursor is somewhere on a line in your editor, you can press Ctrl + Enter to run the line where the cursor is at. This will also move the cursor to the next line, so you can walk through the code from top to bottom, running each line. Try starting on the first line, and pressing Ctrl + Enter six times, to run each line separately.",
    "crumbs": [
      "Getting Started",
      "How to use RStudio"
    ]
  },
  {
    "objectID": "getting-started/using-rstudio.html#using-rstudio-projects",
    "href": "getting-started/using-rstudio.html#using-rstudio-projects",
    "title": "How to use RStudio",
    "section": "Using RStudio projects",
    "text": "Using RStudio projects\nIt is best to put all your code in an RStudio project. This is essentially a folder on your computer in which you can store the R files and data for a project that you are working on. While you do not necessarily need a project to work with R, they are very convenient, and we strongly recommend using them.\nTo create a new project, go to the top-right corner of your RStudio window. Look for the button labeled Project: (None). Click on this button, and select New Project. Follow the instructions to create a new directory with a new project. Name the project “R introduction”.\nNow, open a new R script and immediately save it (select File -&gt; Save in the toolbar, or press ctrl-s). Name the file my_first_r_script.r. In the bottom-right corner, under the Files tab, you’ll now see the file added to the project. The extension .r indicates that the file is an R script.",
    "crumbs": [
      "Getting Started",
      "How to use RStudio"
    ]
  },
  {
    "objectID": "getting-started/functions.html",
    "href": "getting-started/functions.html",
    "title": "Functions",
    "section": "",
    "text": "99% of what you do in R will involve using functions. A function in R is like a mini-program that you can use to perform specific tasks. It takes input, processes it, and gives you an output. For example, there are functions for:\n\nimporting data\ncomputing descriptive statistics\nperforming statistical tests\nvisualizing data\n\nA function in R has the form:\n\noutput &lt;- function_name(argument1, argument2, ...)`\n\n\nfunction_name is a name to indicate which function you want to use. It is followed by parentheses.\narguments are the input of the function, and are inserted within the parentheses.\noutput is anything that is returned by the function.\n\nFor example, the function c combines multiple values into a vector.\n\nx = c(1,2,3,4)\n\nNow, we can use the mean function to calculate the mean of these numbers:\n\nm &lt;- mean(x)\n\nThe calculated mean, 2.5, is now assigned to the name m:\n\nm\n\n[1] 2.5\n\n\n\n\nIn the c and mean functions above, all the arguments were required. To combine numbers into a vector, we needed to provide a list of numbers. To calculate a mean, we needed to provide a numeric vector.\nIn addition to the required arguments, a function can also have optional arguments, that give you more control over what a function does. For example, suppose we have a range of numbers that also contains a missing value. In R a missing value is called NA, which stands for Not Available:\n\nx_with_missing &lt;- c(1, 2, 3, NA, 4)\n\nNow, if we call the mean function, R will say that the mean is unknown, since the third value is unknown:\n\nmean(x_with_missing)\n\n[1] NA\n\n\nThis is statistically a very correct answer. But often, if some values happen to be missing in your data, you want to be able to calculate the mean just for the numbers that are not missing. Fortunately, the mean function has an optional argument na.rm (remove NAs) that you can set to TRUE (or to T, which is short for TRUE) to ignore the NAs:\n\nmean(x, na.rm=TRUE)\n\n[1] 2.5\n\n\nNotice that for the required argument, we directly provide the input x, but for the optional argument we include the argument name na.rm = TRUE. The reason is simply that there are other optional arguments, so we need to specify which one we’re using.\n\n\n\n\n\n\nHow do I know what arguments a function has?\n\n\n\n\n\nTo learn more about what a function does and what arguments it has, you can look it up in the ‘Help’ pane in the bottom right, or run ?function_name in R.\n\n?mean\n\nHere you can learn about the na.rm argument that we just used!\nIf you are just getting to know R, we recommend first finishing the rest of the Getting Started section. Then once you get the hang of things, have a look at the Use ?function help page tutorial.\n\n\n\n\n\n\nThere is another common way to use functions in R using the pipe syntax. With the pipe syntax, you can pipe the first argument into the function, instead of putting it inside the parentheses. As you will see below, this allows you to create a pipeline of functions, which is often easier to read. The pipe syntax uses the |&gt; operator, which is used as follows:\n\nargument1 |&gt; function_name(argument2, ...)\n\nFor example, the following two lines of code give identical output:\n\nmean(x_with_missing, na.rm=T)\n\n[1] 2.5\n\nx_with_missing |&gt; mean(na.rm=T)\n\n[1] 2.5\n\n\nNotice how our first argument, the required argument x_with_missing, is piped into the mean function. Inside the mean function, we only specify the second argument, the optional argument na.rm.\nSo why do we need this alternative way of doing the same thing? The reason is that when writing code, you shouldn’t just think about what the code does, but also about how easy the code is to read. This not only helps you prevent mistakes, but also makes your analysis transparent. As you’ll see later, you’ll encounter many cases where your analysis requires you to string together multiple functions. In these cases, pipes make your code much easier to read.\nFor example, imagine we would want to round the result (2.5) up to a round number (3). With the pipe syntax we can just add the round function to our pipeline.\n\nx_with_missing |&gt; \n  mean(na.rm=T) |&gt; \n  round()\n\nYou’ll see how powerful this can be later on, especially in the Data Management chapter. In order to prepare and clean up your data, you’ll often need to perform a series of functions in a specific order. The pipe syntax allows you to do this in a very readable way.",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "getting-started/functions.html#optional-arguments",
    "href": "getting-started/functions.html#optional-arguments",
    "title": "Functions",
    "section": "",
    "text": "In the c and mean functions above, all the arguments were required. To combine numbers into a vector, we needed to provide a list of numbers. To calculate a mean, we needed to provide a numeric vector.\nIn addition to the required arguments, a function can also have optional arguments, that give you more control over what a function does. For example, suppose we have a range of numbers that also contains a missing value. In R a missing value is called NA, which stands for Not Available:\n\nx_with_missing &lt;- c(1, 2, 3, NA, 4)\n\nNow, if we call the mean function, R will say that the mean is unknown, since the third value is unknown:\n\nmean(x_with_missing)\n\n[1] NA\n\n\nThis is statistically a very correct answer. But often, if some values happen to be missing in your data, you want to be able to calculate the mean just for the numbers that are not missing. Fortunately, the mean function has an optional argument na.rm (remove NAs) that you can set to TRUE (or to T, which is short for TRUE) to ignore the NAs:\n\nmean(x, na.rm=TRUE)\n\n[1] 2.5\n\n\nNotice that for the required argument, we directly provide the input x, but for the optional argument we include the argument name na.rm = TRUE. The reason is simply that there are other optional arguments, so we need to specify which one we’re using.\n\n\n\n\n\n\nHow do I know what arguments a function has?\n\n\n\n\n\nTo learn more about what a function does and what arguments it has, you can look it up in the ‘Help’ pane in the bottom right, or run ?function_name in R.\n\n?mean\n\nHere you can learn about the na.rm argument that we just used!\nIf you are just getting to know R, we recommend first finishing the rest of the Getting Started section. Then once you get the hang of things, have a look at the Use ?function help page tutorial.",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "getting-started/functions.html#using-pipes",
    "href": "getting-started/functions.html#using-pipes",
    "title": "Functions",
    "section": "Using pipes",
    "text": "Using pipes\nThere are two ways for using functions.\n\nThe first is the one shown above, where we put all the arguments between the parentheses: function_name(argument1, argument2, ...).\nThe second way is to pipe the first argument into the function: argument1 |&gt; function_name(argument2, ...)\n\nIf this is your first time seeing pipes, you’re probably wondering why you would want to do this? Why bother having two ways to do the exact same thing? The reason is that when writing code, you shouldn’t just think about what the code does, but also about how easy the code is to read. This not only helps you prevent mistakes, but also makes your analysis transparent.\nAs you’ll see later, you’ll encounter many cases where your analysis requires you to string together multiple functions. In these cases, pipes make your code much easier to read. let’s rewrite our code from above using the pipe notation:\n\nx_with_missing |&gt; mean(na.rm=T)\n\nNotice how our first argument, the required argument x_with_missing, is piped into the mean function. Inside the mean function, we only specify the second argument, the optional argument na.rm.\nNow imagine we would want to round the result (2.5) up to a round number (3). We can do this without pipe notation, but it would be quite ugly and hard to read:\n\nround(mean(x_with_missing, na.rm=T))\n\nThe pipe notation allows us to break this down into a nice pipeline:\n\nx_with_missing |&gt; \n  mean(na.rm=T) |&gt; \n  round()",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "getting-started/names-and-values.html",
    "href": "getting-started/names-and-values.html",
    "title": "Names and Objects",
    "section": "",
    "text": "In R, and in computer programming in general, the most essential operation is to assign objects to names. By object, we then broadly mean any piece of information. a single number, a text, a list of numbers, and even an entire data set. Assigning objects to names is essential, because it allows us to refer to them in our code.\nIn plain terms, assignment is how you make R remember things by assigning them to a name. To assign an object to a name, we use the arrow notation: name &lt;- value. For example:\nx &lt;- 2\nBy running the code x &lt;- 2, you are saying: Assign the value 2 to the name x. Any objects that you assigned to names are stored in your Environment. You can see this environment in the top-right window, under the Environment tab. If you assigned 2 to x, you should see a table with in the left column the names (x) and in the right column a description of the object. For simply objects like numbers, this will just be the value (2).\nFrom hereon, when you use the name x in your code, it will refer to the value 2. So when we run the code x * 5 (x times 5) it will print the number 10\nx * 5\n\n[1] 10",
    "crumbs": [
      "Getting Started",
      "Names and Objects"
    ]
  },
  {
    "objectID": "getting-started/names-and-values.html#assigning-different-types-of-values",
    "href": "getting-started/names-and-values.html#assigning-different-types-of-values",
    "title": "Names and Values",
    "section": "Assigning different types of values",
    "text": "Assigning different types of values\nYou can assign any type of value to a name, and you can use any name, as long as it starts with a letter and doesn’t contain spaces or symbols (but underscores are OK)\n\na_number = 5\nmy_cats_name = \"Hobbes\"\n\nIf you run this code and check you Environment (top-right), you should now see these name-value pairs added.",
    "crumbs": [
      "Getting Started",
      "Names and Values"
    ]
  },
  {
    "objectID": "getting-started/names-and-values.html#assigning-results",
    "href": "getting-started/names-and-values.html#assigning-results",
    "title": "Names and Objects",
    "section": "Assigning results",
    "text": "Assigning results\nTill now we only directly assigned objects to names. This is convenient, but the power of assignment really shines when you use it to store results. For example, we can also do this.\n\nx = 5 + 10\n\nThis a very simple example, but just think for a second what this allows us to do. Since we can assign anything to a name, we can break down any complicated procedure into multiple steps! For now, the key lesson is just to wrap your head around the syntax for assigning objects to names. This is fundamental to everything you will be doing in R (and in programming in general).",
    "crumbs": [
      "Getting Started",
      "Names and Objects"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/index.html",
    "href": "data-management/0_tidyverse/index.html",
    "title": "The Tidyverse Toolkit",
    "section": "",
    "text": "The tidyverse versus base R\n\n\n\n\n\nMany of the things that the tidyverse allows you to do are also possible in base R (i.e. the basic installation of R). Base R also provides functions for importing, managing and visualizing data. So why do we need the tidyverse?\nThe tidyverse is an opinionated framework, which means that it doesn’t just enable you to do things, but also suggests how you should do things. The authors have thought long and hard about how to make data management easy, effective and intuitive (they have even written papers about it). This not only makes the tidyverse much easier and intuitive to learn, but also makes sure everyone writes their code in the same way, which improves transparency and shareability.\nThis is different from base R, which is designed to be a highly flexible programming language, that allows you to do almost anything. Accordingly, it is still worthwhile to learn base R at some point if you want to specialize more in computational research methods. But for our Communication Science program, and for many data science applications in general, you can do all your data management in the tidyverse.\nThe tidyverse is collection of R packages that makes it much easier to import, manage and visualize data. To use the tidyverse, you only need to open the tidyverse package, and it will automatically open all of the tidyverse R packages.\nLike any normal package, you need to first install it once:\ninstall.packages('tidyverse')\nThen in every script that you use the package, you open it with library:\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/index.html#what-packages-does-the-tidyverse-contain",
    "href": "data-management/0_tidyverse/index.html#what-packages-does-the-tidyverse-contain",
    "title": "The Tidyverse Toolkit",
    "section": "What packages does the tidyverse contain?",
    "text": "What packages does the tidyverse contain?\nNotice above that when you run library(tidyverse), R prints all the tidyverse packages that it opened for you. Some of the most important ones that we’ll we using are:\n\ntibble. An optimized way for structuring rectangular data (basically: a spreadsheet of rows and columns)\ndplyr. Functions for manipulating tibbles: select and rename columns, filter rows, mutate values, etc.\nreadr. Read data into R.\nggplot2. One of the best visualization tools out there. Check out the gallery\n\n\n\n\n\n\n\nWhat about the ‘Conflicts’?\n\n\n\n\n\nWhen opening the tidyverse, and when opening packages in general, you can get a Conflicts warning. A very common warning for the tidyverse is:\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nSo what does this mean, and should we be worried?\nSince anyone can write new packages for R, it can happen that two packages provide functions with the same name. In this example, we see that the filter function exists in both the dplyr package (which we opened by opening the tidyverse), and in the stats package (which is included in base R). So now R needs to decide which version of the function to use when you type filter(). In this case, it says that the dplyr::filter() masks stats::filter(), meaning that it will now use the dplyr version.\nIn practice, this will rarely be a problem, because you seldom need two versions of a function in the same script. But if you ever do, there is a simple solution. Instead of just using filter(), you can type dplyr::filter() to specifically use this version. In the following code, we use this notation to specifically open the help page for dplyr::filter and stats::filter.\n\n?dplyr::filter()\n?stats::filter()",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/4_reshaping.html",
    "href": "data-management/0_tidyverse/4_reshaping.html",
    "title": "Long and Wide data",
    "section": "",
    "text": "This tutorial discusses how to reshape data, particularly from long to wide format and vice versa. It mostly follows Chapter 12 of the R4DS book, but uses the pivot_longer and pivot_wider functions that replace gather and spread1. At the time of writing these functions are not yet in the book, but the writers explain the change and the new functions here.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/4_reshaping.html#pivot-longer-wide-to-long",
    "href": "data-management/0_tidyverse/4_reshaping.html#pivot-longer-wide-to-long",
    "title": "Long and Wide data",
    "section": "Pivot longer (wide to long)",
    "text": "Pivot longer (wide to long)\nWe will tidy this data in three steps. First, we pivot_longer the columns into a single column with all measurements. Then, we separate the country from the measurement level. Finally, we pivot_wider the measurement levels to columns again (since they are measurements on the same observation).\nThe first step is the same as above: we gather all columns except for the year column into a single column:\n\nwealth = pivot_longer(wealth_raw, -Year, names_to=\"key\", values_to=\"value\")\nwealth\n\n# A tibble: 315 × 3\n    Year key                             value\n   &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt;\n 1  1810 France: top decile              0.799\n 2  1810 France: top percentile          0.456\n 3  1810 France: top promille            0.171\n 4  1810 Paris: top percentile           0.537\n 5  1810 United Kingdom: top decile      0.829\n 6  1810 United Kingdom: top percentile  0.549\n 7  1810 United States: top decile       0.58 \n 8  1810 United States: top percentile   0.25 \n 9  1810 United States: top promille    NA    \n10  1810 Sweden: top decile              0.839\n# ℹ 305 more rows",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/4_reshaping.html#separating-columns-splitting-one-column-into-two",
    "href": "data-management/0_tidyverse/4_reshaping.html#separating-columns-splitting-one-column-into-two",
    "title": "Long and Wide data",
    "section": "Separating columns (splitting one column into two)",
    "text": "Separating columns (splitting one column into two)\nThe next step is to split the ‘key’ column into two columns, for country and for measurement. This can be done using the separate command, for which you specify the column to split, the new column names, and what separator to split on:\n\nwealth = separate(wealth, key, into = c(\"country\",\"measurement\"), sep=\":\")\nwealth\n\n# A tibble: 315 × 4\n    Year country        measurement        value\n   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;              &lt;dbl&gt;\n 1  1810 France         \" top decile\"      0.799\n 2  1810 France         \" top percentile\"  0.456\n 3  1810 France         \" top promille\"    0.171\n 4  1810 Paris          \" top percentile\"  0.537\n 5  1810 United Kingdom \" top decile\"      0.829\n 6  1810 United Kingdom \" top percentile\"  0.549\n 7  1810 United States  \" top decile\"      0.58 \n 8  1810 United States  \" top percentile\"  0.25 \n 9  1810 United States  \" top promille\"   NA    \n10  1810 Sweden         \" top decile\"      0.839\n# ℹ 305 more rows\n\n\nThe measurement column is quoted in the output because it stars with a space. We could resolve this by specifying sep=\": \" (i.e. adding the space to the separator). We can also solve this by changing the column after the split with mutate. The code below removes the space using the trimws (trim white space) function:\n\nwealth %&gt;% mutate(measurement = trimws(measurement))\n\n# A tibble: 315 × 4\n    Year country        measurement     value\n   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt;\n 1  1810 France         top decile      0.799\n 2  1810 France         top percentile  0.456\n 3  1810 France         top promille    0.171\n 4  1810 Paris          top percentile  0.537\n 5  1810 United Kingdom top decile      0.829\n 6  1810 United Kingdom top percentile  0.549\n 7  1810 United States  top decile      0.58 \n 8  1810 United States  top percentile  0.25 \n 9  1810 United States  top promille   NA    \n10  1810 Sweden         top decile      0.839\n# ℹ 305 more rows\n\n\nWe can also use sub to search and replace (substitute) within a column, in this case changing ” top ” into “capital_top_”:\n\nwealth = wealth %&gt;% mutate(measurement = sub(\" top \", \"capital_top_\", measurement))\nwealth\n\n# A tibble: 315 × 4\n    Year country        measurement             value\n   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;                   &lt;dbl&gt;\n 1  1810 France         capital_top_decile      0.799\n 2  1810 France         capital_top_percentile  0.456\n 3  1810 France         capital_top_promille    0.171\n 4  1810 Paris          capital_top_percentile  0.537\n 5  1810 United Kingdom capital_top_decile      0.829\n 6  1810 United Kingdom capital_top_percentile  0.549\n 7  1810 United States  capital_top_decile      0.58 \n 8  1810 United States  capital_top_percentile  0.25 \n 9  1810 United States  capital_top_promille   NA    \n10  1810 Sweden         capital_top_decile      0.839\n# ℹ 305 more rows",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/4_reshaping.html#pivot-wider-long-to-wide",
    "href": "data-management/0_tidyverse/4_reshaping.html#pivot-wider-long-to-wide",
    "title": "Long and Wide data",
    "section": "Pivot wider (long to wide)",
    "text": "Pivot wider (long to wide)\nThe wealth data above is now ‘too long’ to be tidy: the measurement for each country is spread over multiple rows, listing the three different measurement levels (decile, percentile, promille). In effect, we want to undo one level of gathering, by spreading the column over multiple columns.\nThs syntax for the spread call is similar to that for pivot_longer: pivot_wider(data, names_from=key_column, values_from=value_column). Before we had the arguments names_to and values_to, to specify the column names of the new stacked (i.e. long format) columns. This time, we have the names_from and values_from arguments to reverse the process. For each unique value in the names_from column a new column will be created, with the corresponding value in the values_from column in the cell.\n\nwealth = pivot_wider(wealth, names_from=measurement, values_from=value)\nwealth\n\n# A tibble: 126 × 5\n    Year country  capital_top_decile capital_top_percentile capital_top_promille\n   &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;                &lt;dbl&gt;\n 1  1810 France                0.799                  0.456                0.171\n 2  1810 Paris                NA                      0.537               NA    \n 3  1810 United …              0.829                  0.549               NA    \n 4  1810 United …              0.58                   0.25                NA    \n 5  1810 Sweden                0.839                  0.559               NA    \n 6  1810 Europe                0.822                  0.521               NA    \n 7  1820 France                0.818                  0.467                0.190\n 8  1820 Paris                NA                      0.590               NA    \n 9  1820 United …             NA                     NA                   NA    \n10  1820 United …             NA                     NA                   NA    \n# ℹ 116 more rows\n\n\nSo now each row contains three measurements (columns, variables) relating to each observation (country x year).",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/4_reshaping.html#tidyness-as-a-matter-of-perception",
    "href": "data-management/0_tidyverse/4_reshaping.html#tidyness-as-a-matter-of-perception",
    "title": "Long and Wide data",
    "section": "Tidyness as a matter of perception",
    "text": "Tidyness as a matter of perception\nAs a last exercise, suppose we would like to plot wealth and capital inequality in the same figure as separate lines. You can do this with two separate geom_line commands, and e.g. use a dashed line for income inequality:\n\nggplot(inequality) + geom_line(aes(x=Year, y=capital_top_decile, colour=country)) + \n  geom_line(aes(x=Year, y=income_topdecile, colour=country), linetype=\"dashed\")\n\nWarning: Removed 4 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\nThis works, but it would be nice if we could specify the measurement as colour (or type) and have ggplot automatically make the legend. To do this, the different measurements need to be in rows rather than in columns. In other words, data that is tidy from one perspective can be ‘too wide’ for another.\nLet’s gather the data into a single column, and plot the result for the US:\n\ninequality2 = pivot_longer(inequality, income_topdecile:capital_top_promille, names_to=\"measurement\", values_to=\"value\")\n\ninequality2 %&gt;% \n  filter(country==\"US\") %&gt;% \n  ggplot() + geom_line(aes(x=Year, y=value, linetype=measurement))\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\nWe can also plot only top-decile capital and income in a paneled plot. Note the use of extra options to set legend location and title, vertical label, and main title text and location (horizontal justification):\n\ninequality2 %&gt;% \n  filter(measurement %in% c(\"income_topdecile\", \"capital_top_decile\") & country != \"Europe\") %&gt;% \n  ggplot() + geom_line(aes(x=Year, y=value, linetype=measurement)) + facet_wrap(~ country, nrow = 2) +\n  scale_linetype_discrete(name=\"Variable:\", labels=c(\"Capital\", \"Income\")) +\n  theme(legend.position=\"bottom\", plot.title = element_text(hjust = 0.5)) +\n  ylab(\"Inequality\") + \n  ggtitle(\"Capital and income inequality over time\")",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/4_reshaping.html#footnotes",
    "href": "data-management/0_tidyverse/4_reshaping.html#footnotes",
    "title": "Long and Wide data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe replacement of spread and gather with pivot_wider and pivot_longer is a recent change, so you might still see spread and gather used in code from other. As such, it is still usefull to have a look at how spread and gather work (which is very similar to pivot_wider and pivot_longer). However, make sure to use the new pivot_ functions in your own code, because spread and gather are on their way out.↩︎",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/5_joining.html",
    "href": "data-management/0_tidyverse/5_joining.html",
    "title": "Joining data",
    "section": "",
    "text": "In many cases, you need to combine data from multiple data sources. For example, you can combine a sentiment analysis of tweets with metadata about the tweets; or data on election results with data about the candidates ideological positions or details on the races.\nThis tutorial will teach you the inner_join and other _join commands used to combine two data sets on shared columns. See R4DS Chapter 13: Relational Data for more information and examples.\n\n\nFor this tutorial, we will look at data describing the US presidential primaries. These data can be downloaded from the Houston Data Visualisation github page, who in turn got it from Kaggle.\nIn the CSV folder on the github, you can find (among others)\n\nprimary_results.csv Number of votes in the primary per county per candidate\nprimary_schedule.csv Dates of each primary per state and per party\ncounty_facts.csv Information about the counties and states, including population, ethnicity, age, etc.\n\nFor many research questions, we need to be able to combine the data from these files. For example, we might want to know if Clinton did better in counties or states with more women (needing results and facts), or how Trump’s performance evolved over time (requiring results and calendar).\n\n\n\nBefore we start, let’s download the three data files:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncsv_folder_url &lt;- \"https://raw.githubusercontent.com/houstondatavis/data-jam-august-2016/master/csv\"\nresults &lt;- read_csv(paste(csv_folder_url, \"primary_results.csv\", sep = \"/\"))\n\nRows: 24611 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): state, state_abbreviation, county, party, candidate\ndbl (3): fips, votes, fraction_votes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfacts &lt;- read_csv(paste(csv_folder_url, \"county_facts.csv\", sep = \"/\"))\n\nRows: 3195 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): area_name, state_abbreviation\ndbl (52): fips, Pop_2014_count, Pop_2010_base_count, Pop_change_pct, Pop_201...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nschedule  &lt;- read_csv(paste(csv_folder_url, \"primary_schedule.csv\", sep = \"/\"))\n\nRows: 113 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): date, state, party, type\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote: I use paste to join the base url with the filenames, using a / as a separator.\nHave a look at all three data sets. Before we proceed, there are some things we want to do. First, the facts data frame is really large, with 54 columns. Let’s select a couple interesting ones to work with:\n\nfacts_subset &lt;- facts %&gt;% \n  select(area_name, \n         population = Pop_2014_count, \n         pop_change = Pop_change_pct, \n         over65 = Age_over_65_pct, \n         female = Sex_female_pct, \n         white = Race_white_pct, \n         college = Pop_college_grad_pct, \n         income = Income_per_capita)\n\nNext, the schedule dates are now a character (textual) field rather than date, so let’s fix that using the as.Date function, specifying the dates to be formatted as month/day/year:\n\nschedule &lt;- schedule %&gt;% \n  mutate(date = as.Date(date, format=\"%m/%d/%y\"))\n\nLast, let’s create a data set with per-state (rather than per-country) election results using group_by and summarize:\n\nresults_state &lt;- results %&gt;% \n  group_by(state, party, candidate) %&gt;% \n  summarize(votes = sum(votes))\n\n`summarise()` has grouped output by 'state', 'party'. You can override using\nthe `.groups` argument.\n\nresults_state\n\n# A tibble: 290 × 4\n# Groups:   state, party [95]\n   state   party      candidate        votes\n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 Alabama Democrat   Bernie Sanders   76399\n 2 Alabama Democrat   Hillary Clinton 309928\n 3 Alabama Republican Ben Carson       87517\n 4 Alabama Republican Donald Trump    371735\n 5 Alabama Republican John Kasich      37970\n 6 Alabama Republican Marco Rubio     159802\n 7 Alabama Republican Ted Cruz        180608\n 8 Alaska  Democrat   Bernie Sanders     440\n 9 Alaska  Democrat   Hillary Clinton     99\n10 Alaska  Republican Ben Carson        2401\n# ℹ 280 more rows\n\n\nNote: see R-tidy-5-transformations if you are unsure about the transformations above!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Joining data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/5_joining.html#data",
    "href": "data-management/0_tidyverse/5_joining.html#data",
    "title": "Joining data",
    "section": "",
    "text": "For this tutorial, we will look at data describing the US presidential primaries. These data can be downloaded from the Houston Data Visualisation github page, who in turn got it from Kaggle.\nIn the CSV folder on the github, you can find (among others)\n\nprimary_results.csv Number of votes in the primary per county per candidate\nprimary_schedule.csv Dates of each primary per state and per party\ncounty_facts.csv Information about the counties and states, including population, ethnicity, age, etc.\n\nFor many research questions, we need to be able to combine the data from these files. For example, we might want to know if Clinton did better in counties or states with more women (needing results and facts), or how Trump’s performance evolved over time (requiring results and calendar).",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Joining data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/5_joining.html#downloading-and-preparing-the-data",
    "href": "data-management/0_tidyverse/5_joining.html#downloading-and-preparing-the-data",
    "title": "Joining data",
    "section": "",
    "text": "Before we start, let’s download the three data files:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncsv_folder_url &lt;- \"https://raw.githubusercontent.com/houstondatavis/data-jam-august-2016/master/csv\"\nresults &lt;- read_csv(paste(csv_folder_url, \"primary_results.csv\", sep = \"/\"))\n\nRows: 24611 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): state, state_abbreviation, county, party, candidate\ndbl (3): fips, votes, fraction_votes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfacts &lt;- read_csv(paste(csv_folder_url, \"county_facts.csv\", sep = \"/\"))\n\nRows: 3195 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): area_name, state_abbreviation\ndbl (52): fips, Pop_2014_count, Pop_2010_base_count, Pop_change_pct, Pop_201...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nschedule  &lt;- read_csv(paste(csv_folder_url, \"primary_schedule.csv\", sep = \"/\"))\n\nRows: 113 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): date, state, party, type\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote: I use paste to join the base url with the filenames, using a / as a separator.\nHave a look at all three data sets. Before we proceed, there are some things we want to do. First, the facts data frame is really large, with 54 columns. Let’s select a couple interesting ones to work with:\n\nfacts_subset &lt;- facts %&gt;% \n  select(area_name, \n         population = Pop_2014_count, \n         pop_change = Pop_change_pct, \n         over65 = Age_over_65_pct, \n         female = Sex_female_pct, \n         white = Race_white_pct, \n         college = Pop_college_grad_pct, \n         income = Income_per_capita)\n\nNext, the schedule dates are now a character (textual) field rather than date, so let’s fix that using the as.Date function, specifying the dates to be formatted as month/day/year:\n\nschedule &lt;- schedule %&gt;% \n  mutate(date = as.Date(date, format=\"%m/%d/%y\"))\n\nLast, let’s create a data set with per-state (rather than per-country) election results using group_by and summarize:\n\nresults_state &lt;- results %&gt;% \n  group_by(state, party, candidate) %&gt;% \n  summarize(votes = sum(votes))\n\n`summarise()` has grouped output by 'state', 'party'. You can override using\nthe `.groups` argument.\n\nresults_state\n\n# A tibble: 290 × 4\n# Groups:   state, party [95]\n   state   party      candidate        votes\n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 Alabama Democrat   Bernie Sanders   76399\n 2 Alabama Democrat   Hillary Clinton 309928\n 3 Alabama Republican Ben Carson       87517\n 4 Alabama Republican Donald Trump    371735\n 5 Alabama Republican John Kasich      37970\n 6 Alabama Republican Marco Rubio     159802\n 7 Alabama Republican Ted Cruz        180608\n 8 Alaska  Democrat   Bernie Sanders     440\n 9 Alaska  Democrat   Hillary Clinton     99\n10 Alaska  Republican Ben Carson        2401\n# ℹ 280 more rows\n\n\nNote: see R-tidy-5-transformations if you are unsure about the transformations above!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Joining data"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html",
    "href": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html",
    "title": "Data transformation",
    "section": "",
    "text": "The goal of this tutorial is to get you acquainted with the Tidyverse. Tidyverse is a collection of packages that have been designed around a singular and clearly defined set of principles about what data should look like and how we should work with it. It comes with a nice introduction in the R for Data Science book, for which the digital version is available for free. This tutorial deals with most of the material in chapter 5 of that book.\nIn this part of the tutorial, we’ll focus on working with data using the tidyverse package. This package includes the dplyr (data-pliers) packages, which contains most of the tools we’re using below, but it also contains functions for reading, analysing and visualising data that will be explained later.\n\n\nAs before, install.packages() is used to download and install the package (you only need to do this once on your computer) and library() is used to make the functions from this package available for use (required each session that you use the package).\n\ninstall.packages('tidyverse') # only needed once\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNote: don’t be scared if you see a red message after calling library. RStudio doesn’t see the difference between messages, warnings, and errors, so it displays all three in red. You need to read the message, and it will contain the word ‘error’ if there is an error, such as a misspelled package:\n\nlibrary(tidyvers) # this will cause an error!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#installing-tidyverse",
    "href": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#installing-tidyverse",
    "title": "Data transformation",
    "section": "",
    "text": "As before, install.packages() is used to download and install the package (you only need to do this once on your computer) and library() is used to make the functions from this package available for use (required each session that you use the package).\n\ninstall.packages('tidyverse') # only needed once\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNote: don’t be scared if you see a red message after calling library. RStudio doesn’t see the difference between messages, warnings, and errors, so it displays all three in red. You need to read the message, and it will contain the word ‘error’ if there is an error, such as a misspelled package:\n\nlibrary(tidyvers) # this will cause an error!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#reading-data-read_csv",
    "href": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#reading-data-read_csv",
    "title": "Data transformation",
    "section": "Reading data: read_csv",
    "text": "Reading data: read_csv\nThe example above manually created a data set, but in most cases you will start with data that you get from elsewhere, such as a csv file (e.g. downloaded from an online dataset or exported from excel) or an SPSS or Stata data file.\nTidyverse contains a function read_csv that allows you to read a csv file directly into a data frame. You specify the location of the file, either on your local drive or directly from the Internet!\nThe example below downloads an overview of gun polls from the data analytics site 538, and reads it into a tibble using the read_csv function:\n\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/poll-quiz-guns/guns-polls.csv\"\nd &lt;- read_csv(url)\n\nRows: 57 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Question, Start, End, Pollster, Population, URL\ndbl (3): Support, Republican Support, Democratic Support\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nd\n\n# A tibble: 57 × 9\n   Question     Start   End     Pollster Population Support `Republican Support`\n   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n 1 age-21       2/20/18 2/23/18 CNN/SSRS Registere…      72                   61\n 2 age-21       2/27/18 2/28/18 NPR/Ips… Adults          82                   72\n 3 age-21       3/1/18  3/4/18  Rasmuss… Adults          67                   59\n 4 age-21       2/22/18 2/26/18 Harris … Registere…      84                   77\n 5 age-21       3/3/18  3/5/18  Quinnip… Registere…      78                   63\n 6 age-21       3/4/18  3/6/18  YouGov   Registere…      72                   65\n 7 age-21       3/1/18  3/5/18  Morning… Registere…      76                   72\n 8 arm-teachers 2/23/18 2/25/18 YouGov/… Registere…      41                   69\n 9 arm-teachers 2/20/18 2/23/18 CBS News Adults          44                   68\n10 arm-teachers 2/27/18 2/28/18 Rasmuss… Adults          43                   71\n# ℹ 47 more rows\n# ℹ 2 more variables: `Democratic Support` &lt;dbl&gt;, URL &lt;chr&gt;\n\n\n(Note that you can safely ignore the (red) message, they simply tell you how each column was parsed)\nThe shows the first ten rows of the data set, and if the columns don’t fit they are not printed. The remaining rows and columns are printed at the bottom. For each column the data type is also mentioned ( stands for integer, which is a numeric value;  is textual or character data). If you want to browse through your data, you can also click on the name of the data.frame (d) in the top-right window “Environment” tab or call View(d).",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#subsetting-with-filter",
    "href": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#subsetting-with-filter",
    "title": "Data transformation",
    "section": "Subsetting with filter()",
    "text": "Subsetting with filter()\nThe filter function can be used to select a subset of rows. In the guns data, the Question column specifies which question was asked. We can select only those rows (polls) that asked whether the minimum purchage age for guns should be raised to 21:\n\nage21 &lt;- filter(d, Question == 'age-21')\nage21\n\n# A tibble: 7 × 9\n  Question Start   End     Pollster      Population Support `Republican Support`\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n1 age-21   2/20/18 2/23/18 CNN/SSRS      Registere…      72                   61\n2 age-21   2/27/18 2/28/18 NPR/Ipsos     Adults          82                   72\n3 age-21   3/1/18  3/4/18  Rasmussen     Adults          67                   59\n4 age-21   2/22/18 2/26/18 Harris Inter… Registere…      84                   77\n5 age-21   3/3/18  3/5/18  Quinnipiac    Registere…      78                   63\n6 age-21   3/4/18  3/6/18  YouGov        Registere…      72                   65\n7 age-21   3/1/18  3/5/18  Morning Cons… Registere…      76                   72\n# ℹ 2 more variables: `Democratic Support` &lt;dbl&gt;, URL &lt;chr&gt;\n\n\nThis call is typical for a tidyverse function: the first argument is the data to be used (d), and the remaining argument(s) contain information on what should be done to the data.\nNote the use of == for comparison: In R, = means assingment and == means equals. Other comparisons are e.g. &gt; (greather than), &lt;= (less than or equal) and != (not equal). You can also combine multiple conditions with logical (boolean) operators: & (and), | or, and ! (not), and you can use parentheses like in mathematics.\nSo, we can find all surveys where support for raising the gun age was at least 80%:\n\nfilter(d, Question == 'age-21' & Support &gt;= 80)\n\n# A tibble: 2 × 9\n  Question Start   End     Pollster      Population Support `Republican Support`\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n1 age-21   2/27/18 2/28/18 NPR/Ipsos     Adults          82                   72\n2 age-21   2/22/18 2/26/18 Harris Inter… Registere…      84                   77\n# ℹ 2 more variables: `Democratic Support` &lt;dbl&gt;, URL &lt;chr&gt;\n\n\nNote that this command did not assign the result to an object, so the result is only displayed on the screen but not remembered. This can be a great way to quickly inspect your data, but if you want to continue analysing this subset you need to assign it to an object as above.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#aside-getting-help-on-tidy-function",
    "href": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#aside-getting-help-on-tidy-function",
    "title": "Data transformation",
    "section": "Aside: getting help on (tidy) function",
    "text": "Aside: getting help on (tidy) function\nAs explained earlier, to get help on a function you can type ?filter in the console or search for filter in the help pane. In both cases, you need to specify that you mean filter from the dplyr package, as there is also a filter function in other packages.\nIf you look at the help page, you will first see the general description. This is followed by Usage, which shows how the function should be called. In this case, it lists filter(.data, ...). The first argument (.data) makes sense, but the ... is confusing. What is means is that you can give an arbitrary number of extra arguments, that will (in this case) all be used as filters. This is explained in the Arguments: the ... arguments are ‘Logical predicates defined in terms of the variables in .data’.\nThe remainder give extra information on what exactly the function does (Details), the output it produces (Value), and links to other useful packages, functions, and finally a number examples.\nAlthough it may seem intimidating at first, it is important to get used to style of the R documentation as it is the primary source of information on most functions and packages you will be using!",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#selecting-certain-columns",
    "href": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#selecting-certain-columns",
    "title": "Data transformation",
    "section": "Selecting certain columns",
    "text": "Selecting certain columns\nWhere filter selects specific rows, select allows you to select specific columns. Most simply, we can simply name the columns that we want to retrieve them in that particular order.\n\nselect(age21, Population, Support, Pollster)\n\n# A tibble: 7 × 3\n  Population        Support Pollster          \n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             \n1 Registered Voters      72 CNN/SSRS          \n2 Adults                 82 NPR/Ipsos         \n3 Adults                 67 Rasmussen         \n4 Registered Voters      84 Harris Interactive\n5 Registered Voters      78 Quinnipiac        \n6 Registered Voters      72 YouGov            \n7 Registered Voters      76 Morning Consult   \n\n\nYou can also specify a range of columns, for example all columns from Support to Democratic Support:\n\nselect(age21, Support:`Democratic Support`)\n\n# A tibble: 7 × 3\n  Support `Republican Support` `Democratic Support`\n    &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n1      72                   61                   86\n2      82                   72                   92\n3      67                   59                   76\n4      84                   77                   92\n5      78                   63                   93\n6      72                   65                   80\n7      76                   72                   86\n\n\nNote the use of ‘backticks’ (reverse quotes) to specify the column name, as R does not normally allow spaces in names.\nSelect can also be used to rename columns when selecting them, for example to get rid of the spaces:\n\nselect(age21, Pollster, rep = `Republican Support`, dem = `Democratic Support`)\n\n# A tibble: 7 × 3\n  Pollster             rep   dem\n  &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 CNN/SSRS              61    86\n2 NPR/Ipsos             72    92\n3 Rasmussen             59    76\n4 Harris Interactive    77    92\n5 Quinnipiac            63    93\n6 YouGov                65    80\n7 Morning Consult       72    86\n\n\nNote that select drops all columns not selected. If you only want to rename columns, you can use the rename function:\n\nrename(age21, start_date = Start, end_date = End)\n\n# A tibble: 7 × 9\n  Question start_date end_date Pollster  Population Support `Republican Support`\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n1 age-21   2/20/18    2/23/18  CNN/SSRS  Registere…      72                   61\n2 age-21   2/27/18    2/28/18  NPR/Ipsos Adults          82                   72\n3 age-21   3/1/18     3/4/18   Rasmussen Adults          67                   59\n4 age-21   2/22/18    2/26/18  Harris I… Registere…      84                   77\n5 age-21   3/3/18     3/5/18   Quinnipi… Registere…      78                   63\n6 age-21   3/4/18     3/6/18   YouGov    Registere…      72                   65\n7 age-21   3/1/18     3/5/18   Morning … Registere…      76                   72\n# ℹ 2 more variables: `Democratic Support` &lt;dbl&gt;, URL &lt;chr&gt;\n\n\nFinally, you can drop a variable by adding a minus sign in front of a name:\n\nselect(age21, -Question, -URL)\n\n# A tibble: 7 × 7\n  Start   End     Pollster           Population     Support `Republican Support`\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt;            &lt;dbl&gt;                &lt;dbl&gt;\n1 2/20/18 2/23/18 CNN/SSRS           Registered Vo…      72                   61\n2 2/27/18 2/28/18 NPR/Ipsos          Adults              82                   72\n3 3/1/18  3/4/18  Rasmussen          Adults              67                   59\n4 2/22/18 2/26/18 Harris Interactive Registered Vo…      84                   77\n5 3/3/18  3/5/18  Quinnipiac         Registered Vo…      78                   63\n6 3/4/18  3/6/18  YouGov             Registered Vo…      72                   65\n7 3/1/18  3/5/18  Morning Consult    Registered Vo…      76                   72\n# ℹ 1 more variable: `Democratic Support` &lt;dbl&gt;",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#sorting-with-arrange",
    "href": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#sorting-with-arrange",
    "title": "Data transformation",
    "section": "Sorting with arrange()",
    "text": "Sorting with arrange()\nYou can easily sort a data set with arrange: you first specify the data, and then the column(s) to sort on. To sort in descending order, put a minus in front of a variable. For example, the following orders by population and then by support (descending):\n\nage21 &lt;- arrange(age21, Population, -Support)\nage21\n\n# A tibble: 7 × 9\n  Question Start   End     Pollster      Population Support `Republican Support`\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n1 age-21   2/27/18 2/28/18 NPR/Ipsos     Adults          82                   72\n2 age-21   3/1/18  3/4/18  Rasmussen     Adults          67                   59\n3 age-21   2/22/18 2/26/18 Harris Inter… Registere…      84                   77\n4 age-21   3/3/18  3/5/18  Quinnipiac    Registere…      78                   63\n5 age-21   3/1/18  3/5/18  Morning Cons… Registere…      76                   72\n6 age-21   2/20/18 2/23/18 CNN/SSRS      Registere…      72                   61\n7 age-21   3/4/18  3/6/18  YouGov        Registere…      72                   65\n# ℹ 2 more variables: `Democratic Support` &lt;dbl&gt;, URL &lt;chr&gt;\n\n\nNote that I assigned the result of arranging to the age21 object again, i.e. I replace the object by its sorted version. If I wouldn’t assign it to anything, it would display it on screen but not remember the sorting. Assigning a result to the same name means I don’t create a new object, preventing the environment from being cluttered (and saving me from the bother of thinking up yet another object name). For sorting, this should generally be fine as the sorted data should contain the same data as before. For subsetting, this means that the rows or columns are actually deleted from the dataset (in memory), so you will have to read the file again (or start from an earlier object) if you need those rows or columns later.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#adding-or-transforming-variables-with-mutate",
    "href": "data-management/0_tidyverse/1_selecting-renaming-and-filtering.html#adding-or-transforming-variables-with-mutate",
    "title": "Data transformation",
    "section": "Adding or transforming variables with mutate()",
    "text": "Adding or transforming variables with mutate()\nThe mutate function makes it easy to create new variables or to modify existing ones. For those more familiar with SPSS, this is what you would do with compute and recode.\nIf you look at the documentation page, you see that mutate works similarly to filter() and select(), in the sense that the first argument is the tibble, and then any number of additional arguments can be given to perform mutations. The mutations themselves are named arguments, in which you can provide any calculations using the existing columns.\nHere we’ll first create some variables and then look at the variables (using the select function to focus on the changes). Specifically, we’ll make a column for the absolute difference between the support scores for republicans and democrats, as a measure of how much they disagree.\n\nage21 &lt;- mutate(age21, party_diff = abs(`Republican Support` - `Democratic Support`))\nselect(age21, Question, Pollster, party_diff)\n\n# A tibble: 7 × 3\n  Question Pollster           party_diff\n  &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;\n1 age-21   NPR/Ipsos                  20\n2 age-21   Rasmussen                  17\n3 age-21   Harris Interactive         15\n4 age-21   Quinnipiac                 30\n5 age-21   Morning Consult            14\n6 age-21   CNN/SSRS                   25\n7 age-21   YouGov                     15\n\nage21 &lt;- arrange(age21, Population, -Support)\n\nTo transform (recode) a variable in the same column, you can simply use an existing name in mutate() to overwrite it.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Data transformation"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/2_summarizing.html",
    "href": "data-management/0_tidyverse/2_summarizing.html",
    "title": "Summarizing",
    "section": "",
    "text": "The functions used in the earlier part on data preparation worked on individual rows. Sometimes, you need to compute properties of groups of rows (cases). This is called aggregation (or summarization) and in tidyverse uses the group_by function followed by either summarize or mutate.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Summarizing"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/2_summarizing.html#grouping-rows",
    "href": "data-management/0_tidyverse/2_summarizing.html#grouping-rows",
    "title": "Summarizing",
    "section": "Grouping rows",
    "text": "Grouping rows\nNow, we can use the group_by function to group by, for example, pollster:\n\nd %&gt;% \n  group_by(Question)\n\n# A tibble: 57 × 8\n# Groups:   Question [8]\n   Question     Start   End     Pollster          Population Support   Rep   Dem\n   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;             &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 age-21       2/20/18 2/23/18 CNN/SSRS          Registere…      72    61    86\n 2 age-21       2/27/18 2/28/18 NPR/Ipsos         Adults          82    72    92\n 3 age-21       3/1/18  3/4/18  Rasmussen         Adults          67    59    76\n 4 age-21       2/22/18 2/26/18 Harris Interacti… Registere…      84    77    92\n 5 age-21       3/3/18  3/5/18  Quinnipiac        Registere…      78    63    93\n 6 age-21       3/4/18  3/6/18  YouGov            Registere…      72    65    80\n 7 age-21       3/1/18  3/5/18  Morning Consult   Registere…      76    72    86\n 8 arm-teachers 2/23/18 2/25/18 YouGov/Huffpost   Registere…      41    69    20\n 9 arm-teachers 2/20/18 2/23/18 CBS News          Adults          44    68    20\n10 arm-teachers 2/27/18 2/28/18 Rasmussen         Adults          43    71    24\n# ℹ 47 more rows\n\n\nAs you can see, the data itself didn’t actually change yet, it merely recorded (at the top) that we are now grouping by Question, and that there are 8 groups (different questions) in total.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Summarizing"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/2_summarizing.html#summarizing",
    "href": "data-management/0_tidyverse/2_summarizing.html#summarizing",
    "title": "Summarizing",
    "section": "Summarizing",
    "text": "Summarizing\nTo summarize, you follow the group_by with a call to summarize. Summarize has a syntax that is similar to mutate: summarize(column = calculation, ...). The crucial difference, however, is that you always need to use a function in the calculation, and that function needs to compute a single summary value given a vector of values. Very common summarization functions are sum, mean, and sd (standard deviation).\nFor example, the following computes the average support per question (and sorts by descending support):\n\nd %&gt;% \n  group_by(Question) %&gt;%                    # group by \"Questions\"\n  summarize(Support = mean(Support)) %&gt;%    # average \"Support\" per group\n  arrange(-Support)                         # sort based on \"Support\"\n\n# A tibble: 8 × 2\n  Question                    Support\n  &lt;chr&gt;                         &lt;dbl&gt;\n1 background-checks              87.4\n2 mental-health-own-gun          85.8\n3 age-21                         75.9\n4 ban-high-capacity-magazines    67.3\n5 stricter-gun-laws              66.5\n6 ban-assault-weapons            61.8\n7 arm-teachers                   42  \n8 repeal-2nd-amendment           10  \n\n\nAs you can see, summarize drastically changes the shape of the data. There are now rows equal to the number of groups (8), and the only columns left are the grouping variables and the summarized values.\nYou can also compute summaries of multiple values, and even do ad hoc calculations:\n\nd %&gt;% \n  group_by(Question) %&gt;% \n  summarize(Dem = mean(Dem), \n            Rep = mean(Rep), \n            diff = mean(Dem-Rep)) %&gt;% \n  arrange(-diff)\n\n# A tibble: 8 × 4\n  Question                      Dem   Rep    diff\n  &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 stricter-gun-laws            86.9  46.4  40.5  \n2 ban-assault-weapons          80.7  42.6  38.1  \n3 ban-high-capacity-magazines  83.9  52.7  31.1  \n4 age-21                       86.4  67    19.4  \n5 repeal-2nd-amendment         15     5    10    \n6 background-checks            91.9  83.6   8.29 \n7 mental-health-own-gun        87.5  86.7   0.833\n8 arm-teachers                 17.2  72.2 -55    \n\n\nSo, Democrats are more in favor of all proposed gun laws except arming teachers.\nYou can also compute multiple summaries of a single value. Another useful function is n() (without arguments), which simply counts the values in each group. For example, the following gives the count, mean, and standard deviation of the support:\n\nd %&gt;% \n  group_by(Question) %&gt;% \n  summarize(n = n(),\n            mean = mean(Support), \n            sd = sd(Support))\n\n# A tibble: 8 × 4\n  Question                        n  mean    sd\n  &lt;chr&gt;                       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 age-21                          7  75.9  6.01\n2 arm-teachers                    6  42    1.55\n3 background-checks               7  87.4  7.32\n4 ban-assault-weapons            12  61.8  6.44\n5 ban-high-capacity-magazines     7  67.3  3.86\n6 mental-health-own-gun           6  85.8  5.46\n7 repeal-2nd-amendment            1  10   NA   \n8 stricter-gun-laws              11  66.5  5.15\n\n\nNote: As you can see, one of the values has a missing value (NA) for standard deviation. Why?",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Summarizing"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/2_summarizing.html#using-mutate-with-group_by",
    "href": "data-management/0_tidyverse/2_summarizing.html#using-mutate-with-group_by",
    "title": "Summarizing",
    "section": "Using mutate with group_by",
    "text": "Using mutate with group_by\nThe examples above all reduce the number of cases to the number of groups. Another option is to use mutate after a group_by, which allows you to add summary values to the rows themselves.\nFor example, suppose we wish to see whether a certain poll has a different prediction from the average polling of that question. We can group_by question and then use mutate to calculate the average support:\n\nd2 &lt;- d %&gt;% \n  group_by(Question) %&gt;%\n  mutate(avg_support = mean(Support), \n         diff = Support - avg_support)\nd2\n\n# A tibble: 57 × 10\n# Groups:   Question [8]\n   Question     Start  End   Pollster Population Support   Rep   Dem avg_support\n   &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 age-21       2/20/… 2/23… CNN/SSRS Registere…      72    61    86        75.9\n 2 age-21       2/27/… 2/28… NPR/Ips… Adults          82    72    92        75.9\n 3 age-21       3/1/18 3/4/… Rasmuss… Adults          67    59    76        75.9\n 4 age-21       2/22/… 2/26… Harris … Registere…      84    77    92        75.9\n 5 age-21       3/3/18 3/5/… Quinnip… Registere…      78    63    93        75.9\n 6 age-21       3/4/18 3/6/… YouGov   Registere…      72    65    80        75.9\n 7 age-21       3/1/18 3/5/… Morning… Registere…      76    72    86        75.9\n 8 arm-teachers 2/23/… 2/25… YouGov/… Registere…      41    69    20        42  \n 9 arm-teachers 2/20/… 2/23… CBS News Adults          44    68    20        42  \n10 arm-teachers 2/27/… 2/28… Rasmuss… Adults          43    71    24        42  \n# ℹ 47 more rows\n# ℹ 1 more variable: diff &lt;dbl&gt;\n\n\nAs you can see, where summarize reduces the rows and columns to the groups and summaries, mutate adds a new column which is identical for all rows within a group.",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Summarizing"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/2_summarizing.html#ungrouping",
    "href": "data-management/0_tidyverse/2_summarizing.html#ungrouping",
    "title": "Summarizing",
    "section": "Ungrouping",
    "text": "Ungrouping\nFinally, you can use ungroup to get rid of any groupings.\nFor example, the data produced by the example above is still grouped by Question as mutate does not remove grouping information. So, if we want to compute the overall standard deviation of the difference we could ungroup and then summarize:\n\nd2 %&gt;% \n  ungroup() %&gt;% \n  summarize(diff = sd(diff))\n\n# A tibble: 1 × 1\n   diff\n  &lt;dbl&gt;\n1  5.19\n\n\n(of course, running sd(d2$diff)) would yield the same result.)\nIf you run the same command without the ungroup, what would the result be? Why?",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Summarizing"
    ]
  },
  {
    "objectID": "getting-started/install-r-and-rstudio.html",
    "href": "getting-started/install-r-and-rstudio.html",
    "title": "Install R and RStudio",
    "section": "",
    "text": "To work with R, you will need to install two pieces of software.\nBoth programs can be downloaded for free, and are available for all main operating systems (Windows, macOS and Linux).",
    "crumbs": [
      "Getting Started",
      "Install R and RStudio"
    ]
  },
  {
    "objectID": "getting-started/install-r-and-rstudio.html#installing-r",
    "href": "getting-started/install-r-and-rstudio.html#installing-r",
    "title": "Install R and RStudio",
    "section": "Installing R",
    "text": "Installing R\nTo install R, you can download it from the CRAN (comprehensive R Archive Network) website. Do not be alarmed by the website’s 90’s aesthetics. The website is legit.",
    "crumbs": [
      "Getting Started",
      "Install R and RStudio"
    ]
  },
  {
    "objectID": "getting-started/install-r-and-rstudio.html#installing-rstudio",
    "href": "getting-started/install-r-and-rstudio.html#installing-rstudio",
    "title": "Install R and RStudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nRStudio can be downloaded from the posit.co website, which is the developer of RStudio. Make sure to pick the latest version available for your operating system.",
    "crumbs": [
      "Getting Started",
      "Install R and RStudio"
    ]
  },
  {
    "objectID": "getting-started/r-in-action.html",
    "href": "getting-started/r-in-action.html",
    "title": "R in Action (demo)",
    "section": "",
    "text": "R is a very powerful tool, but it takes some time to learn how to use it before you get to fully appreciate what you can use it for. On this page we show you a quick example of some of the things you will learn here.\nAll the code that you see here will be explained in the rest of this online book. For now, don’t worry about understanding the code, and focus on thinking how the techniques we’ll be using fit into your tool belt as a communication scientist.",
    "crumbs": [
      "Getting Started",
      "R in Action (demo)"
    ]
  },
  {
    "objectID": "getting-started/r-in-action.html#loading-the-packages-well-use",
    "href": "getting-started/r-in-action.html#loading-the-packages-well-use",
    "title": "R in Action (demo)",
    "section": "Loading the packages we’ll use",
    "text": "Loading the packages we’ll use\nOne of the things that makes R so versatile, is that anyone can extend it by writing new packages. You can think of packages kind of like apps in an app-store. For this demo, we’ll need two packages, that you’ll first need to install.\n\ninstall.packages('tidyverse')\ninstall.packages('sjPlot')\n\nYou only need to install packages once, just like apps on you mobile phone. Once downloaded, they are stored in your R library. When you then use the packages in an R script, you just open them like this:\n\nlibrary(tidyverse)\nlibrary(sjPlot)",
    "crumbs": [
      "Getting Started",
      "R in Action (demo)"
    ]
  },
  {
    "objectID": "getting-started/r-in-action.html#import-data",
    "href": "getting-started/r-in-action.html#import-data",
    "title": "R in Action (demo)",
    "section": "Import data",
    "text": "Import data\nThe first step for any analysis is to import your data. Using the read_csv function from the tidyverse package, we can directly download this information from the internet and import it into R.",
    "crumbs": [
      "Getting Started",
      "R in Action (demo)"
    ]
  },
  {
    "objectID": "data-management/0_tidyverse/0_read-data.html",
    "href": "data-management/0_tidyverse/0_read-data.html",
    "title": "Read data into R",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Data management",
      "The Tidyverse Toolkit",
      "Read data into R"
    ]
  },
  {
    "objectID": "data-management/filter-and-arrange.html",
    "href": "data-management/filter-and-arrange.html",
    "title": "Select and filter",
    "section": "",
    "text": "For the examples in this tutorial we’ll use the iris dataset, like we explained here.\nlibrary(tidyverse)\nd &lt;- as_tibble(iris)",
    "crumbs": [
      "Data management",
      "Select and filter"
    ]
  },
  {
    "objectID": "data-management/filter-and-arrange.html#subsetting-rows-with-filter",
    "href": "data-management/filter-and-arrange.html#subsetting-rows-with-filter",
    "title": "Select and filter",
    "section": "Subsetting rows with filter()",
    "text": "Subsetting rows with filter()\nThe filter function can be used to select a subset of rows. For example, the following code selects only the rows where the Species column is equal to setosa. The first argument of the filter function is the tibble you want to filter. The second argument is the condition that should be met for a row to be included in the result. Here we use the == operator to say that the value in the Species column should be equal to setosa.\n\nfilter(d, Species == 'setosa')\n\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 40 more rows\n\n\nNote that we now get 50 out of the 150 rows in the iris dataset.\nWe can use other common operators as well, such as &gt;, &lt;, &gt;=, &lt;=, and != (not equal). And we can also combine multiple conditions with the & (and) and | (or) operators.\nFor example, here we select all rows where the Species is setosa and the Sepal.Length is greater than 5.\n\nfilter(d, Species == 'setosa' & Sepal.Length &gt; 5)\n\nA less common operator that is usefull to know about is %in%, which is used to check if a value is in a list of values. For example, to select all rows where the Species is either setosa or versicolor:\n\nfilter(d, Species %in% c('setosa', 'versicolor'))\n\n\n\n\n\n\n\nA deeper understanding of the filter condition\n\n\n\n\n\nBased on the examples given you probably already have a good enough understanding of how the filter function works to use it in your own code. In this optional information block we’ll go a bit deeper into how the filter condition works, and what operators you can use.\n\nThe condition is a logical expression\nThe condition in filter can be any logical expression. A logical expression is simply a statement that is either TRUE or FALSE. When we use a logical expression in the filter function, we are asking R to evaluate this expression for each row in the tibble. Each row for which the expression evaluates to TRUE is then included in the subset.\nIf you know a bit about how logical expressions work, you will have great control over what rows are included in your subset. Here is an overview of the most important operators for logical expressions.\n\nComparison operators\nComparison operators are used to compare two values.\n\n== equal to\n!= not equal to\n&gt; greater than\n&gt;= greater than or equal to\n&lt; less than\n&lt;= less than or equal to\n%in% is in a list of values (second value must be a list or vector)\n\nExample:\n\n5 &gt; 1    # TRUE:  5 is greater than 1\n\n[1] TRUE\n\n5 &lt; 1    # FALSE: 5 is less than 1\n\n[1] FALSE\n\n\"setosa\" %in% c(\"setosa\", \"versicolor\")  # TRUE: \"setosa\" is in the list\n\n[1] TRUE\n\n\n\n\nLogical operators\nLogical operators are used to combine multiple conditions.\n\n& and\n| or\n! not\n\nExample:\n\n5 &gt; 1 | 5 &lt; 1   # TRUE: 5 is greater than 1 OR 5 is less than 1\n\n[1] TRUE\n\n5 &gt; 1 & 5 &lt; 1   # FALSE: 5 is greater than 1 AND 5 is less than 1\n\n[1] FALSE\n\n!5 &lt; 1          # TRUE: it is not the case that 5 is smaller than 1\n\n[1] TRUE\n\n\n\n\nParentheses\nFor complex conditions, you can use parentheses to group conditions, similar to how you would in a mathematical expression. Say you want to select big flowers, but you want to take into account that some species are generally smaller. So the minimum Sepal.Length for setosa should be 5, but for versicolor it should be 6. You could then write the following condition:\n\nfilter(d, (Species == 'setosa' & Sepal.Length &gt; 5) | \n          (Species == 'versicolor' & Sepal.Length &gt; 6)\n      )",
    "crumbs": [
      "Data management",
      "Select and filter"
    ]
  },
  {
    "objectID": "data-management/filter-and-arrange.html#the-condition-is-a-logical-expression",
    "href": "data-management/filter-and-arrange.html#the-condition-is-a-logical-expression",
    "title": "Select and filter",
    "section": "The condition is a logical expression",
    "text": "The condition is a logical expression\nThe condition in filter can be any logical expression. A logical expression is simply a statement that is either TRUE or FALSE. When we use a logical expression in the filter function, we are asking R to evaluate this expression for each row in the tibble. Each row for which the expression evaluates to TRUE is then included in the subset.\nIf you know a bit about how logical expressions work, you will have great control over what rows are included in your subset. Here is an overview of the most important operators for logical expressions.\n\nComparison operators\nComparison operators are used to compare two values.\n\n== equal to\n!= not equal to\n&gt; greater than\n&gt;= greater than or equal to\n&lt; less than\n&lt;= less than or equal to\n%in% is in a list of values (second value must be a list or vector)\n\nExample:\n\n5 &gt; 1    # TRUE:  5 is greater than 1\n\n[1] TRUE\n\n5 &lt; 1    # FALSE: 5 is less than 1\n\n[1] FALSE\n\n\"setosa\" %in% c(\"setosa\", \"versicolor\")  # TRUE: \"setosa\" is in the list\n\n[1] TRUE\n\n\n\n\nLogical operators\nLogical operators are used to combine multiple conditions.\n\n& and\n| or\n! not\n\nExample:\n\n5 &gt; 1 | 5 &lt; 1   # TRUE: 5 is greater than 1 OR 5 is less than 1\n\n[1] TRUE\n\n5 &gt; 1 & 5 &lt; 1   # FALSE: 5 is greater than 1 AND 5 is less than 1\n\n[1] FALSE\n\n!5 &lt; 1          # TRUE: it is not the case that 5 is smaller than 1\n\n[1] TRUE\n\n\n\n\nParentheses\nFor complex conditions, you can use parentheses to group conditions, similar to how you would in a mathematical expression. Say you want to select big flowers, but you want to take into account that some species are generally smaller. So the minimum Sepal.Length for setosa should be 5, but for versicolor it should be 6. You could then write the following condition:\n\nfilter(d, (Species == 'setosa' & Sepal.Length &gt; 5) | \n          (Species == 'versicolor' & Sepal.Length &gt; 6)\n      )",
    "crumbs": [
      "Data management",
      "Select and filter"
    ]
  },
  {
    "objectID": "data-management/pipes.html",
    "href": "data-management/pipes.html",
    "title": "Pipes",
    "section": "",
    "text": "Working with Pipes\nIf you look at the code above, you notice that the result of each function is stored as an object, and that this object is used as the first argument for the next function. Moreover, we don’t really care about this temporary object, we only care about the final summary table.\nThis is a very common usage pattern, and it can be seen as a pipeline of functions, where the output of each function is the input for the next function. Because this is so common, tidyverse offers a more convenient way of writing the code above using the pipeline operator %&gt;%. In sort, whenever you write f(a, x) you can replace it by a %&gt;% f(x). If you then want to use the output of f(a, x) for a second function, you can just add it to the pipe: a %&gt;% f(x) %&gt;% f2(y) is equivalent to f2(f(a,x), y), or more readable, b=f(a,x); f2(b, y)\nPut simply, pipes take the output of a function, and directly use that output as the input for the .data argument in the next function. As you have seen, all the dplyr functions that we discussed have in common that the first argument is a tibble, and all functions return a tibble. This is intentional, and allows us to pipe all the functions together.\nThis seems a bit abstract, but consider the code below, which is a collection of statements from above:\n\n# d &lt;- read_csv(url)\n# d &lt;- filter(d, Question == 'age-21')\n# d &lt;- mutate(d, party_diff = abs(`Republican Support` - `Democratic Support`))\n# d &lt;- select(d, Question, Pollster, party_diff)\n# arrange(d, -party_diff)\n\nTo recap, this reads the csv, filters by question, computes the difference, drops other variables, and sorts. Since the output of each function is the input of the next, we can also write this as a single pipeline:\n\n# read_csv(url) %&gt;% \n#   filter(Question == 'age-21') %&gt;% \n#   mutate(party_diff = abs(`Republican Support` - `Democratic Support`)) %&gt;%\n#   select(Question, Pollster, party_diff) %&gt;% \n#   arrange(-party_diff)\n\nThe nice thing about pipes is that it makes it really clear what you are doing. Also, it doesn’t require making many intermediate objects (such as ds). If applied right, piping allows you to make nicely contained pieces of code to perform specific parts of your analysis from raw input straight to results, including statistical modeling or visualization. It usually makes sense to have each “step” in the pipeline in its own line. This way, we can easily read the code line by line\nOf course, you probably don’t want to replace your whole script with a single pipe, and often it is nice to store intermediate values. For example, you might want to download, clean, and subset a data set before doing multiple analyses with it. In that case, you probably want to store the result of downloading, cleaning, and subsetting as a variable, and use that in your analyses.\n\n\n\n\n Back to top",
    "crumbs": [
      "Data management",
      "Pipes"
    ]
  },
  {
    "objectID": "data-management/select-and-rename.html",
    "href": "data-management/select-and-rename.html",
    "title": "Select and rename columns",
    "section": "",
    "text": "Open the tidyverse package, and read the data from the URL.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/poll-quiz-guns/guns-polls.csv\"\nd &lt;- read_csv(url)\n\nRows: 57 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Question, Start, End, Pollster, Population, URL\ndbl (3): Support, Republican Support, Democratic Support\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Data management",
      "Select and rename columns"
    ]
  },
  {
    "objectID": "data-management/select-and-rename.html#selecting-certain-columns",
    "href": "data-management/select-and-rename.html#selecting-certain-columns",
    "title": "Select and rename columns",
    "section": "Selecting certain columns",
    "text": "Selecting certain columns\nWhere filter selects specific rows, select allows you to select specific columns. Most simply, we can simply name the columns that we want to retrieve them in that particular order.\n\nselect(d, Population, Support, Pollster)\n\n# A tibble: 57 × 3\n   Population        Support Pollster          \n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             \n 1 Registered Voters      72 CNN/SSRS          \n 2 Adults                 82 NPR/Ipsos         \n 3 Adults                 67 Rasmussen         \n 4 Registered Voters      84 Harris Interactive\n 5 Registered Voters      78 Quinnipiac        \n 6 Registered Voters      72 YouGov            \n 7 Registered Voters      76 Morning Consult   \n 8 Registered Voters      41 YouGov/Huffpost   \n 9 Adults                 44 CBS News          \n10 Adults                 43 Rasmussen         \n# ℹ 47 more rows\n\n\nYou can also specify a range of columns, for example all columns from Support to Democratic Support:\n\nselect(d, Support:`Democratic Support`)\n\n# A tibble: 57 × 3\n   Support `Republican Support` `Democratic Support`\n     &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n 1      72                   61                   86\n 2      82                   72                   92\n 3      67                   59                   76\n 4      84                   77                   92\n 5      78                   63                   93\n 6      72                   65                   80\n 7      76                   72                   86\n 8      41                   69                   20\n 9      44                   68                   20\n10      43                   71                   24\n# ℹ 47 more rows\n\n\nNote the use of ‘backticks’ (reverse quotes) to specify the column name, as R does not normally allow spaces in names.\nSelect can also be used to rename columns when selecting them, for example to get rid of the spaces:\n\nselect(d, Pollster, rep = `Republican Support`, dem = `Democratic Support`)\n\n# A tibble: 57 × 3\n   Pollster             rep   dem\n   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 CNN/SSRS              61    86\n 2 NPR/Ipsos             72    92\n 3 Rasmussen             59    76\n 4 Harris Interactive    77    92\n 5 Quinnipiac            63    93\n 6 YouGov                65    80\n 7 Morning Consult       72    86\n 8 YouGov/Huffpost       69    20\n 9 CBS News              68    20\n10 Rasmussen             71    24\n# ℹ 47 more rows\n\n\nNote that select drops all columns not selected. If you only want to rename columns, you can use the rename function:\n\nrename(d, start_date = Start, end_date = End)\n\n# A tibble: 57 × 9\n   Question start_date end_date Pollster Population Support `Republican Support`\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n 1 age-21   2/20/18    2/23/18  CNN/SSRS Registere…      72                   61\n 2 age-21   2/27/18    2/28/18  NPR/Ips… Adults          82                   72\n 3 age-21   3/1/18     3/4/18   Rasmuss… Adults          67                   59\n 4 age-21   2/22/18    2/26/18  Harris … Registere…      84                   77\n 5 age-21   3/3/18     3/5/18   Quinnip… Registere…      78                   63\n 6 age-21   3/4/18     3/6/18   YouGov   Registere…      72                   65\n 7 age-21   3/1/18     3/5/18   Morning… Registere…      76                   72\n 8 arm-tea… 2/23/18    2/25/18  YouGov/… Registere…      41                   69\n 9 arm-tea… 2/20/18    2/23/18  CBS News Adults          44                   68\n10 arm-tea… 2/27/18    2/28/18  Rasmuss… Adults          43                   71\n# ℹ 47 more rows\n# ℹ 2 more variables: `Democratic Support` &lt;dbl&gt;, URL &lt;chr&gt;\n\n\nFinally, you can drop a variable by adding a minus sign in front of a name:\n\nselect(d, -Question, -URL)\n\n# A tibble: 57 × 7\n   Start   End     Pollster           Population    Support `Republican Support`\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt;           &lt;dbl&gt;                &lt;dbl&gt;\n 1 2/20/18 2/23/18 CNN/SSRS           Registered V…      72                   61\n 2 2/27/18 2/28/18 NPR/Ipsos          Adults             82                   72\n 3 3/1/18  3/4/18  Rasmussen          Adults             67                   59\n 4 2/22/18 2/26/18 Harris Interactive Registered V…      84                   77\n 5 3/3/18  3/5/18  Quinnipiac         Registered V…      78                   63\n 6 3/4/18  3/6/18  YouGov             Registered V…      72                   65\n 7 3/1/18  3/5/18  Morning Consult    Registered V…      76                   72\n 8 2/23/18 2/25/18 YouGov/Huffpost    Registered V…      41                   69\n 9 2/20/18 2/23/18 CBS News           Adults             44                   68\n10 2/27/18 2/28/18 Rasmussen          Adults             43                   71\n# ℹ 47 more rows\n# ℹ 1 more variable: `Democratic Support` &lt;dbl&gt;",
    "crumbs": [
      "Data management",
      "Select and rename columns"
    ]
  },
  {
    "objectID": "data-management/import-and-view.html",
    "href": "data-management/import-and-view.html",
    "title": "Import and view",
    "section": "",
    "text": "Short code summary\n\n\n\n\n\nTo read a CSV file, you can use the read_csv function from the readr package, which is included in the tidyverse.\n\nlibrary(tidyverse)\nd &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/poll-quiz-guns/guns-polls.csv\")\n\nTo see the data, you can just type d in the console, or use the View function to open a spreadsheet view.\n\nd\nView(d)\n\nTo quickly get detailed information about your variables, you can use the dfSummary function from the summarytools package.\n\nlibrary(summarytools)\nview(dfSummary(d))",
    "crumbs": [
      "Data management",
      "Import and view"
    ]
  },
  {
    "objectID": "data-management/import-and-view.html#reading-data-read_csv",
    "href": "data-management/import-and-view.html#reading-data-read_csv",
    "title": "Import and view",
    "section": "",
    "text": "The example above manually created a data set, but in most cases you will start with data that you get from elsewhere, such as a csv file (e.g. downloaded from an online dataset or exported from excel) or an SPSS or Stata data file.\nTidyverse contains a function read_csv that allows you to read a csv file directly into a data frame. You specify the location of the file, either on your local drive or directly from the Internet!\nThe example below downloads an overview of gun polls from the data analytics site 538, and reads it into a tibble using the read_csv function:\n\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/poll-quiz-guns/guns-polls.csv\"\nd &lt;- read_csv(url)\n\nRows: 57 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Question, Start, End, Pollster, Population, URL\ndbl (3): Support, Republican Support, Democratic Support\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nd\n\n# A tibble: 57 × 9\n   Question     Start   End     Pollster Population Support `Republican Support`\n   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n 1 age-21       2/20/18 2/23/18 CNN/SSRS Registere…      72                   61\n 2 age-21       2/27/18 2/28/18 NPR/Ips… Adults          82                   72\n 3 age-21       3/1/18  3/4/18  Rasmuss… Adults          67                   59\n 4 age-21       2/22/18 2/26/18 Harris … Registere…      84                   77\n 5 age-21       3/3/18  3/5/18  Quinnip… Registere…      78                   63\n 6 age-21       3/4/18  3/6/18  YouGov   Registere…      72                   65\n 7 age-21       3/1/18  3/5/18  Morning… Registere…      76                   72\n 8 arm-teachers 2/23/18 2/25/18 YouGov/… Registere…      41                   69\n 9 arm-teachers 2/20/18 2/23/18 CBS News Adults          44                   68\n10 arm-teachers 2/27/18 2/28/18 Rasmuss… Adults          43                   71\n# ℹ 47 more rows\n# ℹ 2 more variables: `Democratic Support` &lt;dbl&gt;, URL &lt;chr&gt;\n\n\n(Note that you can safely ignore the (red) message, they simply tell you how each column was parsed)\nThe shows the first ten rows of the data set, and if the columns don’t fit they are not printed. The remaining rows and columns are printed at the bottom. For each column the data type is also mentioned ( stands for integer, which is a numeric value;  is textual or character data). If you want to browse through your data, you can also click on the name of the data.frame (d) in the top-right window “Environment” tab or call View(d).",
    "crumbs": [
      "Data management",
      "Import and view"
    ]
  },
  {
    "objectID": "data-management/select-and-rename-columns.html",
    "href": "data-management/select-and-rename-columns.html",
    "title": "Select and rename columns",
    "section": "",
    "text": "TLDR summary\n\n\n\n\n\nTo select columns from a tibble (or data frame), you can use the select function.\n\nlibrary(tidyverse)\nd &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")\n\n# select columns age and np_subscription\nd_subset &lt;- select(d, age, np_subscription)\n\n# select columns from experiment_group to trust_t2\nd_subset &lt;- select(d, experiment_group:trust_t2)\n\n# select columns and rename them\nd_subset &lt;- select(d, group = experiment_group, \n                      trust_before = trust_t1, \n                      trust_after = trust_t2)\n\n# select columns with spaces in the name\nd_subset &lt;- select(d, news_consumption = `news consumption`)\n\n# drop columns\nd_subset &lt;- select(d, -np_subscription, -trust_t1)\n\nYou can also rename columns without selecting them using the rename function:\n\nd_renamed &lt;- rename(d, group = experiment_group, \n                        trust_before = trust_t1, \n                        trust_after = trust_t2)\n\n\n\n\nIn this tutorial we use the tidyverse and our simulated practice data.\n\nlibrary(tidyverse)\nd &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")\n\n\nSelecting columns with select\nOften you do not need to use all columns in your data, or you only need a subset of the columns for a specific analysis. You can do this with the select function.\nFirst, let’s see what columns are in our data using the colnames function, which returns the column names of a data frame:\n\ncolnames(d)\n\n[1] \"age\"              \"np_subscription\"  \"news consumption\" \"experiment_group\"\n[5] \"trust_t1\"         \"trust_t2\"         \"id\"              \n\n\n\nSelecting specific columns\nThe simplest way of using select is to explicitly specify the columns you want to keep:\n\nds &lt;- select(d, age, np_subscription)\n\nThis will return a new tibble with only the columns id, age, and np_subscription. We assigned this new tibble to the variable ds (short for “data subset”). Sometimes you want to overwrite the original data frame with the new selection. You can do this by assigning the result to the same name as the input (d in this case):\n\nd &lt;- select(d, age, np_subscription)\n\nJust be carefull with this. One of the nice things about R is that you can have multiple versions of your data in different tibbles. It is often smart to at least keep the original (raw) data frame intact.\n\n\nSelecting a range of columns\nYou can also specify a range of columns using the syntax first_column:last_column. For example, to select all columns from experiment_group to trust_t2:\n\nselect(d, experiment_group:trust_t2)\n\nThis will return a new tibble with only the columns experiment_group, trust_t1, and trust_t2.\nNote that here we did not assign the result to anything. So in this case R will just print the result to the console, but not store it in a variable.\n\n\nSelecting and renaming columns\nWhen you select a column, you can also rename it using the syntax new_name = old_name. The following code selects the columns experiment_group, trust_t1, and trust_t2, and renames them to group, trust_before, and trust_after:\n\nselect(d, group = experiment_group, \n          trust_before = trust_t1, \n          trust_after = trust_t2)\n\n\n\nSelecting columns that have spaces in the name\nSometimes columns names have spaces in them. This is a bit annoying to work with in R, because you need to then tell R where a name starts and ends. You can do this by using backticks (reverse quotes) around the column name. In our practice data, we need this to select the news consumption column. It is then often smart to immediately rename the column to something without spaces, such as just replacing them with underscores:\n\nselect(d, news_consumption = `news consumption`)\n\n\n\nDropping columns\nInstead of selecting which column to keep, you can also specify which columns to drop. You can do this by adding a minus sign in front of the column name. The following code drops the columns np_subscription and trust_t1:\n\nselect(d, -np_subscription, -trust_t1)\n\nThis will return a new tibble with all columns except np_subscription and trust_t1.\n\n\n\nRenaming columns with rename\nSometimes you only want to rename columns without selecting or dropping any. You can do this with the rename function, which works similarly to how you rename columns with select:\n\nrename(d, group = experiment_group, \n          trust_before = trust_t1, \n          trust_after = trust_t2)\n\nIn this case, we do rename the columns, but without dropping all the other columns.\n\n\n\n\n Back to top",
    "crumbs": [
      "Data management",
      "Select and rename columns"
    ]
  },
  {
    "objectID": "data-management/select-and-rename-columns.html#selecting-certain-columns",
    "href": "data-management/select-and-rename-columns.html#selecting-certain-columns",
    "title": "Select and rename columns",
    "section": "Selecting certain columns",
    "text": "Selecting certain columns\nWhere filter selects specific rows, select allows you to select specific columns. Most simply, we can simply name the columns that we want to retrieve them in that particular order.\n\nselect(d, Population, Support, Pollster)\n\n# A tibble: 57 × 3\n   Population        Support Pollster          \n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             \n 1 Registered Voters      72 CNN/SSRS          \n 2 Adults                 82 NPR/Ipsos         \n 3 Adults                 67 Rasmussen         \n 4 Registered Voters      84 Harris Interactive\n 5 Registered Voters      78 Quinnipiac        \n 6 Registered Voters      72 YouGov            \n 7 Registered Voters      76 Morning Consult   \n 8 Registered Voters      41 YouGov/Huffpost   \n 9 Adults                 44 CBS News          \n10 Adults                 43 Rasmussen         \n# ℹ 47 more rows\n\n\nYou can also specify a range of columns, for example all columns from Support to Democratic Support:\n\nselect(d, Support:`Democratic Support`)\n\n# A tibble: 57 × 3\n   Support `Republican Support` `Democratic Support`\n     &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n 1      72                   61                   86\n 2      82                   72                   92\n 3      67                   59                   76\n 4      84                   77                   92\n 5      78                   63                   93\n 6      72                   65                   80\n 7      76                   72                   86\n 8      41                   69                   20\n 9      44                   68                   20\n10      43                   71                   24\n# ℹ 47 more rows\n\n\nNote the use of ‘backticks’ (reverse quotes) to specify the column name, as R does not normally allow spaces in names.\nSelect can also be used to rename columns when selecting them, for example to get rid of the spaces:\n\nselect(d, Pollster, rep = `Republican Support`, dem = `Democratic Support`)\n\n# A tibble: 57 × 3\n   Pollster             rep   dem\n   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 CNN/SSRS              61    86\n 2 NPR/Ipsos             72    92\n 3 Rasmussen             59    76\n 4 Harris Interactive    77    92\n 5 Quinnipiac            63    93\n 6 YouGov                65    80\n 7 Morning Consult       72    86\n 8 YouGov/Huffpost       69    20\n 9 CBS News              68    20\n10 Rasmussen             71    24\n# ℹ 47 more rows\n\n\nNote that select drops all columns not selected. If you only want to rename columns, you can use the rename function:\n\nrename(d, start_date = Start, end_date = End)\n\n# A tibble: 57 × 9\n   Question start_date end_date Pollster Population Support `Republican Support`\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n 1 age-21   2/20/18    2/23/18  CNN/SSRS Registere…      72                   61\n 2 age-21   2/27/18    2/28/18  NPR/Ips… Adults          82                   72\n 3 age-21   3/1/18     3/4/18   Rasmuss… Adults          67                   59\n 4 age-21   2/22/18    2/26/18  Harris … Registere…      84                   77\n 5 age-21   3/3/18     3/5/18   Quinnip… Registere…      78                   63\n 6 age-21   3/4/18     3/6/18   YouGov   Registere…      72                   65\n 7 age-21   3/1/18     3/5/18   Morning… Registere…      76                   72\n 8 arm-tea… 2/23/18    2/25/18  YouGov/… Registere…      41                   69\n 9 arm-tea… 2/20/18    2/23/18  CBS News Adults          44                   68\n10 arm-tea… 2/27/18    2/28/18  Rasmuss… Adults          43                   71\n# ℹ 47 more rows\n# ℹ 2 more variables: `Democratic Support` &lt;dbl&gt;, URL &lt;chr&gt;\n\n\nFinally, you can drop a variable by adding a minus sign in front of a name:\n\nselect(d, -Question, -URL)\n\n# A tibble: 57 × 7\n   Start   End     Pollster           Population    Support `Republican Support`\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt;           &lt;dbl&gt;                &lt;dbl&gt;\n 1 2/20/18 2/23/18 CNN/SSRS           Registered V…      72                   61\n 2 2/27/18 2/28/18 NPR/Ipsos          Adults             82                   72\n 3 3/1/18  3/4/18  Rasmussen          Adults             67                   59\n 4 2/22/18 2/26/18 Harris Interactive Registered V…      84                   77\n 5 3/3/18  3/5/18  Quinnipiac         Registered V…      78                   63\n 6 3/4/18  3/6/18  YouGov             Registered V…      72                   65\n 7 3/1/18  3/5/18  Morning Consult    Registered V…      76                   72\n 8 2/23/18 2/25/18 YouGov/Huffpost    Registered V…      41                   69\n 9 2/20/18 2/23/18 CBS News           Adults             44                   68\n10 2/27/18 2/28/18 Rasmussen          Adults             43                   71\n# ℹ 47 more rows\n# ℹ 1 more variable: `Democratic Support` &lt;dbl&gt;",
    "crumbs": [
      "Data management",
      "Select and rename columns"
    ]
  },
  {
    "objectID": "data-management/filter-and-arrange-rows.html",
    "href": "data-management/filter-and-arrange-rows.html",
    "title": "Filter and Arrange rows",
    "section": "",
    "text": "For the examples in this tutorial we’ll use the iris dataset, like we explained here.\nlibrary(tidyverse)\nd &lt;- as_tibble(iris)",
    "crumbs": [
      "Data management",
      "Filter and Arrange rows"
    ]
  },
  {
    "objectID": "data-management/filter-and-arrange-rows.html#the-condition-is-a-logical-expression",
    "href": "data-management/filter-and-arrange-rows.html#the-condition-is-a-logical-expression",
    "title": "Filter and Arrange rows",
    "section": "The condition is a logical expression",
    "text": "The condition is a logical expression\nThe condition in filter can be any logical expression. A logical expression is simply a statement that is either TRUE or FALSE. When we use a logical expression in the filter function, we are asking R to evaluate this expression for each row in the tibble. Each row for which the expression evaluates to TRUE is then included in the subset.\nIf you know a bit about how logical expressions work, you will have great control over what rows are included in your subset. Here is an overview of the most important operators for logical expressions.\n\nComparison operators\nComparison operators are used to compare two values.\n\n== equal to\n!= not equal to\n&gt; greater than\n&gt;= greater than or equal to\n&lt; less than\n&lt;= less than or equal to\n%in% is in a list of values (second value must be a list or vector)\n\nExample:\n\n5 &gt; 1    # TRUE:  5 is greater than 1\n\n[1] TRUE\n\n5 &lt; 1    # FALSE: 5 is less than 1\n\n[1] FALSE\n\n\"setosa\" %in% c(\"setosa\", \"versicolor\")  # TRUE: \"setosa\" is in the list\n\n[1] TRUE\n\n\n\n\nLogical operators\nLogical operators are used to combine multiple conditions.\n\n& and\n| or\n! not\n\nExample:\n\n5 &gt; 1 | 5 &lt; 1   # TRUE: 5 is greater than 1 OR 5 is less than 1\n\n[1] TRUE\n\n5 &gt; 1 & 5 &lt; 1   # FALSE: 5 is greater than 1 AND 5 is less than 1\n\n[1] FALSE\n\n!5 &lt; 1          # TRUE: it is not the case that 5 is smaller than 1\n\n[1] TRUE\n\n\n\n\nParentheses\nFor complex conditions, you can use parentheses to group conditions, similar to how you would in a mathematical expression. Say you want to select big flowers, but you want to take into account that some species are generally smaller. So the minimum Sepal.Length for setosa should be 5, but for versicolor it should be 6. You could then write the following condition:\n\nfilter(d, (Species == 'setosa' & Sepal.Length &gt; 5) | \n          (Species == 'versicolor' & Sepal.Length &gt; 6)\n      )",
    "crumbs": [
      "Data management",
      "Filter and Arrange rows"
    ]
  },
  {
    "objectID": "data-management/sorting.html",
    "href": "data-management/sorting.html",
    "title": "Ordering rows",
    "section": "",
    "text": "Open the tidyverse package, and read the data from the URL.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/poll-quiz-guns/guns-polls.csv\"\nd &lt;- read_csv(url)\n\nRows: 57 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Question, Start, End, Pollster, Population, URL\ndbl (3): Support, Republican Support, Democratic Support\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Data management",
      "Ordering rows"
    ]
  },
  {
    "objectID": "data-management/sorting.html#sorting-with-arrange",
    "href": "data-management/sorting.html#sorting-with-arrange",
    "title": "Ordering rows",
    "section": "Sorting with arrange()",
    "text": "Sorting with arrange()\nYou can easily sort a data set with arrange: you first specify the data, and then the column(s) to sort on. To sort in descending order, put a minus in front of a variable. For example, the following orders by population and then by support (descending):\n\nd &lt;- arrange(d, Population, -Support)\nd\n\n# A tibble: 57 × 9\n   Question         Start End   Pollster Population Support `Republican Support`\n   &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n 1 background-chec… 2/27… 2/28… NPR/Ips… Adults          94                   89\n 2 mental-health-o… 2/27… 2/28… NPR/Ips… Adults          92                   88\n 3 age-21           2/27… 2/28… NPR/Ips… Adults          82                   72\n 4 background-chec… 2/20… 2/23… CBS News Adults          75                   66\n 5 stricter-gun-la… 2/27… 2/28… NPR/Ips… Adults          75                   59\n 6 ban-high-capaci… 2/27… 2/28… NPR/Ips… Adults          73                   59\n 7 ban-assault-wea… 2/27… 2/28… NPR/Ips… Adults          72                   58\n 8 age-21           3/1/… 3/4/… Rasmuss… Adults          67                   59\n 9 stricter-gun-la… 2/20… 2/23… CBS News Adults          65                   43\n10 ban-assault-wea… 2/20… 2/23… CBS News Adults          53                   39\n# ℹ 47 more rows\n# ℹ 2 more variables: `Democratic Support` &lt;dbl&gt;, URL &lt;chr&gt;\n\n\nNote that we assigned the result of arranging to the d object again, i.e. we replace the object by its sorted version. If we wouldn’t assign it to anything, it would display it on screen but not remember the sorting. Assigning a result to the same name means I don’t create a new object, preventing the environment from being cluttered (and saving me from the bother of thinking up yet another object name). For sorting, this should generally be fine as the sorted data should contain the same data as before. For subsetting, this means that the rows or columns are actually deleted from the dataset (in memory), so you will have to read the file again (or start from an earlier object) if you need those rows or columns later.",
    "crumbs": [
      "Data management",
      "Ordering rows"
    ]
  },
  {
    "objectID": "data-management/import-and-view.html#iris",
    "href": "data-management/import-and-view.html#iris",
    "title": "Import and view",
    "section": "The iris dataset",
    "text": "The iris dataset\nWe’ll be using this iris dataset in some of the next tutorials, so let’s see what data it contains. From the previous output we see that there are five columns:\n\nSepal.Length: the length of the sepal (the green leaf-like part of the flower)\nSepal.Width: the width of the sepal\nPetal.Length: the length of the petal (the colored part of the flower)\nPetal.Width: the width of the petal\nSpecies: the species of the flower\n\nThe data has 150 rows, so it contains measurements of 150 flowers. The sepal and petal columns are numeric, and measured in centimeters. The species column is a factor with three levels: setosa, versicolor, and virginica.",
    "crumbs": [
      "Data management",
      "Import and view"
    ]
  },
  {
    "objectID": "data-management/import-and-view.html#csv-files",
    "href": "data-management/import-and-view.html#csv-files",
    "title": "Import and view",
    "section": "CSV files",
    "text": "CSV files\n\n\n\n\n\n\nWhat is a CSV file?\n\n\n\n\n\nCSV stands for Comma Separated Values. It is a simple text file, that you can open in any text editor. In order to store a data frame (i.e. data in rows and colums), it simply read every line as a row, and separates the columns by a comma (or sometimes another symbol, like a semicolon). For example, the following CSV file contains a data frame with three columns: resp_id, gender, and height. The first row contains the column names, and the following rows contain the data.\n\nresp_id,gender,height\n1,M,176\n2,M,165\n3,F,172\n4,F,160\n\nThe benefit of this simplicity is that any respectable spreadsheet or statistical software (e.g., Excel, Google sheets, SPSS, Stata) can read it. This makes CSV files a great way to share and store data.\n\n\n\nThe Tidyverse contains a function read_csv that allows you to read a csv file directly into a tibble. You can read a file from your own computer, but also directly from the internet. We’ll walk you through this in three steps:\n\nImport data from the internet\nWrite data to a CSV file on your computer\nRead data from this CSV file back into R\n\n\nImporting data from a URL\nFor this example we download data about gun polls from the data analytics site fivethirtyeight.\n\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/poll-quiz-guns/guns-polls.csv\"\ngunpolls &lt;- read_csv(url)\n\nMake sure to always check whether the data was imported correctly:\n\ngunpolls\n\nYou can also view the data in a larger spreadsheet-like view by using the View function:\n\nView(gunpolls)\n\n\n\nWriting data to a CSV file on your computer\nYou can use the write_csv function to write a tibble to a CSV file on your computer. You need to specify the file path where you want to save it. If you just provide a file name, it will be saved in your current working directory.\n\n\n\n\n\n\nFinding and changing your working directory\n\n\n\n\n\nYour working directory is basically the location (i.e. directory) on your computer where R is currently looking for files. Here we provide a short refresher for how to find and change your working directory. For a more detailed explanation, see the file system introduction.\nTo find your working directory, you can use the getwd() function:\n\ngetwd()\n\n[1] \"/home/kasper/projects/R-canon/data-management\"\n\n\nThe path you see here is a directory (i.e. a folder) on your computer. If you write a file without specifying a specific location, it will be saved in this directory. A good practise is therefore to create a directory where you do all your R stuff, and set this as your working directory.\nThere are two ways to set your working directory:\n\nManually: You can set the working directory with the setwd() function.\n\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.\n\n\n\n\n\nwrite_csv(gunpolls, \"gunpoll_data.csv\")\n\nThis will write the gunpolls tibble to a file called gunpoll_data.csv in your current working directory. Try finding it in your file system!\nNow let’s read this file back into R. Since the file is in your working directory, you can just specify the file name:\n\ngunpolls2 &lt;- read_csv(\"gunpoll_data.csv\")\n\nYou can check and verify that the data (gunpolls2) is indeed identical to the original data (gunpolls).\n\n\n\n\n\n\nCSV pitfalls to avoid\n\n\n\n\n\nThere are two important pitfalls to avoid when working with CSV files:\n\nPitfall 1: Corrupting the file by opening it in Excel\nWhen you download a CSV file from the internet, some computers might immediately ask you whether you want to open it in your default spreadsheet program (e.g., Excel, Numbers). Do not do this, but instead download the file directly to your computer. If you open the file and accidentally save it, it can overwrite the CSV file with a different format. Excel in particular has a habit of breaking CSV files this way.\n\n\nPitfall 2: Different flavours of CSV files\nThere are different flavours of CSV files (for historic reasons). Even though we call them “comma separated values”, the separator is sometimes a semicolon or a tab. And depending on language, the decimal separator can be a comma or a dot. In particular, there are two most common versions of the CSV file. This is why tidyverse has two read_csv functions: read_csv and read_csv2. In general, you can just try read_csv first, and if it doesn’t work, try read_csv2.",
    "crumbs": [
      "Data management",
      "Import and view"
    ]
  },
  {
    "objectID": "data-management/welcome-to-the-tidyverse.html",
    "href": "data-management/welcome-to-the-tidyverse.html",
    "title": "Welcome to the tidyverse",
    "section": "",
    "text": "Installing the tidyverse\nThe tidyverse is collection of R packages that makes it much easier to import, manage and visualize data. To use the tidyverse, you only need to open the tidyverse package, and it will automatically open all of the tidyverse R packages.\nLike any normal package, you need to first install it once:\n\ninstall.packages('tidyverse')\n\nThen in every script that you use the package, you open it with library:\n\nlibrary(tidyverse)\n\nWhen you first run this code, it will print a message that shows you all the packages that it opened for you. Some of the most important ones that we’ll we using are:\n\ntibble. An optimized way for structuring rectangular data (basically: a spreadsheet of rows and columns)\ndplyr. Functions for manipulating tibbles: select and rename columns, filter rows, mutate values, etc.\nreadr. Read data into R.\nggplot2. One of the best visualization tools out there. Check out the gallery\n\n\n\n\n\n\n\nWhat about the ‘Conflicts’?\n\n\n\n\n\nWhen opening the tidyverse, and when opening packages in general, you can get a Conflicts warning. A very common warning for the tidyverse is:\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nSo what does this mean, and should we be worried?\nSince anyone can write new packages for R, it can happen that two packages provide functions with the same name. In this example, we see that the filter function exists in both the dplyr package (which we opened by opening the tidyverse), and in the stats package (which is included in base R). So now R needs to decide which version of the function to use when you type filter(). In this case, it says that the dplyr::filter() masks stats::filter(), meaning that it will now use the dplyr version.\nIn practice, this will rarely be a problem, because you seldom need two versions of a function in the same script. But if you ever do, there is a simple solution. Instead of just using filter(), you can type dplyr::filter() to specifically use this version. In the following code, we use this notation to specifically open the help page for dplyr::filter and stats::filter.\n\n?dplyr::filter()\n?stats::filter()\n\n\n\n\n\n\n\n\n\n\nThe tidyverse versus base R\n\n\n\n\n\nMany of the things that the tidyverse allows you to do are also possible in base R (i.e. the basic installation of R). Base R also provides functions for importing, managing and visualizing data. So why do we need the tidyverse?\nThe tidyverse is an opinionated framework, which means that it doesn’t just enable you to do things, but also suggests how you should do things. The authors have thought long and hard about how to make data management easy, effective and intuitive (they have even written papers about it). This not only makes the tidyverse much easier and intuitive to learn, but also makes sure everyone writes their code in the same way, which improves transparency and shareability.\nThis is different from base R, which is designed to be a highly flexible programming language, that allows you to do almost anything. Accordingly, it is still worthwhile to learn base R at some point if you want to specialize more in computational research methods. But for our Communication Science program, and for many data science applications in general, you can do all your data management in the tidyverse.\n\n\n\n\n\nData management with the tidyverse\nThe tidyverse is built around the concept of tidy data (Wickham 2014). The main principles of tidy data are:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nThis type of data is also called a data frame, or a spreadsheet. What the tidyverse does is provide a set of tools that make it easy to work with this type of data. At the core of this is the tibble data structure. As a simple example, the following code creates a tibble containing respondents, their gender, and their height. We’ll call our tibble d (short for data).\n\nd &lt;- tibble(resp_id = c(  1,   2,   3,   4), \n            gender  = c(\"M\", \"M\", \"F\", \"F\"), \n            height  = c(176, 165, 172, 160))\n\nd\n\n# A tibble: 4 × 3\n  resp_id gender height\n    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1       1 M         176\n2       2 M         165\n3       3 F         172\n4       4 F         160\n\n\nThe vast majority of data that we work with in the social sciences can be structured in this way. The rows then typically represent our units of analysis (e.g., respondents, participants, texts, etc.), and the columns represent the variables that we measure on these units. This makes it imperative for us to learn how we can manage this type of data effectively. We need to be able to select columns, filter rows, mutate values, and summarize data. Sometimes we also need to pivot the data, or join it with other data. In this chapter you will learn how to do all of this with the tidyverse.\n\n\n\n\n\n Back to topReferences\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.",
    "crumbs": [
      "Data management",
      "Welcome to the tidyverse"
    ]
  },
  {
    "objectID": "data-management/welcome-to-the-tidyverse.html#what-packages-does-the-tidyverse-contain",
    "href": "data-management/welcome-to-the-tidyverse.html#what-packages-does-the-tidyverse-contain",
    "title": "Welcome to the tidyverse",
    "section": "",
    "text": "Notice above that when you run library(tidyverse), R prints all the tidyverse packages that it opened for you. Some of the most important ones that we’ll we using are:\n\ntibble. An optimized way for structuring rectangular data (basically: a spreadsheet of rows and columns)\ndplyr. Functions for manipulating tibbles: select and rename columns, filter rows, mutate values, etc.\nreadr. Read data into R.\nggplot2. One of the best visualization tools out there. Check out the gallery\n\n\n\n\n\n\n\nWhat about the ‘Conflicts’?\n\n\n\n\n\nWhen opening the tidyverse, and when opening packages in general, you can get a Conflicts warning. A very common warning for the tidyverse is:\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nSo what does this mean, and should we be worried?\nSince anyone can write new packages for R, it can happen that two packages provide functions with the same name. In this example, we see that the filter function exists in both the dplyr package (which we opened by opening the tidyverse), and in the stats package (which is included in base R). So now R needs to decide which version of the function to use when you type filter(). In this case, it says that the dplyr::filter() masks stats::filter(), meaning that it will now use the dplyr version.\nIn practice, this will rarely be a problem, because you seldom need two versions of a function in the same script. But if you ever do, there is a simple solution. Instead of just using filter(), you can type dplyr::filter() to specifically use this version. In the following code, we use this notation to specifically open the help page for dplyr::filter and stats::filter.\n\n?dplyr::filter()\n?stats::filter()\n\n\n\n\n\n\n\n\n\n\nThe tidyverse versus base R\n\n\n\n\n\nMany of the things that the tidyverse allows you to do are also possible in base R (i.e. the basic installation of R). Base R also provides functions for importing, managing and visualizing data. So why do we need the tidyverse?\nThe tidyverse is an opinionated framework, which means that it doesn’t just enable you to do things, but also suggests how you should do things. The authors have thought long and hard about how to make data management easy, effective and intuitive (they have even written papers about it). This not only makes the tidyverse much easier and intuitive to learn, but also makes sure everyone writes their code in the same way, which improves transparency and shareability.\nThis is different from base R, which is designed to be a highly flexible programming language, that allows you to do almost anything. Accordingly, it is still worthwhile to learn base R at some point if you want to specialize more in computational research methods. But for our Communication Science program, and for many data science applications in general, you can do all your data management in the tidyverse.",
    "crumbs": [
      "Data management",
      "Welcome to the tidyverse"
    ]
  },
  {
    "objectID": "data-management/import-and-view.html#printing-the-tibble",
    "href": "data-management/import-and-view.html#printing-the-tibble",
    "title": "Import and view",
    "section": "",
    "text": "The simplest way to view the data is to just print the tibble by typing d and running the code:\n\nd\n\n# A tibble: 4 × 3\n  resp_id gender height\n    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1       1 M         176\n2       2 M         165\n3       3 F         172\n4       4 F         160\n\n\nThis will print a textual representation of the tibble in the console. If the tibble is too large to fit on the screen, R will only print the first few rows and columns, so you don’t have to worry about blowing up your computer. Note that the tibble also reports the number of rows and columns (4 x 3), and below the names of the columns it shows the data type. &lt;dbl&gt; stands for double, which is a numeric value, and &lt;chr&gt; stands for character, which is a textual value.",
    "crumbs": [
      "Data management",
      "Import and view"
    ]
  },
  {
    "objectID": "data-management/import-and-view.html#functions-for-showing-parts-of-the-tibble",
    "href": "data-management/import-and-view.html#functions-for-showing-parts-of-the-tibble",
    "title": "Import and view",
    "section": "",
    "text": "There are several functions that allow you to show specific parts of the tibble.\n\nhead(d)      # Show the first few rows\ntail(d)      # Show the last few rows\ncolnames(d)  # Show the column names\nrownames(d)  # Show the row names (if they exist)\nncol(d)      # Show the number of columns\nnrow(d)      # Show the number of rows",
    "crumbs": [
      "Data management",
      "Import and view"
    ]
  },
  {
    "objectID": "data-management/import-and-view.html#viewing-the-entire-tibble",
    "href": "data-management/import-and-view.html#viewing-the-entire-tibble",
    "title": "Import and view",
    "section": "",
    "text": "If you want, you can ask R to show the entire tibble. You can do this in two ways:\n\nIn the top-right window in RStudio, open the Environment tab. Here you see all the objects you’ve created, including the tibble d. Click on d to view the entire tibble.\nYou can also do this using code, by running View(d).\n\nThis opens up a convenient window with a spreadsheet, where you can scroll through the data. Note that in the top-bar there is also a Filter button, and a Search bar, which can be very useful for exploring your data.",
    "crumbs": [
      "Data management",
      "Import and view"
    ]
  },
  {
    "objectID": "getting-started/projects-and-workspaces.html",
    "href": "getting-started/projects-and-workspaces.html",
    "title": "Projects and workspaces",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Getting Started",
      "Projects and workspaces"
    ]
  },
  {
    "objectID": "good-to-know/0_use-help-pages.html",
    "href": "good-to-know/0_use-help-pages.html",
    "title": "Function Help Pages",
    "section": "",
    "text": "The help page tells you how you can use the function. For mean, it shows you that the main form is\n\nmean(x, trim = 0, na.rm = FALSE, ...)\n\nWhat this means is that\n\n\n\n Back to top",
    "crumbs": [
      "Good to Know",
      "Function Help Pages"
    ]
  },
  {
    "objectID": "getting-started/files-and-projects.html",
    "href": "getting-started/files-and-projects.html",
    "title": "Files and projects",
    "section": "",
    "text": "When working in R, you will often need to read and save files on your computer. Moreover, when you work with sensive data, such as survey responses, you really need to understand where your data is stored and how to access it. Here we provide a basic introduction to the file system on your computer.\nComputers organize files and directories (or folders) in a hierarchical structure that resembles a tree. This tree-like structure starts from a single root directory and branches out into subdirectories, which can contain more subdirectories or files. On windows the root directory is often C:\\, while on macOS and Linux it’s simply /. It kind of looks like this:\n\n/ (Root Directory) \n│\n├── Documents\n│   ├── Work\n│   │   ├── survey.csv\n│   │   └──  \n│   └── Personal\n│       ├── Resume.pdf\n│       └── Budget.xlsx\n│\n├── Downloads\n│   ├── Report.docx\n│   └── image.jpg\n\nBecause of this structure, any file on your computer has a unique path that describes its location in the file system. For example, the path to the Report.docx file in the Work directory would be /Documents/Work/survey.csv (or C:\\Documents\\Work\\survey.csv on Windows). In reality there are more layers, such as your user directory, but you get the gist.\nTo write or read a file in R, you can specify the file path in two ways: using an absolute path (starting from the root directory) or a relative path (starting from the current working directory).\n\n\nAn absolute path specifies the full path to a file or directory from the root directory. There are several ways to find out the absolute path of a file on your computer:\n\nFrom the file explorer: Open the file explorer on your computer, and navigate to the file. You can then right-click on the file and select Properties (or some similar term). Here you’ll find the Location of the file.\nUsing file.choose(): If you run the code file.choose(), R will open a file explorer window. Here you can browse to the file, and when you select it, R will print the file path in the console window.\nUsing RStudio: For some types of files, you can use the RStudio data importer interface. In the bottom-right window, you can go to the Files tab. Here you can browse to the file on your computer. If the file is something that you can read into R (e.g., a CSV file), RStudio will give you the Import Dataset option. This will open a visual interface for importing the data, that also shows the file path.\n\n\n\n\nWhenever you are working in R, you are always working in a specific directory on your computer. This directory is called the working directory. You can check the current working directory with the getwd() function: This prints the current working directory in your console.\nFor example, say that our current working directory is /Documents/Work (in the example file system above). This allows us to access the survey.csv file with a relative path: survey.csv. Also, if we would now create a new file in R called my_analysis.r, it would be saved as /Documents/Work/my_analysis.r.\nIn other words, by setting our working directory, we can avoid having to write out the full path to a file every time we want to read or write. There are two easy ways to set the working directory in R:\n\nManually: You can set the working directory with the setwd() function. For example, to set the working directory to /Documents/Work, you can run setwd(\"/Documents/Work\").\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.",
    "crumbs": [
      "Getting Started",
      "Files and projects"
    ]
  },
  {
    "objectID": "getting-started/files-and-projects.html#absolute-path",
    "href": "getting-started/files-and-projects.html#absolute-path",
    "title": "Files and projects",
    "section": "",
    "text": "An absolute path specifies the full path to a file or directory from the root directory. There are several ways to find out the absolute path of a file on your computer:\n\nFrom the file explorer: Open the file explorer on your computer, and navigate to the file. You can then right-click on the file and select Properties (or some similar term). Here you’ll find the Location of the file.\nUsing file.choose(): If you run the code file.choose(), R will open a file explorer window. Here you can browse to the file, and when you select it, R will print the file path in the console window.\nUsing RStudio: For some types of files, you can use the RStudio data importer interface. In the bottom-right window, you can go to the Files tab. Here you can browse to the file on your computer. If the file is something that you can read into R (e.g., a CSV file), RStudio will give you the Import Dataset option. This will open a visual interface for importing the data, that also shows the file path.",
    "crumbs": [
      "Getting Started",
      "Files and projects"
    ]
  },
  {
    "objectID": "getting-started/files-and-projects.html#relative-path",
    "href": "getting-started/files-and-projects.html#relative-path",
    "title": "Files and projects",
    "section": "",
    "text": "Whenever you are working in R, you are always working in a specific directory on your computer. This directory is called the working directory. You can check the current working directory with the getwd() function: This prints the current working directory in your console.\nFor example, say that our current working directory is /Documents/Work (in the example file system above). This allows us to access the survey.csv file with a relative path: survey.csv. Also, if we would now create a new file in R called my_analysis.r, it would be saved as /Documents/Work/my_analysis.r.\nIn other words, by setting our working directory, we can avoid having to write out the full path to a file every time we want to read or write. There are two easy ways to set the working directory in R:\n\nManually: You can set the working directory with the setwd() function. For example, to set the working directory to /Documents/Work, you can run setwd(\"/Documents/Work\").\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.",
    "crumbs": [
      "Getting Started",
      "Files and projects"
    ]
  },
  {
    "objectID": "data-management/cleaning-data.html",
    "href": "data-management/cleaning-data.html",
    "title": "Basic data",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Data management",
      "Basic data"
    ]
  },
  {
    "objectID": "data-management/2_summarizing.html",
    "href": "data-management/2_summarizing.html",
    "title": "Summarizing",
    "section": "",
    "text": "The functions used in the earlier part on data preparation worked on individual rows. Sometimes, you need to compute properties of groups of rows (cases). This is called aggregation (or summarization) and in tidyverse uses the group_by function followed by either summarize or mutate.",
    "crumbs": [
      "Data management",
      "Summarizing"
    ]
  },
  {
    "objectID": "data-management/2_summarizing.html#grouping-rows",
    "href": "data-management/2_summarizing.html#grouping-rows",
    "title": "Summarizing",
    "section": "Grouping rows",
    "text": "Grouping rows\nNow, we can use the group_by function to group by, for example, pollster:\n\nd %&gt;% \n  group_by(Question)\n\n# A tibble: 57 × 8\n# Groups:   Question [8]\n   Question     Start   End     Pollster          Population Support   Rep   Dem\n   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;             &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 age-21       2/20/18 2/23/18 CNN/SSRS          Registere…      72    61    86\n 2 age-21       2/27/18 2/28/18 NPR/Ipsos         Adults          82    72    92\n 3 age-21       3/1/18  3/4/18  Rasmussen         Adults          67    59    76\n 4 age-21       2/22/18 2/26/18 Harris Interacti… Registere…      84    77    92\n 5 age-21       3/3/18  3/5/18  Quinnipiac        Registere…      78    63    93\n 6 age-21       3/4/18  3/6/18  YouGov            Registere…      72    65    80\n 7 age-21       3/1/18  3/5/18  Morning Consult   Registere…      76    72    86\n 8 arm-teachers 2/23/18 2/25/18 YouGov/Huffpost   Registere…      41    69    20\n 9 arm-teachers 2/20/18 2/23/18 CBS News          Adults          44    68    20\n10 arm-teachers 2/27/18 2/28/18 Rasmussen         Adults          43    71    24\n# ℹ 47 more rows\n\n\nAs you can see, the data itself didn’t actually change yet, it merely recorded (at the top) that we are now grouping by Question, and that there are 8 groups (different questions) in total.",
    "crumbs": [
      "Data management",
      "Summarizing"
    ]
  },
  {
    "objectID": "data-management/2_summarizing.html#summarizing",
    "href": "data-management/2_summarizing.html#summarizing",
    "title": "Summarizing",
    "section": "Summarizing",
    "text": "Summarizing\nTo summarize, you follow the group_by with a call to summarize. Summarize has a syntax that is similar to mutate: summarize(column = calculation, ...). The crucial difference, however, is that you always need to use a function in the calculation, and that function needs to compute a single summary value given a vector of values. Very common summarization functions are sum, mean, and sd (standard deviation).\nFor example, the following computes the average support per question (and sorts by descending support):\n\nd %&gt;% \n  group_by(Question) %&gt;%                    # group by \"Questions\"\n  summarize(Support = mean(Support)) %&gt;%    # average \"Support\" per group\n  arrange(-Support)                         # sort based on \"Support\"\n\n# A tibble: 8 × 2\n  Question                    Support\n  &lt;chr&gt;                         &lt;dbl&gt;\n1 background-checks              87.4\n2 mental-health-own-gun          85.8\n3 age-21                         75.9\n4 ban-high-capacity-magazines    67.3\n5 stricter-gun-laws              66.5\n6 ban-assault-weapons            61.8\n7 arm-teachers                   42  \n8 repeal-2nd-amendment           10  \n\n\nAs you can see, summarize drastically changes the shape of the data. There are now rows equal to the number of groups (8), and the only columns left are the grouping variables and the summarized values.\nYou can also compute summaries of multiple values, and even do ad hoc calculations:\n\nd %&gt;% \n  group_by(Question) %&gt;% \n  summarize(Dem = mean(Dem), \n            Rep = mean(Rep), \n            diff = mean(Dem-Rep)) %&gt;% \n  arrange(-diff)\n\n# A tibble: 8 × 4\n  Question                      Dem   Rep    diff\n  &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 stricter-gun-laws            86.9  46.4  40.5  \n2 ban-assault-weapons          80.7  42.6  38.1  \n3 ban-high-capacity-magazines  83.9  52.7  31.1  \n4 age-21                       86.4  67    19.4  \n5 repeal-2nd-amendment         15     5    10    \n6 background-checks            91.9  83.6   8.29 \n7 mental-health-own-gun        87.5  86.7   0.833\n8 arm-teachers                 17.2  72.2 -55    \n\n\nSo, Democrats are more in favor of all proposed gun laws except arming teachers.\nYou can also compute multiple summaries of a single value. Another useful function is n() (without arguments), which simply counts the values in each group. For example, the following gives the count, mean, and standard deviation of the support:\n\nd %&gt;% \n  group_by(Question) %&gt;% \n  summarize(n = n(),\n            mean = mean(Support), \n            sd = sd(Support))\n\n# A tibble: 8 × 4\n  Question                        n  mean    sd\n  &lt;chr&gt;                       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 age-21                          7  75.9  6.01\n2 arm-teachers                    6  42    1.55\n3 background-checks               7  87.4  7.32\n4 ban-assault-weapons            12  61.8  6.44\n5 ban-high-capacity-magazines     7  67.3  3.86\n6 mental-health-own-gun           6  85.8  5.46\n7 repeal-2nd-amendment            1  10   NA   \n8 stricter-gun-laws              11  66.5  5.15\n\n\nNote: As you can see, one of the values has a missing value (NA) for standard deviation. Why?",
    "crumbs": [
      "Data management",
      "Summarizing"
    ]
  },
  {
    "objectID": "data-management/2_summarizing.html#using-mutate-with-group_by",
    "href": "data-management/2_summarizing.html#using-mutate-with-group_by",
    "title": "Summarizing",
    "section": "Using mutate with group_by",
    "text": "Using mutate with group_by\nThe examples above all reduce the number of cases to the number of groups. Another option is to use mutate after a group_by, which allows you to add summary values to the rows themselves.\nFor example, suppose we wish to see whether a certain poll has a different prediction from the average polling of that question. We can group_by question and then use mutate to calculate the average support:\n\nd2 &lt;- d %&gt;% \n  group_by(Question) %&gt;%\n  mutate(avg_support = mean(Support), \n         diff = Support - avg_support)\nd2\n\n# A tibble: 57 × 10\n# Groups:   Question [8]\n   Question     Start  End   Pollster Population Support   Rep   Dem avg_support\n   &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 age-21       2/20/… 2/23… CNN/SSRS Registere…      72    61    86        75.9\n 2 age-21       2/27/… 2/28… NPR/Ips… Adults          82    72    92        75.9\n 3 age-21       3/1/18 3/4/… Rasmuss… Adults          67    59    76        75.9\n 4 age-21       2/22/… 2/26… Harris … Registere…      84    77    92        75.9\n 5 age-21       3/3/18 3/5/… Quinnip… Registere…      78    63    93        75.9\n 6 age-21       3/4/18 3/6/… YouGov   Registere…      72    65    80        75.9\n 7 age-21       3/1/18 3/5/… Morning… Registere…      76    72    86        75.9\n 8 arm-teachers 2/23/… 2/25… YouGov/… Registere…      41    69    20        42  \n 9 arm-teachers 2/20/… 2/23… CBS News Adults          44    68    20        42  \n10 arm-teachers 2/27/… 2/28… Rasmuss… Adults          43    71    24        42  \n# ℹ 47 more rows\n# ℹ 1 more variable: diff &lt;dbl&gt;\n\n\nAs you can see, where summarize reduces the rows and columns to the groups and summaries, mutate adds a new column which is identical for all rows within a group.",
    "crumbs": [
      "Data management",
      "Summarizing"
    ]
  },
  {
    "objectID": "data-management/2_summarizing.html#ungrouping",
    "href": "data-management/2_summarizing.html#ungrouping",
    "title": "Summarizing",
    "section": "Ungrouping",
    "text": "Ungrouping\nFinally, you can use ungroup to get rid of any groupings.\nFor example, the data produced by the example above is still grouped by Question as mutate does not remove grouping information. So, if we want to compute the overall standard deviation of the difference we could ungroup and then summarize:\n\nd2 %&gt;% \n  ungroup() %&gt;% \n  summarize(diff = sd(diff))\n\n# A tibble: 1 × 1\n   diff\n  &lt;dbl&gt;\n1  5.19\n\n\n(of course, running sd(d2$diff)) would yield the same result.)\nIf you run the same command without the ungroup, what would the result be? Why?",
    "crumbs": [
      "Data management",
      "Summarizing"
    ]
  },
  {
    "objectID": "data-management/4_reshaping.html",
    "href": "data-management/4_reshaping.html",
    "title": "Long and Wide data",
    "section": "",
    "text": "This tutorial discusses how to reshape data, particularly from long to wide format and vice versa. It mostly follows Chapter 12 of the R4DS book, but uses the pivot_longer and pivot_wider functions that replace gather and spread1. At the time of writing these functions are not yet in the book, but the writers explain the change and the new functions here.",
    "crumbs": [
      "Data management",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/4_reshaping.html#pivot-longer-wide-to-long",
    "href": "data-management/4_reshaping.html#pivot-longer-wide-to-long",
    "title": "Long and Wide data",
    "section": "Pivot longer (wide to long)",
    "text": "Pivot longer (wide to long)\nWe will tidy this data in three steps. First, we pivot_longer the columns into a single column with all measurements. Then, we separate the country from the measurement level. Finally, we pivot_wider the measurement levels to columns again (since they are measurements on the same observation).\nThe first step is the same as above: we gather all columns except for the year column into a single column:\n\nwealth = pivot_longer(wealth_raw, -Year, names_to=\"key\", values_to=\"value\")\nwealth\n\n# A tibble: 315 × 3\n    Year key                             value\n   &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt;\n 1  1810 France: top decile              0.799\n 2  1810 France: top percentile          0.456\n 3  1810 France: top promille            0.171\n 4  1810 Paris: top percentile           0.537\n 5  1810 United Kingdom: top decile      0.829\n 6  1810 United Kingdom: top percentile  0.549\n 7  1810 United States: top decile       0.58 \n 8  1810 United States: top percentile   0.25 \n 9  1810 United States: top promille    NA    \n10  1810 Sweden: top decile              0.839\n# ℹ 305 more rows",
    "crumbs": [
      "Data management",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/4_reshaping.html#separating-columns-splitting-one-column-into-two",
    "href": "data-management/4_reshaping.html#separating-columns-splitting-one-column-into-two",
    "title": "Long and Wide data",
    "section": "Separating columns (splitting one column into two)",
    "text": "Separating columns (splitting one column into two)\nThe next step is to split the ‘key’ column into two columns, for country and for measurement. This can be done using the separate command, for which you specify the column to split, the new column names, and what separator to split on:\n\nwealth = separate(wealth, key, into = c(\"country\",\"measurement\"), sep=\":\")\nwealth\n\n# A tibble: 315 × 4\n    Year country        measurement        value\n   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;              &lt;dbl&gt;\n 1  1810 France         \" top decile\"      0.799\n 2  1810 France         \" top percentile\"  0.456\n 3  1810 France         \" top promille\"    0.171\n 4  1810 Paris          \" top percentile\"  0.537\n 5  1810 United Kingdom \" top decile\"      0.829\n 6  1810 United Kingdom \" top percentile\"  0.549\n 7  1810 United States  \" top decile\"      0.58 \n 8  1810 United States  \" top percentile\"  0.25 \n 9  1810 United States  \" top promille\"   NA    \n10  1810 Sweden         \" top decile\"      0.839\n# ℹ 305 more rows\n\n\nThe measurement column is quoted in the output because it stars with a space. We could resolve this by specifying sep=\": \" (i.e. adding the space to the separator). We can also solve this by changing the column after the split with mutate. The code below removes the space using the trimws (trim white space) function:\n\nwealth %&gt;% mutate(measurement = trimws(measurement))\n\n# A tibble: 315 × 4\n    Year country        measurement     value\n   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt;\n 1  1810 France         top decile      0.799\n 2  1810 France         top percentile  0.456\n 3  1810 France         top promille    0.171\n 4  1810 Paris          top percentile  0.537\n 5  1810 United Kingdom top decile      0.829\n 6  1810 United Kingdom top percentile  0.549\n 7  1810 United States  top decile      0.58 \n 8  1810 United States  top percentile  0.25 \n 9  1810 United States  top promille   NA    \n10  1810 Sweden         top decile      0.839\n# ℹ 305 more rows\n\n\nWe can also use sub to search and replace (substitute) within a column, in this case changing ” top ” into “capital_top_”:\n\nwealth = wealth %&gt;% mutate(measurement = sub(\" top \", \"capital_top_\", measurement))\nwealth\n\n# A tibble: 315 × 4\n    Year country        measurement             value\n   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;                   &lt;dbl&gt;\n 1  1810 France         capital_top_decile      0.799\n 2  1810 France         capital_top_percentile  0.456\n 3  1810 France         capital_top_promille    0.171\n 4  1810 Paris          capital_top_percentile  0.537\n 5  1810 United Kingdom capital_top_decile      0.829\n 6  1810 United Kingdom capital_top_percentile  0.549\n 7  1810 United States  capital_top_decile      0.58 \n 8  1810 United States  capital_top_percentile  0.25 \n 9  1810 United States  capital_top_promille   NA    \n10  1810 Sweden         capital_top_decile      0.839\n# ℹ 305 more rows",
    "crumbs": [
      "Data management",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/4_reshaping.html#pivot-wider-long-to-wide",
    "href": "data-management/4_reshaping.html#pivot-wider-long-to-wide",
    "title": "Long and Wide data",
    "section": "Pivot wider (long to wide)",
    "text": "Pivot wider (long to wide)\nThe wealth data above is now ‘too long’ to be tidy: the measurement for each country is spread over multiple rows, listing the three different measurement levels (decile, percentile, promille). In effect, we want to undo one level of gathering, by spreading the column over multiple columns.\nThs syntax for the spread call is similar to that for pivot_longer: pivot_wider(data, names_from=key_column, values_from=value_column). Before we had the arguments names_to and values_to, to specify the column names of the new stacked (i.e. long format) columns. This time, we have the names_from and values_from arguments to reverse the process. For each unique value in the names_from column a new column will be created, with the corresponding value in the values_from column in the cell.\n\nwealth = pivot_wider(wealth, names_from=measurement, values_from=value)\nwealth\n\n# A tibble: 126 × 5\n    Year country  capital_top_decile capital_top_percentile capital_top_promille\n   &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;                &lt;dbl&gt;\n 1  1810 France                0.799                  0.456                0.171\n 2  1810 Paris                NA                      0.537               NA    \n 3  1810 United …              0.829                  0.549               NA    \n 4  1810 United …              0.58                   0.25                NA    \n 5  1810 Sweden                0.839                  0.559               NA    \n 6  1810 Europe                0.822                  0.521               NA    \n 7  1820 France                0.818                  0.467                0.190\n 8  1820 Paris                NA                      0.590               NA    \n 9  1820 United …             NA                     NA                   NA    \n10  1820 United …             NA                     NA                   NA    \n# ℹ 116 more rows\n\n\nSo now each row contains three measurements (columns, variables) relating to each observation (country x year).",
    "crumbs": [
      "Data management",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/4_reshaping.html#tidyness-as-a-matter-of-perception",
    "href": "data-management/4_reshaping.html#tidyness-as-a-matter-of-perception",
    "title": "Long and Wide data",
    "section": "Tidyness as a matter of perception",
    "text": "Tidyness as a matter of perception\nAs a last exercise, suppose we would like to plot wealth and capital inequality in the same figure as separate lines. You can do this with two separate geom_line commands, and e.g. use a dashed line for income inequality:\n\nggplot(inequality) + geom_line(aes(x=Year, y=capital_top_decile, colour=country)) + \n  geom_line(aes(x=Year, y=income_topdecile, colour=country), linetype=\"dashed\")\n\nWarning: Removed 4 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\nThis works, but it would be nice if we could specify the measurement as colour (or type) and have ggplot automatically make the legend. To do this, the different measurements need to be in rows rather than in columns. In other words, data that is tidy from one perspective can be ‘too wide’ for another.\nLet’s gather the data into a single column, and plot the result for the US:\n\ninequality2 = pivot_longer(inequality, income_topdecile:capital_top_promille, names_to=\"measurement\", values_to=\"value\")\n\ninequality2 %&gt;% \n  filter(country==\"US\") %&gt;% \n  ggplot() + geom_line(aes(x=Year, y=value, linetype=measurement))\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n\n\nWe can also plot only top-decile capital and income in a paneled plot. Note the use of extra options to set legend location and title, vertical label, and main title text and location (horizontal justification):\n\ninequality2 %&gt;% \n  filter(measurement %in% c(\"income_topdecile\", \"capital_top_decile\") & country != \"Europe\") %&gt;% \n  ggplot() + geom_line(aes(x=Year, y=value, linetype=measurement)) + facet_wrap(~ country, nrow = 2) +\n  scale_linetype_discrete(name=\"Variable:\", labels=c(\"Capital\", \"Income\")) +\n  theme(legend.position=\"bottom\", plot.title = element_text(hjust = 0.5)) +\n  ylab(\"Inequality\") + \n  ggtitle(\"Capital and income inequality over time\")",
    "crumbs": [
      "Data management",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/4_reshaping.html#footnotes",
    "href": "data-management/4_reshaping.html#footnotes",
    "title": "Long and Wide data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe replacement of spread and gather with pivot_wider and pivot_longer is a recent change, so you might still see spread and gather used in code from other. As such, it is still usefull to have a look at how spread and gather work (which is very similar to pivot_wider and pivot_longer). However, make sure to use the new pivot_ functions in your own code, because spread and gather are on their way out.↩︎",
    "crumbs": [
      "Data management",
      "Long and Wide data"
    ]
  },
  {
    "objectID": "data-management/mutate.html",
    "href": "data-management/mutate.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Data management",
      "Mutate"
    ]
  },
  {
    "objectID": "data-management/6_strings.html",
    "href": "data-management/6_strings.html",
    "title": "Textual data",
    "section": "",
    "text": "The goal of this tutorial is to get you acquainted with basic string handling in R. A large part of this uses the stringr included in the Tidyverse. See also chapter 14 of R for Data Science and the stringr cheat sheet\nNote that ‘string’ is not an official word in R (which uses character to denote textual data), but since it’s the word used in most documentations I will also use strings to refer to objects containing textual data. (AFAIK, the name originates from seeing a text as a string or sequence of characters)",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/6_strings.html#combining-strings",
    "href": "data-management/6_strings.html#combining-strings",
    "title": "Textual data",
    "section": "Combining strings",
    "text": "Combining strings\nTo combine two strings, you can use str_c (which is equivalent to built-in paste0):\n\nstr_c(\"john\", \"mary\")\n\n[1] \"johnmary\"\n\nstr_c(\"john\", \"mary\", sep = \" & \")\n\n[1] \"john & mary\"\n\n\nIt can also work of longer vectors, where shorter vectors are repeated as needed:\n\nnames = c(\"john\", \"mary\")\nstr_c(\"Hello, \", names)\n\n[1] \"Hello, john\" \"Hello, mary\"\n\n\nFinally, you can also ask it to collapse longer vectors after the initial pasting:\n\nstr_c(names, collapse=\" & \")\n\n[1] \"john & mary\"\n\nstr_c(\"Hello, \", names, collapse=\" and \")\n\n[1] \"Hello, john and Hello, mary\"",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/6_strings.html#subsetting-strings",
    "href": "data-management/6_strings.html#subsetting-strings",
    "title": "Textual data",
    "section": "Subsetting strings",
    "text": "Subsetting strings\nTo take a fixed subset of a string, you can use str_sub. This can be useful, for example, to strip the time part off dates:\n\ndates = c(\"2019-04-01T12:00\", \"2012-07-29 01:12\")\nstr_sub(dates, start = 1, end = 10)\n\n[1] \"2019-04-01\" \"2012-07-29\"\n\n\nYou can also replace a substring, for example to make sure the ‘T’ notation is used in the dates:\n\nstr_sub(dates, start=11, end=11) = \"T\"\ndates\n\n[1] \"2019-04-01T12:00\" \"2012-07-29T01:12\"",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/6_strings.html#finding-patterns",
    "href": "data-management/6_strings.html#finding-patterns",
    "title": "Textual data",
    "section": "Finding patterns",
    "text": "Finding patterns\nRegular expressions can be used e.g. to find rows containing a specific pattern. For example, if we had a data frame containing the earlier texts, we can filter for rows containing an email address:\n\nt = tibble(id=1:3, text=txt)\nt\n\n# A tibble: 3 × 2\n     id text                                 \n  &lt;int&gt; &lt;chr&gt;                                \n1     1 Hi, I'm Bob                          \n2     2 my email address  is  Bob@example.com\n3     3 A #hashtag for the #millenials       \n\nt |&gt; filter(str_detect(text, regex_email))\n\n# A tibble: 1 × 2\n     id text                                 \n  &lt;int&gt; &lt;chr&gt;                                \n1     2 my email address  is  Bob@example.com\n\n\nYou can also str_count to count how many matches of a pattern are found in each text:\n\nt |&gt; mutate(n_hashtags = str_count(text, \"#\\\\w+\"))\n\n# A tibble: 3 × 3\n     id text                                  n_hashtags\n  &lt;int&gt; &lt;chr&gt;                                      &lt;int&gt;\n1     1 Hi, I'm Bob                                    0\n2     2 my email address  is  Bob@example.com          0\n3     3 A #hashtag for the #millenials                 2",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/6_strings.html#replacing-patterns",
    "href": "data-management/6_strings.html#replacing-patterns",
    "title": "Textual data",
    "section": "Replacing patterns",
    "text": "Replacing patterns\nYou can also use regular expressions to do find-an-replace. For example, you can remove all punctionation, normalize whitespace, or redact email addresses:\n\nt |&gt; mutate(nopunct = str_replace_all(text, \"[^\\\\w ]\", \"\"),\n             normalized = str_replace_all(text, \"\\\\s+\", \" \"),\n             redacted = str_replace_all(text, \"\\\\w+@\", \"****@\"),\n             text = NULL)\n\n# A tibble: 3 × 4\n     id nopunct                             normalized                  redacted\n  &lt;int&gt; &lt;chr&gt;                               &lt;chr&gt;                       &lt;chr&gt;   \n1     1 Hi Im Bob                           Hi, I'm Bob                 Hi, I'm…\n2     2 my email address  is  Bobexamplecom my email address is Bob@ex… my emai…\n3     3 A hashtag for the millenials        A #hashtag for the #millen… A #hash…\n\n\nNote the use of setting text to NULL as an alternative way to drop a column. In textr, most functions have a _all variant which replaces/finds/extracts all matches, rather than just the first.",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/6_strings.html#extracting-patterns",
    "href": "data-management/6_strings.html#extracting-patterns",
    "title": "Textual data",
    "section": "Extracting patterns",
    "text": "Extracting patterns\nBesides replacing patterns, it can also be useful to extract elements from a string, for example the email or hashtag:\n\nregex_email = regex(\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+\")\nt |&gt; mutate(email = str_extract(text, regex_email),\n             hashtag = str_extract(text, \"#\\\\w+\"))\n\n# A tibble: 3 × 4\n     id text                                  email           hashtag \n  &lt;int&gt; &lt;chr&gt;                                 &lt;chr&gt;           &lt;chr&gt;   \n1     1 Hi, I'm Bob                           &lt;NA&gt;            &lt;NA&gt;    \n2     2 my email address  is  Bob@example.com Bob@example.com &lt;NA&gt;    \n3     3 A #hashtag for the #millenials        &lt;NA&gt;            #hashtag\n\n\nNote that for the hashtag, it only extracted the first hit. You can use the str_extract_all function, but since it can match zero, once, or more often per text, it returns a list containing all matches per row (or more correctly, per element of the input vector):\n\nstr_extract_all(t$text, \"#\\\\w+\")\n\n[[1]]\ncharacter(0)\n\n[[2]]\ncharacter(0)\n\n[[3]]\n[1] \"#hashtag\"    \"#millenials\"\n\n\nThe best way to deal with this in the context of a data frame is to use unnest_longer to turn the list into a long format. First, use mutate to create a column containing the lists (so this is a column which itself contains complex data)\n\nhashtags = mutate(t, tags = str_extract_all(t$text, \"#\\\\w+\"))\nhashtags\n\n# A tibble: 3 × 3\n     id text                                  tags     \n  &lt;int&gt; &lt;chr&gt;                                 &lt;list&gt;   \n1     1 Hi, I'm Bob                           &lt;chr [0]&gt;\n2     2 my email address  is  Bob@example.com &lt;chr [0]&gt;\n3     3 A #hashtag for the #millenials        &lt;chr [2]&gt;\n\n\nAs you can see, the tags column has ‘list’ as its type, with the last row containing two elements. To turn this into a more usable dataframe, we ‘unnest’ it into a ‘long’ format using unnest_longer:\n\nhashtags |&gt; unnest_longer(\"tags\")\n\n# A tibble: 2 × 3\n     id text                           tags       \n  &lt;int&gt; &lt;chr&gt;                          &lt;chr&gt;      \n1     3 A #hashtag for the #millenials #hashtag   \n2     3 A #hashtag for the #millenials #millenials\n\n\nAs you can see, this creates a new data frame with one row per hash tag.",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/6_strings.html#using-separate",
    "href": "data-management/6_strings.html#using-separate",
    "title": "Textual data",
    "section": "Using separate",
    "text": "Using separate\nFirst, if the number of elements is known and you can use multiple you can use separate to separate the column into seperate columns:\n\nd |&gt; separate(person, into=c(\"firstname\", \"lastname\"), sep=\" \")\n\n# A tibble: 2 × 3\n  firstname lastname books                                  \n  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                                  \n1 Liam      Jones    The great Gatsby, To kill a Mockingbird\n2 Olivia    Smith    Pride and Prejudice, 1984, Moby Dick   \n\n\nAs said, this is mostly useful if a column always contains a fixed number of data points that each have a distinct meaning, e.g. first and last name or city and state.",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/6_strings.html#using-str_split",
    "href": "data-management/6_strings.html#using-str_split",
    "title": "Textual data",
    "section": "Using str_split",
    "text": "Using str_split\nIf there is variable number of data points such as the list of books in the data set above, you can use str_split, which takes a regular expression argument to split the column:\n\nd |&gt; mutate(books = str_split(books, pattern=\",\"))\n\n# A tibble: 2 × 2\n  person       books    \n  &lt;chr&gt;        &lt;list&gt;   \n1 Liam Jones   &lt;chr [2]&gt;\n2 Olivia Smith &lt;chr [3]&gt;\n\n\nJust like above, this produces a column of type ‘list’ which contains multiple values per row. To normalize this, use unnest_longer as above:\n\nd |&gt; mutate(books = str_split(books, pattern=\",\")) |&gt; unnest_longer(books)\n\n# A tibble: 5 × 2\n  person       books                   \n  &lt;chr&gt;        &lt;chr&gt;                   \n1 Liam Jones   \"The great Gatsby\"      \n2 Liam Jones   \" To kill a Mockingbird\"\n3 Olivia Smith \"Pride and Prejudice\"   \n4 Olivia Smith \" 1984\"                 \n5 Olivia Smith \" Moby Dick\"            \n\n\nAs you can also see, the spaces around the book titles are not removed. You can fix this in two ways, either by changing the pattern to incluide optional whitespace (\\\\w is white space, * means the preceding element is optional and can be repeated); or by adding a trimws call afterwards:\n\nd |&gt; mutate(books = str_split(books, pattern=\"\\\\s*,\\\\s*\")) |&gt; unnest_longer(books)\n\n# A tibble: 5 × 2\n  person       books                \n  &lt;chr&gt;        &lt;chr&gt;                \n1 Liam Jones   The great Gatsby     \n2 Liam Jones   To kill a Mockingbird\n3 Olivia Smith Pride and Prejudice  \n4 Olivia Smith 1984                 \n5 Olivia Smith Moby Dick            \n\nd |&gt; mutate(books = str_split(books, pattern=\",\")) |&gt; unnest_longer(books) |&gt; mutate(books=trimws(books))\n\n# A tibble: 5 × 2\n  person       books                \n  &lt;chr&gt;        &lt;chr&gt;                \n1 Liam Jones   The great Gatsby     \n2 Liam Jones   To kill a Mockingbird\n3 Olivia Smith Pride and Prejudice  \n4 Olivia Smith 1984                 \n5 Olivia Smith Moby Dick",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/6_strings.html#background",
    "href": "data-management/6_strings.html#background",
    "title": "Textual data",
    "section": "Background",
    "text": "Background\nA fairly short version of the story is as follows: when computers were mostly dealing with English text, life was easy, as there are not a lot of different letters and they could easily assign each letter and some punctuation marks to a number below 128, so it could be stored as 7 bits. For example, A is number 65. This encoding was called ‘ASCII’.\nIt turned out, however, that many people needed more than 26 letters, for example to write accented letters. For this reason, the 7 bits were expanded to 8, and many accented latin letters were added. This representation is called latin-1, also known as ISO-8859-1.\nOf course, many languages don’t use the latin script, so other 8-bit encodings were invented to deal with Cyrillic, Arabic, and other scripts. Most of these are based on ASCII, meaning that 65 still refers to ‘A’ in e.g. the Hebrew encoding. However, character 228 could refer to greek δ, cyrillic ф, or hebrew ה. Things get even more complex if you consider Chinese, where you can’t fit all characters in 256 numbers, so several larger (multi-byte) encodings were used.\nThis can cause a lot of confusion if you read a text that was encoding in e.g. greek as if it were encoded in Hebrew. A famous example of this confusion is that Microsoft Exchange used the WingDings font and encoding for rendering symbols in emails, amongst others using character 74 as a smiley. For non-exchange users (who didn’t have that font), however, it renders as the ASCII character nr 74: “J”. So, if you see an email from a microsoft user with a J where you expected a smiley, now you know :).\nTo end this confusion, unicode was invented, which assigns a unique number (called a code point) to each letter. A is still 65 (or “1” in hexadecimal R notataion), but δ is now uniquely “3B4”, and ф is uniquely “444”. There are over 1M possible unicode characters, of which about 100 thousand have been currently assigned. This gives enough room for Chinese, old Nordic runes, and even Klingon to be encoded.\nYou can directly use these in an R string:\n\n\"Some Unicode letters: \\u41 \\u03B4 \\u0444\"\n\n[1] \"Some Unicode letters: A δ ф\"\n\n\nNow, to be able to write all 1M characters to string, one would need almost 24 bits per character, tripling the storage and memory needed to handle most text. So, more efficient encodings were invented that would normally take only 8 or 16 bits per character, but can take more bits if needed. So, while the problem of defining characters is solved, unfortunately you still need to know the actual encoding of a text. Fortunately, UTF-8 (which uses 1 byte for latin characters, but more for non-western text) is emerging as a de facto standard for most texts. This is a compromise which is most efficient for latin alphabeters, but is still able to unambiguously express all languages.\nIt is still quite common, however, to encounter text in other encodings, so it can be good to understand what problems you can face and how to deal with them",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/6_strings.html#text-encoding-in-r",
    "href": "data-management/6_strings.html#text-encoding-in-r",
    "title": "Textual data",
    "section": "Text encoding in R",
    "text": "Text encoding in R\nTo show how this works in R, we can use the charToRaw function to see how a character is encoded in R:\n\ncharToRaw('A')\n\n[1] 41\n\n\nNote that the output of this function depends on your regional settings (called ‘locale’). On most computers, this should produce 41 however, as most encodings are based on ASCII.\nFor other alphabets it can be more tricky. The Chinese character “蘭” (unicode “62d”) on my computer is expressed in UTF-8, where it takes 3 bytes:",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "data-management/6_strings.html#dealing-with-encodings",
    "href": "data-management/6_strings.html#dealing-with-encodings",
    "title": "Textual data",
    "section": "Dealing with encodings",
    "text": "Dealing with encodings\nTo convert between encodings, you can use the iconv function. For example, to express the Chinese character above in GB2312 (Chinese national standard) encoding:\n\ncharToRaw(iconv('蘭', to='GB2312'))\n\n[1] 4e 41\n\n\nThe most common way of dealing with encodings is to ignore the problem and hope it goes away. However, outside the English world this is often not an option. Also, due to general unicode ignorance many people will use the wrong encoding, and you will even see things like double-utf8-encoded text.\nThe sane way to deal with encodings is to make sure that all text stored inside your program is encoded in a standard encoding, presumably UTF-8. This means that whenever you read text from an external source, you need to convert it to UTF-8 if it isn’t already in that form.\nThis means that when you use read_csv (on text data) or readtext, you should ideally always specify which encoding the text is encoded in:\n\nreadtext::readtext(\"file.txt\", encoding = \"utf-8\")\nread_csv(\"file.csv\", locale=locale(encoding='utf-8'))\n\nIf you don’t know what encoding a text is in, you can try utf-8 and the most common local encodings (e.g. latin-1 in many western countries), you can inspect the raw bytes, or you can use the guessEncoding function from readr:\n\nguess_encoding(\"file.txt\")",
    "crumbs": [
      "Data management",
      "Textual data"
    ]
  },
  {
    "objectID": "getting-started/functions.html#using-pipeond-way-is-to-pipe-the-first-argument-into-the-function-argument1-function_nameargument2-...",
    "href": "getting-started/functions.html#using-pipeond-way-is-to-pipe-the-first-argument-into-the-function-argument1-function_nameargument2-...",
    "title": "Functions",
    "section": "Using pipeond way is to pipe the first argument into the function: argument1 |> function_name(argument2, ...)",
    "text": "Using pipeond way is to pipe the first argument into the function: argument1 |&gt; function_name(argument2, ...)\nIf this is your first time seeing pipes, you’re probably wondering why you would want to do this? Why bother having two ways to do the exact same thing? The reason is that when writing code, you shouldn’t just think about what the code does, but also about how easy the code is to read. This not only helps you prevent mistakes, but also makes your analysis transparent.\nAs you’ll see later, you’ll encounter many cases where your analysis requires you to string together multiple functions. In these cases, pipes make your code much easier to read. let’s rewrite our code from above using the pipe notation:\n\nx_with_missing |&gt; mean(na.rm=T)\n\nNotice how our first argument, the required argument x_with_missing, is piped into the mean function. Inside the mean function, we only specify the second argument, the optional argument na.rm.\nNow imagine we would want to round the result (2.5) up to a round number (3). We can do this without pipe notation, but it would be quite ugly and hard to read:\n\nround(mean(x_with_missing, na.rm=T))\n\nThe pipe notation allows us to break this down into a nice pipeline:\n\nx_with_missing |&gt; \n  mean(na.rm=T) |&gt; \n  round()",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "data-management/3_visualization.html",
    "href": "data-management/3_visualization.html",
    "title": "Visualization basics",
    "section": "",
    "text": "This tutorial teaches the basics of data visualization using the ggplot2 package (included in tidyverse). For more information, see R4DS Chapter 3: Da`ta Visualization and R4DS Chapter 7: Exploratory Data Analysis.\nFor many cool visualization examples using gplot2 (with R code included!) see the R Graph Gallery. For inspiration (but unfortunately no R code), there is also a 538 blog post on data visualization from 2016. Finally, see the article on ‘the grammar of graphics’ published by Hadley Wickham for more insight into the ideas behind ggplot.",
    "crumbs": [
      "Data management",
      "Visualization basics"
    ]
  },
  {
    "objectID": "data-management/3_visualization.html#important-note-on-ggplot-command-syntax",
    "href": "data-management/3_visualization.html#important-note-on-ggplot-command-syntax",
    "title": "Visualization basics",
    "section": "Important note on ggplot command syntax",
    "text": "Important note on ggplot command syntax\nFor the plot to work, R needs to execute the whole ggplot call and all layers as a single statement. Practically, that means that if you combine a plot over multiple lines, the plus sign needs to be at the end of the line, so R knows more is coming. The general syntax is always:\n\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n\nSo, the following is good:\n\nggplot(data = facts_state) + \n  geom_point(mapping = aes(x = college, y = income))\n\n\n\n\n\n\n\n\nBut this is not:\n\nggplot(data = facts_state) \n  + geom_point(mapping = aes(x = college, y = income))\n\nAlso note that the data and mapping arguments are the first arguments the functions expect, so you can also leave them out:\n\nggplot(facts_state) + \n  geom_point(aes(x = college, y = income))",
    "crumbs": [
      "Data management",
      "Visualization basics"
    ]
  },
  {
    "objectID": "data-management/3_visualization.html#other-aesthetics",
    "href": "data-management/3_visualization.html#other-aesthetics",
    "title": "Visualization basics",
    "section": "Other aesthetics",
    "text": "Other aesthetics\nTo find out which visual elements can be used in a layer, use e.g. ?geom_point. According to the help file, we can (among others) set the colour, alpha (transparency), and size of points. Let’s first set the size of points to the (log) population of each state, creating a bubble plot:\n\nggplot(data = facts_state) + \n  geom_point(aes(x = college, y = income, size = population))\n\n\n\n\n\n\n\n\nSince it is difficult to see overlapping points, let’s make all points somewhat transparent. Note: Since we want to set the alpha of all points to a single value, this is not a mapping (as it is not mapped to a column from the data frame), but a constant. These are set outside the mapping argument:\n\nggplot(data = facts_state) + \n  geom_point(aes(x = college, y = income, size = population), \n             alpha = .5, \n             colour = \"red\")\n\n\n\n\n\n\n\n\nInstead of setting colour to a constant value, we can also let it vary with the data. For example, we can colour the states by percentage of population that is identified as ‘white’:\n\nggplot(data = facts_state) + \n  geom_point(aes(x=college, y=income, size=population, colour=white), \n             alpha=.9)\n\n\n\n\n\n\n\n\nFinally, you can map to a categorical value as well. Let’s categorize states into whether population is growing (at least 1%) or stable or declining. We use the if_else(condition, iftrue, iffalse) function, which assigns the iftrue value if the condition is true, and iffalse otherwise:\n\nfacts_state &lt;- facts_state %&gt;% \n  mutate(growth = ifelse(pop_change &gt; 1, \"Growing\", \"Stable\"))\n\nggplot(data=facts_state) + \n  geom_point(aes(x = college, y = income, size = population, colour = growth), \n             alpha=.9)\n\n\n\n\n\n\n\n\nAs you can see in these examples, ggplot tries to be smart about the mapping you ask. It automatically sets the x and y ranges to the values in your data. It mapped the size such that there are small and large points, but not e.g. a point so large that it would dominate the graph. For the colour, for interval variables it created a colour scale, while for a categorical variable it automatically assigned a colour to each group.\nOf course, each of those choices can be customized, and sometimes it makes a lot of sense to do so. For example, you might wish to use red for republicans and blue for democrats, if your audience is used to those colors; or you may wish to use grayscale for an old-fashioned paper publication. We’ll explore more options in a later tutorial, but for now let’s be happy that ggplot does a lot of work for us!",
    "crumbs": [
      "Data management",
      "Visualization basics"
    ]
  },
  {
    "objectID": "data-management/3_visualization.html#setting-graph-options",
    "href": "data-management/3_visualization.html#setting-graph-options",
    "title": "Visualization basics",
    "section": "Setting graph options",
    "text": "Setting graph options\nSome options, like labels, legends, and the coordinate system are graph-wide rather than per layer. You add these options to the graph by adding extra functions to the call. For example, we can use coord_flip() to swap the x and y axes:\n\nggplot(nh_gop) + \n  geom_col(aes(x=candidate, y=votes)) +\n  coord_flip()\n\n\n\n\n\n\n\n\nYou can also reorder categories with the fct_reorder function, for example to sort by number of votes. Also, let’s add some colour (just because we can!):\n\nggplot(nh_gop) + \n  geom_bar(aes(x=fct_reorder(candidate, votes), y=votes, fill=candidate), \n           stat='identity') + \n  coord_flip()\n\n\n\n\n\n\n\n\n(Note: this works because ggplot assumes all labels are factors, which have an ordering; you can use other functions from the forcats package (generally starting with fct_) to do other things such as reversing the order, manually specifying the order, etc).\nThis is getting somewhere, but the y-axis label is not very pretty and we don’t need guides for the fill mapping. This can be remedied by more graph-level options. Also, we can use a theme to alter the appearance of the graph, for example using the minimal theme:\n\nggplot(nh_gop) + \n  geom_bar(aes(x=reorder(candidate, votes), y=votes, fill=candidate), \n           stat='identity') + \n  coord_flip() + \n  xlab(\"Candidate\") + \n  guides(fill=\"none\") + \n  theme_minimal()",
    "crumbs": [
      "Data management",
      "Visualization basics"
    ]
  },
  {
    "objectID": "data-management/3_visualization.html#grouped-bar-plots",
    "href": "data-management/3_visualization.html#grouped-bar-plots",
    "title": "Visualization basics",
    "section": "Grouped bar plots",
    "text": "Grouped bar plots\nWe can also add groups to bar plots. For example, we can set the x category to state (taking only NH and IA to keep the plot readable), and then group by candidate:\n\ngop2 &lt;- results_state %&gt;% \n  filter(party == \"Republican\" & (state == \"New Hampshire\" | state == \"Iowa\")) \nggplot(gop2) + geom_col(aes(x=state, y=votes, fill=candidate))\n\n\n\n\n\n\n\n\nBy default, the groups are stacked. This can be controlled with the position parameter, which can be dodge (for grouped bars) or fill (stacking to 100%): (note that the position is a constant, not an aesthetic mapping, so it goes outside the aes argument)\n\nggplot(gop2) + geom_col(aes(x=state, y=votes, fill=candidate), position='dodge')\n\n\n\n\n\n\n\nggplot(gop2) + geom_col(aes(x=state, y=votes, fill=candidate), position='fill')\n\n\n\n\n\n\n\n\nOf course, you can also make the grouped bars add up to 100% by computing the proportion manually, which can give you a bit more control over the process.\nNote that the example below pipes the preprocessing output directly into the ggplot command, that is, it doesn’t create a new temporary data set like gop2 above. This is entirely a stylistic choice, but can be useful for operations that are only intended for a single visualization.\n\ngop2 %&gt;% \n  group_by(state) %&gt;% \n  mutate(vote_prop=votes/sum(votes)) %&gt;%\n  ggplot() + \n    geom_col(aes(x=state, y=vote_prop, fill=candidate), position='dodge') + \n    ylab(\"Votes (%)\")\n\n\n\n\n\n\n\n\nNote that where group_by %&gt;% summarize replaces the data frame by a summarization, group_by %&gt;% mutate adds a column to the existing data frame, using the grouped values for e.g. sums. See our tutorial on Data Summarization for more details.",
    "crumbs": [
      "Data management",
      "Visualization basics"
    ]
  },
  {
    "objectID": "data-management/5_joining.html",
    "href": "data-management/5_joining.html",
    "title": "Joining data",
    "section": "",
    "text": "In many cases, you need to combine data from multiple data sources. For example, you can combine a sentiment analysis of tweets with metadata about the tweets; or data on election results with data about the candidates ideological positions or details on the races.\nThis tutorial will teach you the inner_join and other _join commands used to combine two data sets on shared columns. See R4DS Chapter 13: Relational Data for more information and examples.\n\n\nFor this tutorial, we will look at data describing the US presidential primaries. These data can be downloaded from the Houston Data Visualisation github page, who in turn got it from Kaggle.\nIn the CSV folder on the github, you can find (among others)\n\nprimary_results.csv Number of votes in the primary per county per candidate\nprimary_schedule.csv Dates of each primary per state and per party\ncounty_facts.csv Information about the counties and states, including population, ethnicity, age, etc.\n\nFor many research questions, we need to be able to combine the data from these files. For example, we might want to know if Clinton did better in counties or states with more women (needing results and facts), or how Trump’s performance evolved over time (requiring results and calendar).\n\n\n\nBefore we start, let’s download the three data files:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncsv_folder_url &lt;- \"https://raw.githubusercontent.com/houstondatavis/data-jam-august-2016/master/csv\"\nresults &lt;- read_csv(paste(csv_folder_url, \"primary_results.csv\", sep = \"/\"))\n\nRows: 24611 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): state, state_abbreviation, county, party, candidate\ndbl (3): fips, votes, fraction_votes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfacts &lt;- read_csv(paste(csv_folder_url, \"county_facts.csv\", sep = \"/\"))\n\nRows: 3195 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): area_name, state_abbreviation\ndbl (52): fips, Pop_2014_count, Pop_2010_base_count, Pop_change_pct, Pop_201...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nschedule  &lt;- read_csv(paste(csv_folder_url, \"primary_schedule.csv\", sep = \"/\"))\n\nRows: 113 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): date, state, party, type\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote: I use paste to join the base url with the filenames, using a / as a separator.\nHave a look at all three data sets. Before we proceed, there are some things we want to do. First, the facts data frame is really large, with 54 columns. Let’s select a couple interesting ones to work with:\n\nfacts_subset &lt;- facts %&gt;% \n  select(area_name, \n         population = Pop_2014_count, \n         pop_change = Pop_change_pct, \n         over65 = Age_over_65_pct, \n         female = Sex_female_pct, \n         white = Race_white_pct, \n         college = Pop_college_grad_pct, \n         income = Income_per_capita)\n\nNext, the schedule dates are now a character (textual) field rather than date, so let’s fix that using the as.Date function, specifying the dates to be formatted as month/day/year:\n\nschedule &lt;- schedule %&gt;% \n  mutate(date = as.Date(date, format=\"%m/%d/%y\"))\n\nLast, let’s create a data set with per-state (rather than per-country) election results using group_by and summarize:\n\nresults_state &lt;- results %&gt;% \n  group_by(state, party, candidate) %&gt;% \n  summarize(votes = sum(votes))\n\n`summarise()` has grouped output by 'state', 'party'. You can override using\nthe `.groups` argument.\n\nresults_state\n\n# A tibble: 290 × 4\n# Groups:   state, party [95]\n   state   party      candidate        votes\n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 Alabama Democrat   Bernie Sanders   76399\n 2 Alabama Democrat   Hillary Clinton 309928\n 3 Alabama Republican Ben Carson       87517\n 4 Alabama Republican Donald Trump    371735\n 5 Alabama Republican John Kasich      37970\n 6 Alabama Republican Marco Rubio     159802\n 7 Alabama Republican Ted Cruz        180608\n 8 Alaska  Democrat   Bernie Sanders     440\n 9 Alaska  Democrat   Hillary Clinton     99\n10 Alaska  Republican Ben Carson        2401\n# ℹ 280 more rows\n\n\nNote: see R-tidy-5-transformations if you are unsure about the transformations above!",
    "crumbs": [
      "Data management",
      "Joining data"
    ]
  },
  {
    "objectID": "data-management/5_joining.html#data",
    "href": "data-management/5_joining.html#data",
    "title": "Joining data",
    "section": "",
    "text": "For this tutorial, we will look at data describing the US presidential primaries. These data can be downloaded from the Houston Data Visualisation github page, who in turn got it from Kaggle.\nIn the CSV folder on the github, you can find (among others)\n\nprimary_results.csv Number of votes in the primary per county per candidate\nprimary_schedule.csv Dates of each primary per state and per party\ncounty_facts.csv Information about the counties and states, including population, ethnicity, age, etc.\n\nFor many research questions, we need to be able to combine the data from these files. For example, we might want to know if Clinton did better in counties or states with more women (needing results and facts), or how Trump’s performance evolved over time (requiring results and calendar).",
    "crumbs": [
      "Data management",
      "Joining data"
    ]
  },
  {
    "objectID": "data-management/5_joining.html#downloading-and-preparing-the-data",
    "href": "data-management/5_joining.html#downloading-and-preparing-the-data",
    "title": "Joining data",
    "section": "",
    "text": "Before we start, let’s download the three data files:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncsv_folder_url &lt;- \"https://raw.githubusercontent.com/houstondatavis/data-jam-august-2016/master/csv\"\nresults &lt;- read_csv(paste(csv_folder_url, \"primary_results.csv\", sep = \"/\"))\n\nRows: 24611 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): state, state_abbreviation, county, party, candidate\ndbl (3): fips, votes, fraction_votes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfacts &lt;- read_csv(paste(csv_folder_url, \"county_facts.csv\", sep = \"/\"))\n\nRows: 3195 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): area_name, state_abbreviation\ndbl (52): fips, Pop_2014_count, Pop_2010_base_count, Pop_change_pct, Pop_201...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nschedule  &lt;- read_csv(paste(csv_folder_url, \"primary_schedule.csv\", sep = \"/\"))\n\nRows: 113 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): date, state, party, type\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote: I use paste to join the base url with the filenames, using a / as a separator.\nHave a look at all three data sets. Before we proceed, there are some things we want to do. First, the facts data frame is really large, with 54 columns. Let’s select a couple interesting ones to work with:\n\nfacts_subset &lt;- facts %&gt;% \n  select(area_name, \n         population = Pop_2014_count, \n         pop_change = Pop_change_pct, \n         over65 = Age_over_65_pct, \n         female = Sex_female_pct, \n         white = Race_white_pct, \n         college = Pop_college_grad_pct, \n         income = Income_per_capita)\n\nNext, the schedule dates are now a character (textual) field rather than date, so let’s fix that using the as.Date function, specifying the dates to be formatted as month/day/year:\n\nschedule &lt;- schedule %&gt;% \n  mutate(date = as.Date(date, format=\"%m/%d/%y\"))\n\nLast, let’s create a data set with per-state (rather than per-country) election results using group_by and summarize:\n\nresults_state &lt;- results %&gt;% \n  group_by(state, party, candidate) %&gt;% \n  summarize(votes = sum(votes))\n\n`summarise()` has grouped output by 'state', 'party'. You can override using\nthe `.groups` argument.\n\nresults_state\n\n# A tibble: 290 × 4\n# Groups:   state, party [95]\n   state   party      candidate        votes\n   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 Alabama Democrat   Bernie Sanders   76399\n 2 Alabama Democrat   Hillary Clinton 309928\n 3 Alabama Republican Ben Carson       87517\n 4 Alabama Republican Donald Trump    371735\n 5 Alabama Republican John Kasich      37970\n 6 Alabama Republican Marco Rubio     159802\n 7 Alabama Republican Ted Cruz        180608\n 8 Alaska  Democrat   Bernie Sanders     440\n 9 Alaska  Democrat   Hillary Clinton     99\n10 Alaska  Republican Ben Carson        2401\n# ℹ 280 more rows\n\n\nNote: see R-tidy-5-transformations if you are unsure about the transformations above!",
    "crumbs": [
      "Data management",
      "Joining data"
    ]
  },
  {
    "objectID": "data-management/real-world-examples.html",
    "href": "data-management/real-world-examples.html",
    "title": "Real world examples",
    "section": "",
    "text": "In the previous tutorials we’ve mostly worked with the iris dataset, which is a very clean and simple dataset. However, in the real world you’ll often work with datasets that are much larger and more complex. Here we’ll show you some examples of how you can use the techniques you just learned to work with real world data from the field of communication science.\n\n\n\n Back to top",
    "crumbs": [
      "Data management",
      "Real world examples"
    ]
  },
  {
    "objectID": "data-management/import-and-view.html#viewing-the-tibble",
    "href": "data-management/import-and-view.html#viewing-the-tibble",
    "title": "Import and view",
    "section": "",
    "text": "The simplest way to view the data is to just print the tibble by typing d and running the code:\n\nd\n\n# A tibble: 4 × 3\n  resp_id gender height\n    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1       1 M         176\n2       2 M         165\n3       3 F         172\n4       4 F         160\n\n\nThis will print a textual representation of the tibble in the console. If the tibble is too large to fit on the screen, R will only print the first few rows and columns, so you don’t have to worry about blowing up your computer. Note that the tibble also reports the number of rows and columns (4 x 3), and below the names of the columns it shows the data type. &lt;dbl&gt; stands for double, which is a numeric value, and &lt;chr&gt; stands for character, which is a textual value.",
    "crumbs": [
      "Data management",
      "Import and view"
    ]
  },
  {
    "objectID": "data-management/import-and-view.html#viewing-a-summary-of-every-column",
    "href": "data-management/import-and-view.html#viewing-a-summary-of-every-column",
    "title": "Import and view",
    "section": "Viewing a summary of every column",
    "text": "Viewing a summary of every column\nOften, the main information that you’re interested in is the summary of each column. You can compute this yourself (as we’ll show later), but there is also a nice package called summarytools that can generate a handy overview. You can create the summary as follows (remember to install the package first):\n\nlibrary(summarytools)\ndfSummary(d)\n\nThis will print the summary in your console, but you can also render it in a more readable format with the view function.\n\nview(dfSummary(d))\n\n\n\n\nData Frame Summary\nd\nDimensions: 57 x 9\n  Duplicates: 0\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n\n1\nQuestion [character]\n\n\n\n1. age-21\n\n\n2. arm-teachers\n\n\n3. background-checks\n\n\n4. ban-assault-weapons\n\n\n5. ban-high-capacity-magazin\n\n\n6. mental-health-own-gun\n\n\n7. repeal-2nd-amendment\n\n\n8. stricter-gun-laws\n\n\n\n\n\n\n7\n(\n12.3%\n)\n\n\n6\n(\n10.5%\n)\n\n\n7\n(\n12.3%\n)\n\n\n12\n(\n21.1%\n)\n\n\n7\n(\n12.3%\n)\n\n\n6\n(\n10.5%\n)\n\n\n1\n(\n1.8%\n)\n\n\n11\n(\n19.3%\n)\n\n\n\n\n57 (100.0%)\n0 (0.0%)\n\n\n2\nStart [character]\n\n\n\n1. 2/20/18\n\n\n2. 2/27/18\n\n\n3. 3/1/18\n\n\n4. 2/15/18\n\n\n5. 2/18/18\n\n\n6. 2/25/18\n\n\n7. 3/3/18\n\n\n8. 2/16/18\n\n\n9. 2/22/18\n\n\n10. 3/4/18\n\n\n[ 2 others ]\n\n\n\n\n\n\n12\n(\n21.1%\n)\n\n\n8\n(\n14.0%\n)\n\n\n7\n(\n12.3%\n)\n\n\n5\n(\n8.8%\n)\n\n\n5\n(\n8.8%\n)\n\n\n5\n(\n8.8%\n)\n\n\n5\n(\n8.8%\n)\n\n\n4\n(\n7.0%\n)\n\n\n2\n(\n3.5%\n)\n\n\n2\n(\n3.5%\n)\n\n\n2\n(\n3.5%\n)\n\n\n\n\n57 (100.0%)\n0 (0.0%)\n\n\n3\nEnd [character]\n\n\n\n1. 3/5/18\n\n\n2. 2/23/18\n\n\n3. 2/28/18\n\n\n4. 2/19/18\n\n\n5. 2/20/18\n\n\n6. 2/27/18\n\n\n7. 2/24/18\n\n\n8. 2/26/18\n\n\n9. 3/6/18\n\n\n10. 2/18/18\n\n\n[ 3 others ]\n\n\n\n\n\n\n11\n(\n19.3%\n)\n\n\n9\n(\n15.8%\n)\n\n\n9\n(\n15.8%\n)\n\n\n8\n(\n14.0%\n)\n\n\n5\n(\n8.8%\n)\n\n\n5\n(\n8.8%\n)\n\n\n2\n(\n3.5%\n)\n\n\n2\n(\n3.5%\n)\n\n\n2\n(\n3.5%\n)\n\n\n1\n(\n1.8%\n)\n\n\n3\n(\n5.3%\n)\n\n\n\n\n57 (100.0%)\n0 (0.0%)\n\n\n4\nPollster [character]\n\n\n\n1. YouGov\n\n\n2. Morning Consult\n\n\n3. Quinnipiac\n\n\n4. NPR/Ipsos\n\n\n5. CNN/SSRS\n\n\n6. CBS News\n\n\n7. Rasmussen\n\n\n8. Suffolk\n\n\n9. ABC/Washington Post\n\n\n10. Harris Interactive\n\n\n[ 4 others ]\n\n\n\n\n\n\n12\n(\n21.1%\n)\n\n\n11\n(\n19.3%\n)\n\n\n8\n(\n14.0%\n)\n\n\n7\n(\n12.3%\n)\n\n\n5\n(\n8.8%\n)\n\n\n4\n(\n7.0%\n)\n\n\n2\n(\n3.5%\n)\n\n\n2\n(\n3.5%\n)\n\n\n1\n(\n1.8%\n)\n\n\n1\n(\n1.8%\n)\n\n\n4\n(\n7.0%\n)\n\n\n\n\n57 (100.0%)\n0 (0.0%)\n\n\n5\nPopulation [character]\n\n\n\n1. Adults\n\n\n2. Registered Voters\n\n\n\n\n\n\n13\n(\n22.8%\n)\n\n\n44\n(\n77.2%\n)\n\n\n\n\n57 (100.0%)\n0 (0.0%)\n\n\n6\nSupport [numeric]\n\n\n\nMean (sd) : 67.8 (16)\n\n\nmin ≤ med ≤ max:\n\n\n10 ≤ 68 ≤ 97\n\n\nIQR (CV) : 15 (0.2)\n\n\n\n34 distinct values\n\n57 (100.0%)\n0 (0.0%)\n\n\n7\nRepublican Support [numeric]\n\n\n\nMean (sd) : 59.7 (19.8)\n\n\nmin ≤ med ≤ max:\n\n\n5 ≤ 59 ≤ 97\n\n\nIQR (CV) : 33 (0.3)\n\n\n\n41 distinct values\n\n57 (100.0%)\n0 (0.0%)\n\n\n8\nDemocratic Support [numeric]\n\n\n\nMean (sd) : 77.2 (23.7)\n\n\nmin ≤ med ≤ max:\n\n\n10 ≤ 85 ≤ 99\n\n\nIQR (CV) : 10 (0.3)\n\n\n\n27 distinct values\n\n57 (100.0%)\n0 (0.0%)\n\n\n9\nURL [character]\n\n\n\n1. https://www.ipsos.com/en-\n\n\n2. https://morningconsult.co\n\n\n3. http://cdn.cnn.com/cnn/20\n\n\n4. https://d25d2506sfb94s.cl\n\n\n5. https://d25d2506sfb94s.cl\n\n\n6. https://poll.qu.edu/natio\n\n\n7. https://drive.google.com/\n\n\n8. https://morningconsult.co\n\n\n9. https://poll.qu.edu/image\n\n\n10. http://www.suffolk.edu/do\n\n\n[ 10 others ]\n\n\n\n\n\n\n7\n(\n12.3%\n)\n\n\n6\n(\n10.5%\n)\n\n\n5\n(\n8.8%\n)\n\n\n5\n(\n8.8%\n)\n\n\n5\n(\n8.8%\n)\n\n\n5\n(\n8.8%\n)\n\n\n4\n(\n7.0%\n)\n\n\n4\n(\n7.0%\n)\n\n\n3\n(\n5.3%\n)\n\n\n2\n(\n3.5%\n)\n\n\n11\n(\n19.3%\n)\n\n\n\n\n57 (100.0%)\n0 (0.0%)\n\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.4.1)2024-08-26\n\n\n\n\n\n\n\n\n\nUsing summarytools in Quarty or RMarkdown\n\n\n\n\n\nIf you’re working in Quarto or RMarkdown, the normal way of using summarytools doesn’t create a nice table. Instead, what you need to do is tell Quarto to render the output as HTML. You can do this by setting the method argument to render.\n\nprint(dfSummary(iris), \n      method='render', \n      max.tbl.height = 500)\n\nNote that we also set the max.tbl.height argument to 500, to make sure that very large tables are put inside a scrollable window.",
    "crumbs": [
      "Data management",
      "Import and view"
    ]
  },
  {
    "objectID": "data-management/import-and-view.html#reading-other-file-formats-like-excel-and-spss",
    "href": "data-management/import-and-view.html#reading-other-file-formats-like-excel-and-spss",
    "title": "Import and view",
    "section": "Reading other file formats, like Excel and SPSS",
    "text": "Reading other file formats, like Excel and SPSS\nNot that you know how to read and write CSV files, reading other file formats is a piece of cake. It works almost the same way, but you just need to download a package that can read the file format.\nFor example, to read an Excel file, you can use the readxl package, which provides the read_excel function. To read an SPSS file, you can use the haven package, which provides the read_sav function.",
    "crumbs": [
      "Data management",
      "Import and view"
    ]
  },
  {
    "objectID": "data-management/import-and-export.html",
    "href": "data-management/import-and-export.html",
    "title": "Import and export data",
    "section": "",
    "text": "TLDR summary\n\n\n\n\n\nOne of the most common formats for storing and sharing rectangular data (i.e., data in rows and columns) is the csv format. With the read_csv and write_csv functions you can easily read and write CSV files in R.\n\nlibrary(tidyverse)\nd  &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")  # read CSV file from the internet\nwrite_csv(d, \"my_data.csv\")                            # write to a CSV file on your computer\nd2 &lt;- read_csv(\"my_data.csv\")                          # read CSV file from your own computer",
    "crumbs": [
      "Data management",
      "Import and export data"
    ]
  },
  {
    "objectID": "data-management/import-and-export.html#csv-files",
    "href": "data-management/import-and-export.html#csv-files",
    "title": "Import and export data",
    "section": "CSV files",
    "text": "CSV files\n\n\n\n\n\n\nWhat is a CSV file?\n\n\n\n\n\nCSV stands for Comma Separated Values. It is a simple text file, that you can open in any text editor. In order to store a data frame (i.e. data in rows and colums), it simply read every line as a row, and separates the columns by a comma (or sometimes another symbol, like a semicolon).\nFor example, the following CSV file contains a data frame with three columns: resp_id, gender, and height. The first row contains the column names, and the following rows contain the data.\n\nresp_id,gender,height\n1,M,176\n2,M,165\n3,F,172\n4,F,160\n\nThe benefit of this simplicity is that any respectable spreadsheet or statistical software (e.g., Excel, Google sheets, SPSS, Stata) can read it. This makes CSV files a great way to share and store data.\nAnd just in case you’re worried, yes, CSV can also handle textual data. It uses some tricks to make sure that commas inside the text are not interpreted as column separators.\n\n\n\nThe Tidyverse contains a function read_csv that allows you to read a csv file directly into a tibble. You can read a file from your own computer, but also directly from the internet. We’ll walk you through this in three steps:\n\nImport data from the internet\nWrite data to a CSV file on your computer\nRead data from this CSV file back into R\n\n\nImporting data from a URL\nThe following code downloads the\n\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/poll-quiz-guns/guns-polls.csv\"\ngunpolls &lt;- read_csv(url)\n\nMake sure to always check whether the data was imported correctly:\n\ngunpolls\n\nYou can also view the data in a larger spreadsheet-like view by using the View function:\n\nView(gunpolls)\n\n\n\nWriting data to a CSV file on your computer\nYou can use the write_csv function to write a tibble to a CSV file on your computer. You need to specify the file path where you want to save it. If you just provide a file name, it will be saved in your current working directory.\n\n\n\n\n\n\nFinding and changing your working directory\n\n\n\n\n\nYour working directory is basically the location (i.e. directory) on your computer where R is currently looking for files. Here we provide a short refresher for how to find and change your working directory. For a more detailed explanation, see the file system introduction.\nTo find your working directory, you can use the getwd() function:\n\ngetwd()\n\n[1] \"/home/kasper/projects/R-canon/data-management\"\n\n\nThe path you see here is a directory (i.e. a folder) on your computer. If you write a file without specifying a specific location, it will be saved in this directory. A good practise is therefore to create a directory where you do all your R stuff, and set this as your working directory.\nThere are two ways to set your working directory:\n\nManually: You can set the working directory with the setwd() function.\n\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.\n\n\n\n\n\nwrite_csv(gunpolls, \"gunpoll_data.csv\")\n\nThis will write the gunpolls tibble to a file called gunpoll_data.csv in your current working directory. Try finding it in your file system!\nNow let’s read this file back into R. Since the file is in your working directory, you can just specify the file name:\n\ngunpolls2 &lt;- read_csv(\"gunpoll_data.csv\")\n\nYou can check and verify that the data (gunpolls2) is indeed identical to the original data (gunpolls).\n\n\n\n\n\n\nCSV pitfalls to avoid\n\n\n\n\n\nThere are two important pitfalls to avoid when working with CSV files:\n\nPitfall 1: Corrupting the file by opening it in Excel\nWhen you download a CSV file from the internet, some computers might immediately ask you whether you want to open it in your default spreadsheet program (e.g., Excel, Numbers). Do not do this, but instead download the file directly to your computer. If you open the file and accidentally save it, it can overwrite the CSV file with a different format. Excel in particular has a habit of breaking CSV files this way.\n\n\nPitfall 2: Different flavours of CSV files\nThere are different flavours of CSV files (for historic reasons). Even though we call them “comma separated values”, the separator is sometimes a semicolon or a tab. And depending on language, the decimal separator can be a comma or a dot. In particular, there are two most common versions of the CSV file. This is why tidyverse has two read_csv functions: read_csv and read_csv2. In general, you can just try read_csv first, and if it doesn’t work, try read_csv2.",
    "crumbs": [
      "Data management",
      "Import and export data"
    ]
  },
  {
    "objectID": "data-management/import-and-export.html#reading-other-file-formats-like-excel-and-spss",
    "href": "data-management/import-and-export.html#reading-other-file-formats-like-excel-and-spss",
    "title": "Import and export data",
    "section": "Reading other file formats, like Excel and SPSS",
    "text": "Reading other file formats, like Excel and SPSS\nNow that you know how to read and write CSV files, reading other file formats is a piece of cake. It works almost the same way, but you just need to download a package that can read the file format.\nFor instance, to read an Excel file, you can use the readxl package, which provides the read_excel function. To read an SPSS file, you can use the haven package, which provides the read_sav function. You might have to take care of some additional details, such as the sheet name in the Excel file, or the variable labels in the SPSS file. But once you’ve got the hang of managing your data with the tidyverse, you’ll be able to handle any data frames formats that come your way.",
    "crumbs": [
      "Data management",
      "Import and export data"
    ]
  },
  {
    "objectID": "3_statistics/0_general-concepts/index.html",
    "href": "3_statistics/0_general-concepts/index.html",
    "title": "General Concepts",
    "section": "",
    "text": "In this folder we can put explanations of general concepts, like p-values, distributions, etc.\n\n\n\n Back to top",
    "crumbs": [
      "Statistics",
      "General Concepts"
    ]
  },
  {
    "objectID": "3_statistics/2_tests/0_selecting-the-right-test.html",
    "href": "3_statistics/2_tests/0_selecting-the-right-test.html",
    "title": "Selecting the right test",
    "section": "",
    "text": "On this page we can explain when to use what test, and link to the right page.\n\n\n\n Back to top",
    "crumbs": [
      "Statistics",
      "Tests",
      "Selecting the right test"
    ]
  },
  {
    "objectID": "3_statistics/2_tests/index.html",
    "href": "3_statistics/2_tests/index.html",
    "title": "Tests",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "3_statistics/2_tests/t-test.html",
    "href": "3_statistics/2_tests/t-test.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Statistics",
      "Tests",
      "T Test"
    ]
  },
  {
    "objectID": "3_statistics/1_causality.html",
    "href": "3_statistics/1_causality.html",
    "title": "Causality",
    "section": "",
    "text": "xkcd: correlation",
    "crumbs": [
      "Statistics",
      "Causality"
    ]
  },
  {
    "objectID": "3_statistics/1_causality.html#correlation-versus-causation",
    "href": "3_statistics/1_causality.html#correlation-versus-causation",
    "title": "Causality",
    "section": "Correlation versus causation",
    "text": "Correlation versus causation\n\nCausation\nCausation implies a cause-and-effect relationship, where a change in one variable (the cause) leads to a change in the other (the effect). If we were to somehow manipulate the cause (which we could do in an experiment), we would expect to see a corresponding change in the effect.\nFor instance, if we could somehow make the news media talk more about climate change, we would expect the public to think and talk about it more, too. Basically, this is why PR exists, and why politicians try to get media to talk about the issues they care about.\n\n\nCorrelation\nCorrelation, on the other hand, merely indicates an association or relationship between two variables. When two variables are correlated, they tend to change together. This can be due to a causal relationship in either or both directions, but it could also be due to a third variable that causes both of them to change. This is known as a confounding variable, and a correlation that results from a confounding variable is called a spurious correlation.\n\n\n\n\n\n\nFamous spurious correlations\n\n\n\n\n\nA famous example is that across European countries the number of storks is quite strongly correlated with the number of newborn babies (\\(\\rho = 0.62\\)). The confounder in this case is the size of the country: larger countries have more storks and more babies. So the correlation is not evidence for the folk theory that storks deliver babies.\nSpurious correlations can also just be coincidental. Especially when we look at many different variables over time, we are likely to find some that are correlated just by chance. A whole website, Spurious Correlations, is dedicated to finding statistical gems, such as the correlation between the number of people who drowned by falling into a pool and the number of films Nicolas Cage appeared in. This is why it’s important to have a theoretical foundation for the relationships we test, and always be cautious when interpreting statistical results.\n\n\n\nIn the case of screen time and psychological well-being study, the authors did observe an “association”, but were not able to establish which variable causes the other. Based on their data, “it is not possible to determine if screen time leads to low well-being, low well-being leads to screen time, or both” (Twenge and Campbell 2018, 281). Indeed, it could be that children who are already unhappy end up spending more time on screens, which would mean that taking away their phone might not help (and might even make things worse). Furthermore, possible confounders could be that parents who are more involved in their children’s lives might both limit screen time and help develop better psychological well-being.",
    "crumbs": [
      "Statistics",
      "Causality"
    ]
  },
  {
    "objectID": "3_statistics/1_causality.html#how-to-establish-causality",
    "href": "3_statistics/1_causality.html#how-to-establish-causality",
    "title": "Causality",
    "section": "How to establish causality",
    "text": "How to establish causality\nSo when we observe a correlation between two variables, how can we determine whether there is a causal relationship between them?\n\nThe gold standard: controlled experiments\nThe gold standard for establishing causation is the randomized controlled experiment, in which the researcher manipulates the cause and observes the effect, while keeping all other variables constant. Since the treatment group and the control group are statistically identical before the treatment is applied, any difference in the outcome can be attributed to the treatment. Where possible, this is the best way to establish causation.\nHowever, this is often not possible or feasible in communication science. In those cases, where we only have observational data (e.g., surveys, interviews, content analysis), we therefore have to rely on special statistical methods to control for confounding variables and establish causation.\n\n\nThe next best thing: controlling for confounders\nUsing statistical methods, we can try to control for confounding variables. This is not perfect, because there can always be other confounders, but it does allow us to build evidence to support or refute our theories. In fact, many well supported causal theories in communication science have only been supported this way, such as the relationship between media coverage and public opinion (Mccombs and Shaw 1972). And even common knowledge like “smoking causes cancer” relies heavily on evidence from statistical methods that control for confounders. Given that many communication science theories cannot be tested with experiments, being able to gather some level of evidence for causal relations from observational data is an essential part of the communication scientist’s toolkit.",
    "crumbs": [
      "Statistics",
      "Causality"
    ]
  },
  {
    "objectID": "2_data_preparation/inspect-and-clean.html",
    "href": "2_data_preparation/inspect-and-clean.html",
    "title": "Inspecting and Cleaning Data",
    "section": "",
    "text": "In this tutorial we use the tidyverse and our simulated practice data.\nlibrary(tidyverse)\nd &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")",
    "crumbs": [
      "Data preparation",
      "Inspecting and Cleaning Data"
    ]
  },
  {
    "objectID": "2_data_preparation/inspect-and-clean.html#viewing-a-summary-of-every-column",
    "href": "2_data_preparation/inspect-and-clean.html#viewing-a-summary-of-every-column",
    "title": "Inspecting and Cleaning Data",
    "section": "Viewing a summary of every column",
    "text": "Viewing a summary of every column\nViewing the top of your data is a good start, but there are many things that can go wrong that you won’t be able to spot. There can be missing values, incorrect data types, or outliers that are hard to spot just by looking at the data. So in addition you will want to view summaries of all the columns that you intend to use in your analysis.\nOne way to get a quick summary is by just using the summary function in R. This is a generic function that you can use on many types of objects, and when you use it on a tibble (or regular data frame) it will give you a summary of each column.\n\nsummary(d)\n\nThe summary includes various usefull statistics, like the minimum and maximum values, the median, and the number of missing values (NA). However, this output can be a bit hard to read, especially when you have many columns. A great alternative is to use the summarytools package, which can create summaries in a nice table, including small graphs for each column to get a quick impression of the distribution.\n\nlibrary(summarytools)\ndfSummary(d)\n\nThis will print the summary in your console, but you can also render it in a more readable format with the view (With a small v) function.\n\ndfSummary(d) |&gt; view()\n\n\n\n\nData Frame Summary\nd\nDimensions: 600 x 7\n  Duplicates: 0\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n\n1\nage [numeric]\n\n\n\nMean (sd) : 51.3 (137.9)\n\n\nmin ≤ med ≤ max:\n\n\n18 ≤ 41 ≤ 1987\n\n\nIQR (CV) : 22.5 (2.7)\n\n\n\n51 distinct values\n\n595 (99.2%)\n5 (0.8%)\n\n\n2\nnp_subscription [character]\n\n\n\n1. no\n\n\n2. yes\n\n\n\n\n\n\n263\n(\n43.8%\n)\n\n\n337\n(\n56.2%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n3\nnews consumption [numeric]\n\n\n\nMean (sd) : 10.1 (3.2)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 10.8 ≤ 16.6\n\n\nIQR (CV) : 3.9 (0.3)\n\n\n\n598 distinct values\n\n600 (100.0%)\n0 (0.0%)\n\n\n4\nexperiment_group [character]\n\n\n\n1. control\n\n\n2. negative\n\n\n3. positive\n\n\n\n\n\n\n200\n(\n33.3%\n)\n\n\n200\n(\n33.3%\n)\n\n\n200\n(\n33.3%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n5\ntrust_t1 [numeric]\n\n\n\nMean (sd) : 5.1 (1.3)\n\n\nmin ≤ med ≤ max:\n\n\n1.4 ≤ 5.1 ≤ 9.4\n\n\nIQR (CV) : 1.8 (0.2)\n\n\n\n600 distinct values\n\n600 (100.0%)\n0 (0.0%)\n\n\n6\ntrust_t2 [numeric]\n\n\n\nMean (sd) : 4.9 (1.8)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 5 ≤ 10\n\n\nIQR (CV) : 2.4 (0.4)\n\n\n\n589 distinct values\n\n600 (100.0%)\n0 (0.0%)\n\n\n7\nid [numeric]\n\n\n\nMean (sd) : 300.5 (173.3)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 300.5 ≤ 600\n\n\nIQR (CV) : 299.5 (0.6)\n\n\n\n600 distinct values\n\n600 (100.0%)\n0 (0.0%)\n\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.3.1)2024-08-28\n\n\n\n\n\n\n\n\n\nUsing summarytools in Quarty or RMarkdown\n\n\n\n\n\nIf you’re working in Quarto or RMarkdown, the normal way of using summarytools doesn’t create a nice table. Instead, what you need to do is tell Quarto to render the output as HTML. You can do this by setting the method argument to render.\n\nprint(dfSummary(iris), \n      method='render', \n      max.tbl.height = 500)\n\nNote that we also set the max.tbl.height argument to 500, to make sure that very large tables are put inside a scrollable window.",
    "crumbs": [
      "Data preparation",
      "Inspecting and Cleaning Data"
    ]
  },
  {
    "objectID": "3_statistics/0_hypotheses.html",
    "href": "3_statistics/0_hypotheses.html",
    "title": "Hypotheses",
    "section": "",
    "text": "In order to use statistical methods to test theories, we first need to derive hypotheses from them. A hypothesis is a testable statement, that when proven true or false, provides evidence for or against the theory.\nFor example, consider the theory that playing violent video games leads to increased aggression in children. There is much disagreement about whether this theory is correct (Griffiths 1999), so it’s important to test it empirically. In order to do so, we need to derive hypotheses that we can conduct to rigorous statistical tests. Some examples of hypotheses are:\n\nThere is a relation “between arcade game use and teachers’ ratings of impulsiveness and aggresiveness”. (Lin and Lepper 1987, 81)\nPlaying violent video games leads to higher levels of heart rate and blood pressure compared to playing non-violent video games. (Lynch 1994)\nThere will be a “linear increase in aggressive affect after playing nonaggressive, moderately aggresive, and highly aggressive games”. (Scott 1995, 125)\n\nNotice that these hypotheses are more specific than the theory they are derived from. There are multiple ways to define and measure “playing violent video games” and “increased aggression”, and it could also be that the relation only holds for certain groups of people, or only for short periods of time. Hypotheses help us make (different aspects of) the theory more explicit, so that we can conduct empirical tests.\n\n\nIf an hypothesis is confirmed, it provides evidence in support of the theory. However, note that it does not necessarily prove the theory. For example, if we find that children who play violent video games are more aggressive, it does not yet prove that the games caused the aggression, because there are other possible explanations for finding this relation. Perhaps people who are already aggressive are more likely to play violent video games, or there could be a confounding variable that causes both aggression and video game playing (e.g., gender, socioeconomic status, or parental involvement).\nHypotheses are thus the bridge between theory and statistical methods, and are essential for conducting rigorous research in communication science. They need to be specific enough to be testable, but close enough to the theory they are derived from to allow for meaningful conclusions.",
    "crumbs": [
      "Statistics",
      "Hypotheses"
    ]
  },
  {
    "objectID": "3_statistics/0_hypotheses.html#evidence-neq-proof",
    "href": "3_statistics/0_hypotheses.html#evidence-neq-proof",
    "title": "Hypotheses",
    "section": "",
    "text": "If an hypothesis is confirmed, it provides evidence in support of the theory. However, note that it does not necessarily prove the theory. For example, if we find that children who play violent video games are more aggressive, it does not yet prove that the games caused the aggression, because there are other possible explanations for finding this relation. Perhaps people who are already aggressive are more likely to play violent video games, or there could be a confounding variable that causes both aggression and video game playing (e.g., gender, socioeconomic status, or parental involvement).\nHypotheses are thus the bridge between theory and statistical methods, and are essential for conducting rigorous research in communication science. They need to be specific enough to be testable, but close enough to the theory they are derived from to allow for meaningful conclusions.",
    "crumbs": [
      "Statistics",
      "Hypotheses"
    ]
  },
  {
    "objectID": "3_statistics/index.html",
    "href": "3_statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "3_statistics/2_tests/chi2.html",
    "href": "3_statistics/2_tests/chi2.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Statistics",
      "Tests",
      "Chi2"
    ]
  },
  {
    "objectID": "3_statistics/2_tests/anova.html",
    "href": "3_statistics/2_tests/anova.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Statistics",
      "Tests",
      "Anova"
    ]
  },
  {
    "objectID": "3_statistics/2_tests/regression.html",
    "href": "3_statistics/2_tests/regression.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Statistics",
      "Tests",
      "Regression"
    ]
  },
  {
    "objectID": "data-management/index.html#structure-of-the-data",
    "href": "data-management/index.html#structure-of-the-data",
    "title": "Data management",
    "section": "Structure of the data",
    "text": "Structure of the data\nThe practice data describes a fictional study about how popular media might influence people’s attitudes. It includes an experiment to measure the effect of watching moview about journalism on people’s trust in journalists, and includes a range of measures about the participants.\nNumber of participants (n): 600\nVariables:\n\n\n\n\n\n\n\n\nVariable name\nType\nDescription\n\n\n\n\nid\nnumeric\nUnique identifier for each participant\n\n\nage\nnumeric\nAge of the participant in years\n\n\nnp_subscription\nfactor\nWhether the participant has a newspaper subscription (yes, no)\n\n\nnews_consumption\nnumeric\nAverage number of hours per week the participant consumes news\n\n\nexperiment_group\nfactor\nType of movie shown: neutral (control group), positive (positive portrayal of journalism), negative (negative portrayal of journalism)\n\n\ntrust_t1\nnumeric\nTrust in journalists before the experiment (range: 1-10)\n\n\ntrust_t2\nnumeric\nTrust in journalists after the experiment (range: 1-10)",
    "crumbs": [
      "Data management"
    ]
  },
  {
    "objectID": "data-management/index.html#reading-the-data-into-r",
    "href": "data-management/index.html#reading-the-data-into-r",
    "title": "Data management",
    "section": "Reading the data into R",
    "text": "Reading the data into R\nThe following code reads the practice data into R. We will explain this code in the next tutorials, and include it at the top of every tutorial that uses it. Note that in order to run this code, you need to have installed the tidyverse package.\n\nlibrary(tidyverse)\nd &lt;- read_csv('https://tinyurl.com/R-practice-data')\n\nThis is what the full data looks like:",
    "crumbs": [
      "Data management"
    ]
  },
  {
    "objectID": "data-management/read-and-write.html",
    "href": "data-management/read-and-write.html",
    "title": "Read and write data",
    "section": "",
    "text": "TLDR summary\n\n\n\n\n\nOne of the most common formats for storing and sharing rectangular data (i.e., data in rows and columns) is the csv format. With the read_csv and write_csv functions you can easily read and write CSV files in R.\n\nlibrary(tidyverse)\n\n# read CSV file from the internet\nd  &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")\n\n# write to a CSV file on your computer\nwrite_csv(d, \"my_data.csv\")                            \n\n# read CSV file from your own computer \nd2 &lt;- read_csv(\"my_data.csv\")",
    "crumbs": [
      "Data management",
      "Read and write data"
    ]
  },
  {
    "objectID": "data-management/read-and-write.html#csv-files",
    "href": "data-management/read-and-write.html#csv-files",
    "title": "Read and write data",
    "section": "CSV files",
    "text": "CSV files\n\n\n\n\n\n\nWhat is a CSV file?\n\n\n\n\n\nCSV stands for Comma Separated Values. It is a simple text file, that you can open in any text editor. In order to store a data frame (i.e. data in rows and colums), it simply read every line as a row, and separates the columns by a comma (or sometimes another symbol, like a semicolon).\nFor example, the following CSV file contains a data frame with three columns: resp_id, gender, and height. The first row contains the column names, and the following rows contain the data.\n\nresp_id,gender,height\n1,M,176\n2,M,165\n3,F,172\n4,F,160\n\nThe benefit of this simplicity is that any respectable spreadsheet or statistical software (e.g., Excel, Google sheets, SPSS, Stata) can read it. This makes CSV files a great way to share and store data.\nAnd just in case you’re worried, yes, CSV can also handle textual data. It uses some tricks to make sure that commas inside the text are not interpreted as column separators.\n\n\n\nTo show you how to work with CSV files, we’ll first import a dataset from the internet. Then we’ll show how to write this data to a CSV file on your computer, and how to read it back into R.\n\nImporting data from a URL\nTo read CSV files into R, you can use the read_csv from the tidyverse (more specifically from the readr package). If you provide a URL, it will download the file from the internet. Here we read the data and assign it to the name d (short for data). You can use any name you like, but since you’ll be referring to this data a lot, it’s convenient to keep it short.\n\nlibrary(tidyverse)\n\nurl &lt;- \"https://tinyurl.com/R-practice-data\"\nd &lt;- read_csv(url)\n\nMake sure to always check whether the data was imported correctly:\n\nd\n\nYou can also view the data in a larger spreadsheet-like view using the View function. Either click on the name (d) in the Environment tab in RStudio (top right panel), or use the View function:\n\nView(d)\n\nThis will open a new tab in RStudio that shows all the data. In the top menu bar you can also filter the data and search for specific values, or click on column names to sort the data.\n\n\nWriting data to a CSV file on your computer\nYou can use the write_csv function to write a tibble to a CSV file on your computer. If you just provide a file name, it will be saved in your current working directory. Remember that you can set your working directory with setwd(), or using the RStudio interface (Session -&gt; Set Working Directory -&gt; Choose Directory).\n\nwrite_csv(d, \"practice_data.csv\")\n\nThis will write the d tibble to a file called practice_data.csv in your current working directory. Try finding it in your file system!\n\n\nReading data from a CSV file on your computer\nNow let’s read this file back into R. Since the file is in your working directory, you can just specify the file name:\n\nd2 &lt;- read_csv(\"practice_data.csv\")\n\nYou can check and verify that the data (d2) is indeed identical to the original data (d).\n\n\n\n\n\n\nCSV pitfalls to avoid\n\n\n\n\n\nThere are two important pitfalls to avoid when working with CSV files:\n\nPitfall 1: Corrupting the file by opening it in Excel\nWhen you download a CSV file from the internet, some computers might immediately ask you whether you want to open it in your default spreadsheet program (e.g., Excel, Numbers). Do not do this, but instead download the file directly to your computer. If you open the file and accidentally save it, it can overwrite the CSV file with a different format. Excel in particular has a habit of breaking CSV files this way.\n\n\nPitfall 2: Different flavours of CSV files\nThere are different flavours of CSV files (for historic reasons). Even though we call them “comma separated values”, the separator is sometimes a semicolon or a tab. And depending on language, the decimal separator can be a comma or a dot. In particular, there are two most common versions of the CSV file. This is why tidyverse has two read_csv functions: read_csv and read_csv2. In general, you can just try read_csv first, and if it doesn’t work, try read_csv2.",
    "crumbs": [
      "Data management",
      "Read and write data"
    ]
  },
  {
    "objectID": "data-management/read-and-write.html#reading-other-file-formats-like-excel-and-spss",
    "href": "data-management/read-and-write.html#reading-other-file-formats-like-excel-and-spss",
    "title": "Read and write data",
    "section": "Reading other file formats, like Excel and SPSS",
    "text": "Reading other file formats, like Excel and SPSS\nNow that you know how to read and write CSV files, reading other file formats is a piece of cake. It works almost the same way, but you just need to download a package that can read the file format.\nFor instance, to read an Excel file, you can use the readxl package, which provides the read_excel function. To read an SPSS file, you can use the haven package, which provides the read_sav function. You might have to take care of some additional details, such as the sheet name in the Excel file, or the variable labels in the SPSS file. But once you’ve got the hang of managing your data with the tidyverse, you’ll be able to handle any data frames formats that come your way.",
    "crumbs": [
      "Data management",
      "Read and write data"
    ]
  },
  {
    "objectID": "getting-started/functions.html#pipe-syntax",
    "href": "getting-started/functions.html#pipe-syntax",
    "title": "Functions",
    "section": "",
    "text": "There is another common way to use functions in R using the pipe syntax. With the pipe syntax, you can pipe the first argument into the function, instead of putting it inside the parentheses. As you will see below, this allows you to create a pipeline of functions, which is often easier to read. The pipe syntax uses the |&gt; operator, which is used as follows:\n\nargument1 |&gt; function_name(argument2, ...)\n\nFor example, the following two lines of code give identical output:\n\nmean(x_with_missing, na.rm=T)\n\n[1] 2.5\n\nx_with_missing |&gt; mean(na.rm=T)\n\n[1] 2.5\n\n\nNotice how our first argument, the required argument x_with_missing, is piped into the mean function. Inside the mean function, we only specify the second argument, the optional argument na.rm.\nSo why do we need this alternative way of doing the same thing? The reason is that when writing code, you shouldn’t just think about what the code does, but also about how easy the code is to read. This not only helps you prevent mistakes, but also makes your analysis transparent. As you’ll see later, you’ll encounter many cases where your analysis requires you to string together multiple functions. In these cases, pipes make your code much easier to read.\nFor example, imagine we would want to round the result (2.5) up to a round number (3). With the pipe syntax we can just add the round function to our pipeline.\n\nx_with_missing |&gt; \n  mean(na.rm=T) |&gt; \n  round()\n\nYou’ll see how powerful this can be later on, especially in the Data Management chapter. In order to prepare and clean up your data, you’ll often need to perform a series of functions in a specific order. The pipe syntax allows you to do this in a very readable way.",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "getting-started/packages.html#install-a-package-with-install.packages",
    "href": "getting-started/packages.html#install-a-package-with-install.packages",
    "title": "Packages",
    "section": "1. Install a package with install.packages():",
    "text": "1. Install a package with install.packages():\nMost packages are available on the Comprehensive R Archive Network (CRAN), which is the main repository for R packages. To install these packages, all you need to know is their name.\nFor example, there is a package called lubridate that makes it easier to work with dates and times in your data. To install this package, you can use the install.packages() function:\n\ninstall.packages(\"lubridate\")",
    "crumbs": [
      "Getting Started",
      "Packages"
    ]
  },
  {
    "objectID": "getting-started/packages.html#load-a-package-with-library",
    "href": "getting-started/packages.html#load-a-package-with-library",
    "title": "Packages",
    "section": "2. Load a package with library():",
    "text": "2. Load a package with library():\nOnce you have installed a package, it is not yet loaded into your current R session. Similar to when you install a new app on your phone, you need to open it every time you want to use it.\nTo use a package in your current R session, you can load it with the library() function:\n\nlibrary(lubridate)",
    "crumbs": [
      "Getting Started",
      "Packages"
    ]
  },
  {
    "objectID": "good-to-know/function-documentation.html",
    "href": "good-to-know/function-documentation.html",
    "title": "Function Documentation",
    "section": "",
    "text": "One of the things that can new users in R can find overwhelming, is that they think they need to learn all functions by heart. This is not the case! Aside from a handfull of functions that you will use all the time, you will often need to look up how to use a function. Rather than learning everything by heart, you therefore need to learn some tricks for how to quickly look up information about functions.\nOne of the most important tricks is to use the built-in help system in R. You can quickly access documentation for any function using the ? symbol. This is a powerful tool that can help you understand how to use functions, what arguments they require, and what they return.\n\n\nTo view the documentation for a specific function, you simply need to type ? followed by the function name. For example, if you want to learn more about the mean() function, you would type:\n\n?mean\n\nThis will open the help page, often in the bottom right pane of RStudio.\n\n\n\nThe help page for a function is divided into several sections. The most important sections are:\n\n\nA brief description of what the function does. For the mean() function, the description is: Generic Function for the (Arithmetic) Mean.\nBy generic function, they mean that the function can have multiple implementations. When you think of the mean, you are probably thinking of the mean of a vector of numbers.\n\nx = c(1,2,3,4)\nmean(x)\n\n[1] 2.5\n\n\nBut you can do more! For example, you can also calculate the mean of a vector of Date values:\n\ndates = as.Date(c(\"2021-01-01\", \"2021-01-03\"))\nmean(dates)\n\n[1] \"2021-01-02\"\n\n\n\n\n\nThe syntax of the function, including all the arguments it takes. For example, the mean() function has the following usage:\n\nmean(x, ...)\n\n## Default S3 method:\nmean(x, trim = 0, na.rm = FALSE, ...)\n\nThe first part tells you that the most basic way to use the function is to provide an argument called x. What x is, is explained in the arguments section that we discuss below.\nThe ... at the end means that the function can take additional arguments. This is because the mean function is a generic function. Depending on the type of input you provide (e.g., numbers, dates), some arguments might not be relevant.\nThe second part tells you that the default method for the mean function has two additional arguments in addition to x: trim, and na.rm. Note an important difference with the x argument: these arguments have default values (0 and FALSE, respectively). This means that these arguments are optional. If you don’t specify them, the function will use these default values.\nFor example, notice that the na.rm argument is set to FALSE by default. As we can see in the Arguments section, this means that the function will not remove missing values by default. (NA stands for Not Available, and is used in R to indicate missing values, so na.rm is short for remove NAs).\n\nx_with_missing &lt;- c(1, 2, 3, NA, 4)\nmean(x_with_missing)\n\n[1] NA\n\n\nIf we want to remove missing values, we can set na.rm to TRUE:\n\nmean(x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\n\n\n\n\n\n\nTo name or not to name your arguments\n\n\n\nNotice how in the code above we specify the argument name na.rm = TRUE to indicate that we want to use this optional argument. For the x argument we don’t need to specify the argument name, because it’s the first argument and the function knows that the first argument is x. Generally speaking, if you don’t specify the argument name, R will assume that you are providing the arguments in the order that they are listed in the usage section. Let’s think a bit about when we should and should not use argument names!\nYou could decide to always use argument names:\n\nmean(x=x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\nThis is fine, and sometimes you might want to do this for sake of clarity. But it’s also often unnecessary. For the mean function, it is obvious that the first argument is the input over which you want to calculate the mean, so you don’t need to specify the argument name.\nOn the opposite end of the spectrum, you could decide to never use argument names:\n\nmean(x_with_missing, 0, TRUE)\n\n[1] 2.5\n\n\nHere the three arguments follow the order in the usage section: x, trim, na.rm.\nThis has two obvious downsides:\n\nIt is not obvious what the 0 and TRUE arguments are. The reader might thus have to look up the function documentation.\nWe now also need to specify the trim argument, because it comes before na.rm in the usage section.\n\nSo in general, it is often good to use argument names for optional arguments, like na.rm. For required arguments, like x, it is often not necessary. Arguably the best way to use the mean function with na.rm is therefore:\n\nmean(x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\n\n\n\n\n\nA description of all the arguments that the function takes. This should cover all the arguments that are listed in the usage section.\nFor example, the mean() function explains that the x argument is can be a numeric vector, but also something like a logical or date vector. For the na.rm argument it explains that if set to TRUE, missing values will be removed before calculating the mean.\n\n\n\nThe value section explains what the function returns (i.e. the output).\n\n\n\nThe examples section shows you how to use the function. Honestly, this is often the most useful part of the help page. If you are not sure how to use a function, a great way to learn is to look at the examples. Usually, you can directly copy-paste these examples into your script and run them to see how the function works.",
    "crumbs": [
      "Good to Know",
      "Function Documentation"
    ]
  },
  {
    "objectID": "good-to-know/function-documentation.html#how-to-use-the-symbol",
    "href": "good-to-know/function-documentation.html#how-to-use-the-symbol",
    "title": "Function Documentation",
    "section": "",
    "text": "To view the documentation for a specific function, you simply need to type ? followed by the function name. For example, if you want to learn more about the mean() function, you would type:\n\n?mean\n\nThis will open the help page, often in the bottom right pane of RStudio.",
    "crumbs": [
      "Good to Know",
      "Function Documentation"
    ]
  },
  {
    "objectID": "good-to-know/function-documentation.html#how-to-read-the-help-page",
    "href": "good-to-know/function-documentation.html#how-to-read-the-help-page",
    "title": "Function Documentation",
    "section": "",
    "text": "The help page for a function is divided into several sections. The most important sections are:\n\n\nA brief description of what the function does. For the mean() function, the description is: Generic Function for the (Arithmetic) Mean.\nBy generic function, they mean that the function can have multiple implementations. When you think of the mean, you are probably thinking of the mean of a vector of numbers.\n\nx = c(1,2,3,4)\nmean(x)\n\n[1] 2.5\n\n\nBut you can do more! For example, you can also calculate the mean of a vector of Date values:\n\ndates = as.Date(c(\"2021-01-01\", \"2021-01-03\"))\nmean(dates)\n\n[1] \"2021-01-02\"\n\n\n\n\n\nThe syntax of the function, including all the arguments it takes. For example, the mean() function has the following usage:\n\nmean(x, ...)\n\n## Default S3 method:\nmean(x, trim = 0, na.rm = FALSE, ...)\n\nThe first part tells you that the most basic way to use the function is to provide an argument called x. What x is, is explained in the arguments section that we discuss below.\nThe ... at the end means that the function can take additional arguments. This is because the mean function is a generic function. Depending on the type of input you provide (e.g., numbers, dates), some arguments might not be relevant.\nThe second part tells you that the default method for the mean function has two additional arguments in addition to x: trim, and na.rm. Note an important difference with the x argument: these arguments have default values (0 and FALSE, respectively). This means that these arguments are optional. If you don’t specify them, the function will use these default values.\nFor example, notice that the na.rm argument is set to FALSE by default. As we can see in the Arguments section, this means that the function will not remove missing values by default. (NA stands for Not Available, and is used in R to indicate missing values, so na.rm is short for remove NAs).\n\nx_with_missing &lt;- c(1, 2, 3, NA, 4)\nmean(x_with_missing)\n\n[1] NA\n\n\nIf we want to remove missing values, we can set na.rm to TRUE:\n\nmean(x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\n\n\n\n\n\n\nTo name or not to name your arguments\n\n\n\nNotice how in the code above we specify the argument name na.rm = TRUE to indicate that we want to use this optional argument. For the x argument we don’t need to specify the argument name, because it’s the first argument and the function knows that the first argument is x. Generally speaking, if you don’t specify the argument name, R will assume that you are providing the arguments in the order that they are listed in the usage section. Let’s think a bit about when we should and should not use argument names!\nYou could decide to always use argument names:\n\nmean(x=x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\nThis is fine, and sometimes you might want to do this for sake of clarity. But it’s also often unnecessary. For the mean function, it is obvious that the first argument is the input over which you want to calculate the mean, so you don’t need to specify the argument name.\nOn the opposite end of the spectrum, you could decide to never use argument names:\n\nmean(x_with_missing, 0, TRUE)\n\n[1] 2.5\n\n\nHere the three arguments follow the order in the usage section: x, trim, na.rm.\nThis has two obvious downsides:\n\nIt is not obvious what the 0 and TRUE arguments are. The reader might thus have to look up the function documentation.\nWe now also need to specify the trim argument, because it comes before na.rm in the usage section.\n\nSo in general, it is often good to use argument names for optional arguments, like na.rm. For required arguments, like x, it is often not necessary. Arguably the best way to use the mean function with na.rm is therefore:\n\nmean(x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\n\n\n\n\n\nA description of all the arguments that the function takes. This should cover all the arguments that are listed in the usage section.\nFor example, the mean() function explains that the x argument is can be a numeric vector, but also something like a logical or date vector. For the na.rm argument it explains that if set to TRUE, missing values will be removed before calculating the mean.\n\n\n\nThe value section explains what the function returns (i.e. the output).\n\n\n\nThe examples section shows you how to use the function. Honestly, this is often the most useful part of the help page. If you are not sure how to use a function, a great way to learn is to look at the examples. Usually, you can directly copy-paste these examples into your script and run them to see how the function works.",
    "crumbs": [
      "Good to Know",
      "Function Documentation"
    ]
  },
  {
    "objectID": "good-to-know/function-documentation.html#try-using-tab-completion-everywhere",
    "href": "good-to-know/function-documentation.html#try-using-tab-completion-everywhere",
    "title": "Function Documentation",
    "section": "Try using tab completion everywhere",
    "text": "Try using tab completion everywhere\nWell ok, not everywhere. But you might be surprised how often it can help you. It can even help you find files on your computer. If you use tab completion between quotes, RStudio will show you all the files in your working directory that match the characters you’ve typed so far. So you can use this inside functions like read_csv to quickly find the file you want to read.\n\nlibrary(tidyverse)\nread_csv(\"\")\n\nTry it out!",
    "crumbs": [
      "Good to Know",
      "Function Documentation"
    ]
  },
  {
    "objectID": "getting-started/packages.html#install-a-package",
    "href": "getting-started/packages.html#install-a-package",
    "title": "Packages",
    "section": "1. Install a package",
    "text": "1. Install a package\nMost packages are available on the Comprehensive R Archive Network (CRAN), which is the main repository for R packages. To install these packages, all you need to know is their name.\nFor example, there is a package called lubridate that makes it easier to work with dates and times in your data. To install this package, you can use the install.packages() function:\n\ninstall.packages(\"lubridate\")",
    "crumbs": [
      "Getting Started",
      "Packages"
    ]
  },
  {
    "objectID": "getting-started/packages.html#load-a-package",
    "href": "getting-started/packages.html#load-a-package",
    "title": "Packages",
    "section": "2. Load a package",
    "text": "2. Load a package\nOnce you have installed a package, it is not yet loaded into your current R session. Similar to when you install a new app on your phone, you need to open it every time you want to use it.\nTo use a package in your current R session, you can load it with the library() function:\n\nlibrary(lubridate)",
    "crumbs": [
      "Getting Started",
      "Packages"
    ]
  },
  {
    "objectID": "getting-started/files-and-projects copy.html",
    "href": "getting-started/files-and-projects copy.html",
    "title": "Working directory",
    "section": "",
    "text": "When working in R, you will often need to read and save files on your computer. Moreover, when you work with sensive data, such as survey responses, you really need to understand where your data is stored. To write or read a file in R, you can specify the file path in two ways: using an absolute path (starting from the root directory) or a relative path (starting from the current working directory). Let’s practice this a bit using both approaches.\n\n\n\n\n\n\nHow does your computer organize files?\n\n\n\n\n\nComputers organize files and directories (or folders) in a hierarchical structure that resembles a tree, called the file system. This tree-like structure starts from a single root directory and branches out into subdirectories, which can contain more subdirectories or files. It looks a bit different on Windows and Mac (or Linux), but the basic idea is the same. Here is a simplified example (normally there are more layers, such as your user directory):\n\n/ (Root Directory) \n│\n├── Documents\n│   ├── Work\n│   │   ├── survey.csv\n│   │   └──  \n│   └── Personal\n│       ├── Resume.pdf\n│       └── Budget.xlsx\n│\n├── Downloads\n│   ├── Report.docx\n│   └── image.jpg\n\nBecause of this structure, any file on your computer has a unique path that describes its location in the file system. For example, the path to the Report.docx file in the Work directory would be /Documents/Work/survey.csv (or C:\\Documents\\Work\\survey.csv on Windows).\n\n\n\nFirst, download the following CSV file containing some practice data. Don’t worry about the content for now (we’ll get to that later), just download it to your computer. You computer will probably save it in your Downloads directory, but otherwise just pick a location where you can find it. Check to make sure that the file actually downloaded by opening your file explorer (outside of RStudio) and navigating to the file.\n\n\nFirst, let’s try to read the file using the absolute path, using the following code. Don’t worry about understanding the code for now, as we’ll get back to this in the data management chapter.\n\nlibrary(tidyverse)\n\nread_csv('ENTER THE ABSOLUTE PATH TO THE FILE HERE')\n\nSo how do you find the absolute path to the file? There are several options, so try to find at least one that you like.\n\nUsing file.choose(): If you run the code file.choose(), R will open a file explorer window. Here you can browse to the file, and when you select it, R will print the file path in the console window.\nFrom the file explorer: Open the file explorer on your computer, and navigate to the file. You can then right-click on the file and select Properties (or some similar term). Here you’ll find the Location of the file.\nUsing RStudio: For some types of files, you can use the RStudio data importer interface. In the bottom-right window, you can go to the Files tab. Here you can browse to the file on your computer. If the file is something that you can read into R (e.g., a CSV file), RStudio will give you the Import Dataset option. This will open a visual interface for importing the data, that also shows the file path.\n\n\n\n\n\n\n\nCool kids use tab completion\n\n\n\n\n\nThere is one other nice trick that you can use to find files on your computer: tab completion. Whenever you are writing something in a code editor, you can often use the Tab key (above caps lock) to automatically complete it (like auto-complete on your phone). If you’ve ever seen programmers work really fast, it’s because they’re using tab completion all the time.\nTo use tab completion for file paths, put your text cursor between the quotes in read_csv(\"\"), and then press the Tab key. If there are multiple files that match the characters you’ve typed so far, RStudio will show you all the options. Keep typing to narrow down the options, and once you see the file or directory you want, press tab again to complete it.\n\n\n\n\n\n\nWhenever you are working in R, you are always working in a specific directory on your computer. This directory is called the working directory. You can check the current working directory with the getwd() (get working directory) function:\n\ngetwd()\n\nIf a file is inside of your working directory, you can easily read it by just providing the file name. Also, if you write a file without specifying a directory, it will be saved in this directory. Therefore, a good way to make data management in R easy, is by making a directory for your current project, and setting this as your working directory.\nOpen your file explorer (outside of RStudio) and create a directory somewhere with a name like “R_projects”. Then copy the fake_demo_data.csv file you downloaded earlier into this directory.\nNow, you can set this directory as your working directory in R. There are two ways to do this:\n\nManually: You can set the working directory with the setwd() function. For example, to set the working directory to /home/you/R_projects, you can run setwd(\"/home/you/R_projects\").\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.\n\nIf you did everything correctly, you should now be able to read the file using the following code:\n\nread_csv('fake_demo_data.csv')",
    "crumbs": [
      "Getting Started",
      "Working directory"
    ]
  },
  {
    "objectID": "getting-started/files-and-projects copy.html#absolute-path",
    "href": "getting-started/files-and-projects copy.html#absolute-path",
    "title": "Working directory",
    "section": "",
    "text": "An absolute path specifies the full path to a file or directory from the root directory. There are several ways to find out the absolute path of a file on your computer:\n\nFrom the file explorer: Open the file explorer on your computer, and navigate to the file. You can then right-click on the file and select Properties (or some similar term). Here you’ll find the Location of the file.\nUsing file.choose(): If you run the code file.choose(), R will open a file explorer window. Here you can browse to the file, and when you select it, R will print the file path in the console window.\nUsing RStudio: For some types of files, you can use the RStudio data importer interface. In the bottom-right window, you can go to the Files tab. Here you can browse to the file on your computer. If the file is something that you can read into R (e.g., a CSV file), RStudio will give you the Import Dataset option. This will open a visual interface for importing the data, that also shows the file path.",
    "crumbs": [
      "Getting Started",
      "Working directory"
    ]
  },
  {
    "objectID": "getting-started/files-and-projects copy.html#relative-path",
    "href": "getting-started/files-and-projects copy.html#relative-path",
    "title": "Working directory",
    "section": "",
    "text": "Whenever you are working in R, you are always working in a specific directory on your computer. This directory is called the working directory. You can check the current working directory with the getwd() function: This prints the current working directory in your console.\nFor example, say that our current working directory is /Documents/Work (in the example file system above). This allows us to access the survey.csv file with a relative path: survey.csv. Also, if we would now create a new file in R called my_analysis.r, it would be saved as /Documents/Work/my_analysis.r.\nIn other words, by setting our working directory, we can avoid having to write out the full path to a file every time we want to read or write. There are two easy ways to set the working directory in R:\n\nManually: You can set the working directory with the setwd() function. For example, to set the working directory to /Documents/Work, you can run setwd(\"/Documents/Work\").\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.",
    "crumbs": [
      "Getting Started",
      "Working directory"
    ]
  },
  {
    "objectID": "getting-started/files-and-projects copy.html#relative-to-the-working-directory",
    "href": "getting-started/files-and-projects copy.html#relative-to-the-working-directory",
    "title": "Working directory",
    "section": "",
    "text": "Whenever you are working in R, you are always working in a specific directory on your computer. This directory is called the working directory. You can check the current working directory with the getwd() function: This prints the current working directory in your console.\nFor example, say that our current working directory is /Documents/Work (in the example file system above). This allows us to access the survey.csv file with a relative path: survey.csv. Also, if we would now create a new file in R called my_analysis.r, it would be saved as /Documents/Work/my_analysis.r.\nIn other words, by setting our working directory, we can avoid having to write out the full path to a file every time we want to read or write. There are two easy ways to set the working directory in R:\n\nManually: You can set the working directory with the setwd() function. For example, to set the working directory to /Documents/Work, you can run setwd(\"/Documents/Work\").\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.",
    "crumbs": [
      "Getting Started",
      "Working directory"
    ]
  },
  {
    "objectID": "getting-started/files-and-projects copy.html#reading-the-file-using-the-absolute-path",
    "href": "getting-started/files-and-projects copy.html#reading-the-file-using-the-absolute-path",
    "title": "Working directory",
    "section": "",
    "text": "First, let’s try to read the file using the absolute path, using the following code. Don’t worry about understanding the code for now, as we’ll get back to this in the data management chapter.\n\nlibrary(tidyverse)\n\nread_csv('ENTER THE ABSOLUTE PATH TO THE FILE HERE')\n\nSo how do you find the absolute path to the file? There are several options, so try to find at least one that you like.\n\nUsing file.choose(): If you run the code file.choose(), R will open a file explorer window. Here you can browse to the file, and when you select it, R will print the file path in the console window.\nFrom the file explorer: Open the file explorer on your computer, and navigate to the file. You can then right-click on the file and select Properties (or some similar term). Here you’ll find the Location of the file.\nUsing RStudio: For some types of files, you can use the RStudio data importer interface. In the bottom-right window, you can go to the Files tab. Here you can browse to the file on your computer. If the file is something that you can read into R (e.g., a CSV file), RStudio will give you the Import Dataset option. This will open a visual interface for importing the data, that also shows the file path.\n\n\n\n\n\n\n\nCool kids use tab completion\n\n\n\n\n\nThere is one other nice trick that you can use to find files on your computer: tab completion. Whenever you are writing something in a code editor, you can often use the Tab key (above caps lock) to automatically complete it (like auto-complete on your phone). If you’ve ever seen programmers work really fast, it’s because they’re using tab completion all the time.\nTo use tab completion for file paths, put your text cursor between the quotes in read_csv(\"\"), and then press the Tab key. If there are multiple files that match the characters you’ve typed so far, RStudio will show you all the options. Keep typing to narrow down the options, and once you see the file or directory you want, press tab again to complete it.",
    "crumbs": [
      "Getting Started",
      "Working directory"
    ]
  },
  {
    "objectID": "getting-started/files-and-projects copy.html#setting-your-working-directory-to-read-files-easily",
    "href": "getting-started/files-and-projects copy.html#setting-your-working-directory-to-read-files-easily",
    "title": "Working directory",
    "section": "",
    "text": "Whenever you are working in R, you are always working in a specific directory on your computer. This directory is called the working directory. You can check the current working directory with the getwd() (get working directory) function:\n\ngetwd()\n\nIf a file is inside of your working directory, you can easily read it by just providing the file name. Also, if you write a file without specifying a directory, it will be saved in this directory. Therefore, a good way to make data management in R easy, is by making a directory for your current project, and setting this as your working directory.\nOpen your file explorer (outside of RStudio) and create a directory somewhere with a name like “R_projects”. Then copy the fake_demo_data.csv file you downloaded earlier into this directory.\nNow, you can set this directory as your working directory in R. There are two ways to do this:\n\nManually: You can set the working directory with the setwd() function. For example, to set the working directory to /home/you/R_projects, you can run setwd(\"/home/you/R_projects\").\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.\n\nIf you did everything correctly, you should now be able to read the file using the following code:\n\nread_csv('fake_demo_data.csv')",
    "crumbs": [
      "Getting Started",
      "Working directory"
    ]
  },
  {
    "objectID": "getting-started/file-management.html",
    "href": "getting-started/file-management.html",
    "title": "File management",
    "section": "",
    "text": "In order to read and write files in R, you need to be able to refer to them by their file path (i.e. their name and location on your computer). You can specify this path in two ways: using an absolute path (starting from the root directory) or a relative path (starting from the current working directory). Let’s practice a bit using both approaches.\nFirst, download this CSV file containing some practice data. Don’t worry about the content for now (we’ll get to that later). You computer will probably save it in your Downloads directory, but otherwise just pick a location where you can find it. Check to make sure that the file actually downloaded by opening your file explorer (outside of RStudio) and navigating to the file.\n\n\n\n\n\n\nHow does your computer organize files?\n\n\n\n\n\nComputers organize files and directories (or folders) in a hierarchical structure that resembles a tree, called the file system. This tree-like structure starts from a single root directory and branches out into subdirectories, which can contain more subdirectories or files. It looks a bit different on Windows and Mac (or Linux), but the basic idea is the same. Here is a simplified example (normally there are more layers, such as your user directory):\n\n/ (Root Directory) \n│\n├── Documents\n│   ├── Work\n│   │   ├── survey.csv\n│   │   └──  \n│   └── Personal\n│       ├── Resume.pdf\n│       └── Budget.xlsx\n│\n├── Downloads\n│   ├── Report.docx\n│   └── image.jpg\n\nBecause of this structure, any file on your computer has a unique path that describes its location in the file system. For example, the path to the Report.docx file in the Work directory would be /Documents/Work/survey.csv (or C:\\Documents\\Work\\survey.csv on Windows).\n\n\n\n\n\nFirst, let’s try to read the file using the absolute path, using the following code. Don’t worry about understanding the code for now, as we’ll get back to this in the data management chapter.\n\nlibrary(tidyverse)\n\nread_csv('ENTER THE ABSOLUTE PATH TO THE FILE HERE')\n\nSo how do you find the absolute path to the file? There are several options, so try to find at least one that you like.\n\nUsing file.choose(): If you run the code file.choose(), R will open a file explorer window. Here you can browse to the file, and when you select it, R will print the file path in the console window.\nFrom the file explorer: Open the file explorer on your computer, and navigate to the file. You can then right-click on the file and select Properties (or some similar term). Here you’ll find the Location of the file.\nUsing RStudio: For some types of files, you can use the RStudio data importer interface. In the bottom-right window, you can go to the Files tab. Here you can browse to the file on your computer. If the file is something that you can read into R (e.g., a CSV file), RStudio will give you the Import Dataset option. This will open a visual interface for importing the data, that also shows the file path.\n\n\n\n\n\n\n\nCool kids use tab completion\n\n\n\n\n\nThere is one other nice trick that you can use to find files on your computer: tab completion. Whenever you are writing something in a code editor, you can often use the Tab key (above caps lock) to automatically complete it (like auto-complete on your phone). If you’ve ever seen programmers work really fast, it’s because they’re using tab completion all the time.\nTo use tab completion for file paths, put your text cursor between the quotes in read_csv(\"\"), and then press the Tab key. If there are multiple files that match the characters you’ve typed so far, RStudio will show you all the options. Keep typing to narrow down the options, and once you see the file or directory you want, press tab again to complete it.\n\n\n\n\n\n\nWhenever you are working in R, you are always working in a specific directory on your computer. This directory is called the working directory. You can check the current working directory with the getwd() (get working directory) function:\n\ngetwd()\n\nIf a file is inside of your working directory, you can easily read it by just providing the file name. Also, if you write a file without specifying a directory, it will be saved in this directory. Therefore, a good way to make data management in R easy, is by making a directory for your current project, and setting this as your working directory.\nOpen your file explorer (outside of RStudio) and create a directory somewhere with a name like “R_projects”. Then copy the fake_demo_data.csv file you downloaded earlier into this directory.\nNow, you can set this directory as your working directory in R. There are two ways to do this:\n\nManually: You can set the working directory with the setwd() function. For example, to set the working directory to /home/you/R_projects, you can run setwd(\"/home/you/R_projects\").\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.\n\nIf you did everything correctly, you should now be able to read the file using the following code:\n\nread_csv('fake_demo_data.csv')",
    "crumbs": [
      "Getting Started",
      "File management"
    ]
  },
  {
    "objectID": "getting-started/file-management.html#reading-the-file-using-the-absolute-path",
    "href": "getting-started/file-management.html#reading-the-file-using-the-absolute-path",
    "title": "File management",
    "section": "",
    "text": "First, let’s try to read the file using the absolute path, using the following code. Don’t worry about understanding the code for now, as we’ll get back to this in the data management chapter.\n\nlibrary(tidyverse)\n\nread_csv('ENTER THE ABSOLUTE PATH TO THE FILE HERE')\n\nSo how do you find the absolute path to the file? There are several options, so try to find at least one that you like.\n\nUsing file.choose(): If you run the code file.choose(), R will open a file explorer window. Here you can browse to the file, and when you select it, R will print the file path in the console window.\nFrom the file explorer: Open the file explorer on your computer, and navigate to the file. You can then right-click on the file and select Properties (or some similar term). Here you’ll find the Location of the file.\nUsing RStudio: For some types of files, you can use the RStudio data importer interface. In the bottom-right window, you can go to the Files tab. Here you can browse to the file on your computer. If the file is something that you can read into R (e.g., a CSV file), RStudio will give you the Import Dataset option. This will open a visual interface for importing the data, that also shows the file path.\n\n\n\n\n\n\n\nCool kids use tab completion\n\n\n\n\n\nThere is one other nice trick that you can use to find files on your computer: tab completion. Whenever you are writing something in a code editor, you can often use the Tab key (above caps lock) to automatically complete it (like auto-complete on your phone). If you’ve ever seen programmers work really fast, it’s because they’re using tab completion all the time.\nTo use tab completion for file paths, put your text cursor between the quotes in read_csv(\"\"), and then press the Tab key. If there are multiple files that match the characters you’ve typed so far, RStudio will show you all the options. Keep typing to narrow down the options, and once you see the file or directory you want, press tab again to complete it.",
    "crumbs": [
      "Getting Started",
      "File management"
    ]
  },
  {
    "objectID": "getting-started/file-management.html#setting-your-working-directory-to-read-files-easily",
    "href": "getting-started/file-management.html#setting-your-working-directory-to-read-files-easily",
    "title": "File management",
    "section": "",
    "text": "Whenever you are working in R, you are always working in a specific directory on your computer. This directory is called the working directory. You can check the current working directory with the getwd() (get working directory) function:\n\ngetwd()\n\nIf a file is inside of your working directory, you can easily read it by just providing the file name. Also, if you write a file without specifying a directory, it will be saved in this directory. Therefore, a good way to make data management in R easy, is by making a directory for your current project, and setting this as your working directory.\nOpen your file explorer (outside of RStudio) and create a directory somewhere with a name like “R_projects”. Then copy the fake_demo_data.csv file you downloaded earlier into this directory.\nNow, you can set this directory as your working directory in R. There are two ways to do this:\n\nManually: You can set the working directory with the setwd() function. For example, to set the working directory to /home/you/R_projects, you can run setwd(\"/home/you/R_projects\").\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.\n\nIf you did everything correctly, you should now be able to read the file using the following code:\n\nread_csv('fake_demo_data.csv')",
    "crumbs": [
      "Getting Started",
      "File management"
    ]
  },
  {
    "objectID": "getting-started/names-and-values.html#assigning-different-types-of-objects",
    "href": "getting-started/names-and-values.html#assigning-different-types-of-objects",
    "title": "Names and Objects",
    "section": "Assigning different types of objects",
    "text": "Assigning different types of objects\nYou can assign any type of object to a name, and you can use any name, as long as it starts with a letter and doesn’t contain spaces or symbols (but underscores are OK)\n\na_number = 5\nmy_cats_name = \"Hobbes\"\n\nIf you run this code and check you Environment (top-right), you should now see these name-object pairs added.",
    "crumbs": [
      "Getting Started",
      "Names and Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html",
    "href": "getting-started/types-of-objects.html",
    "title": "Types of Objects",
    "section": "",
    "text": "You can work with many types of data in R. Here are some of the most common types of objects you’ll encounter:\n\nNumeric: Numbers, like 5, 3.14, or -0.5.\nCharacter: Text, like \"Hello, world!\".\nFactor: Categorical data, like education_level or country.\n\nLet’s see how these types of objects work in practice.\n\nnumber &lt;- 5\nnumber\n\ncharacter &lt;- \"Hello, world!\"\ncharacter\n\nfactor &lt;- factor(c(1,2,2,3), labels = c(\"A\", \"B\", \"C\"))\nfactor\n\n\n\nThe type of an object determines what you can do with it. For example, you can perform mathematical operations on numeric objects, but not on character objects.\n\n10 + 10    # returns 20\n\"10\" + 10  # throws an error\n\n\n\n\nSometimes you have an object of the wrong type. For instance, your numeric data might have been read in as a character object.\n\nnumber &lt;- \"5\"    \n\nIf I want to perform mathematical operations on number, I need to first convert it to a numeric object. You can do this using the as.numeric function.\n\nnumber &lt;- as.numeric(number)\nnumber\nclass(number)    # numeric\n\nNote that you cannot always convert objects to a different type. Just use your common sense here.\n\nas.numeric(\"I am not a number\") \n\nWhen coercion is not possible, R will return NA (missing value) and give you a warning.",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#numeric",
    "href": "getting-started/types-of-objects.html#numeric",
    "title": "Types of Objects",
    "section": "",
    "text": "Numeric objects are used to store numbers. You can perform mathematical operations on them, like addition, subtraction and multiplication. There are also special functions for operations like rounding, taking the square root, and calculating the logarithm.\n\n10 + 10            # addition\n\n[1] 20\n\n10 - 5             # subtraction\n\n[1] 5\n\n5 * 10             # multiplication \n\n[1] 50\n\nround(3.14159, 2)  # round to 2 decimal places\n\n[1] 3.14\n\nsqrt(16)           # square root\n\n[1] 4\n\nlog(100)           # natural logarithm\n\n[1] 4.60517",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#character",
    "href": "getting-started/types-of-objects.html#character",
    "title": "Types of Objects",
    "section": "Character",
    "text": "Character\nCharacter objects are used to store text. You can use them to store names, labels, or any other kind of text data.\n“Hello, world!” # a simple character object “R is fun!” # another character object",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#types-determine-what-you-can-do-with-an-object",
    "href": "getting-started/types-of-objects.html#types-determine-what-you-can-do-with-an-object",
    "title": "Types of Objects",
    "section": "",
    "text": "The type of an object determines what you can do with it. For example, you can perform mathematical operations on numeric objects, but not on character objects.\n\n10 + 10    # returns 20\n\"10\" + 10  # throws an error",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#you-can-coerce-objects-to-different-types",
    "href": "getting-started/types-of-objects.html#you-can-coerce-objects-to-different-types",
    "title": "Types of Objects",
    "section": "",
    "text": "Sometimes you have an object of the wrong type. For instance, your numeric data might have been read in as a character object.\n\nnumber &lt;- \"5\"    \n\nIf I want to perform mathematical operations on number, I need to first convert it to a numeric object. You can do this using the as.numeric function.\n\nnumber &lt;- as.numeric(number)\nnumber\nclass(number)    # numeric\n\nNote that you cannot always convert objects to a different type. Just use your common sense here.\n\nas.numeric(\"I am not a number\") \n\nWhen coercion is not possible, R will return NA (missing value) and give you a warning.",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#what-is-a-vector",
    "href": "getting-started/types-of-objects.html#what-is-a-vector",
    "title": "Types of Objects",
    "section": "What is a vector",
    "text": "What is a vector\nA vector is simply a list of objects. We can create a vector by combining objects with the c function.\n\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\nNow we can perform mathematical operations on the entire vector at once.\n\nnumbers * 10\n\n[1] 10 20 30 40 50\n\n\nNumeric objects are used to store numbers. You can perform mathematical operations on them, like addition, subtraction and multiplication. There are also special functions for operations like rounding, taking the square root, and calculating the logarithm.\n\n10 + 10            # addition\n\n[1] 20\n\n10 - 5             # subtraction\n\n[1] 5\n\n5 * 10             # multiplication \n\n[1] 50\n\nround(3.14159, 2)  # round to 2 decimal places\n\n[1] 3.14\n\nsqrt(16)           # square root\n\n[1] 4\n\nlog(100)           # natural logarithm\n\n[1] 4.60517",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#from-single-values-to-vectors",
    "href": "getting-started/types-of-objects.html#from-single-values-to-vectors",
    "title": "Types of Objects",
    "section": "From Single Values to Vectors",
    "text": "From Single Values to Vectors\nA vector is a collection of values of the same type (e.g., all numeric or all character). You create a vector by combining individual values using the c() (combine) function. Just like with single numbers, a vector has a type.\n\nnumbers &lt;- c(1, 2, 3, 4, 5)\nclass(numbers)\n\n[1] \"numeric\"\n\n\nNow, you can perform operations on the entire vector at once:\n\nnumbers * 10\n\n[1] 10 20 30 40 50\n\nsum(numbers)\n\n[1] 15",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#from-vectors-to-data-frames",
    "href": "getting-started/types-of-objects.html#from-vectors-to-data-frames",
    "title": "Types of Objects",
    "section": "From Vectors to Data Frames",
    "text": "From Vectors to Data Frames\nSingle vectors are still not very usefull for data analysis, since we’re often interested in relations between variables. This is where data frames come in.\nA data frame is a table with rows and columns, where each row is an observation and each column is a variable. You can think of a data frame as a collection of vectors, where each vector represents a column in the dataset. We can create a data frame by combining vectors using the data.frame() function.1\n\ncountry &lt;- c(\"NL\", \"NL\", \"BE\", \"BE\", \"DE\", \"DE\", \"FR\", \"FR\", \"UK\", \"UK\")\nheight  &lt;- c(176 , 165 , 172 , 160 , 180 , 170 , 175 , 165 , 185 , 175 )\n\nd &lt;- data.frame(country, height)\n\nNow, you can perform operations on the entire data frame at once! You can for instance perform a statistical test to see if the average height differs between countries.\nAnd just like with single values and vectors, we need to take the type of each column into account. For instance, we can’t perform a correlation analysis using the country column, because it’s a character vector.",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#footnotes",
    "href": "getting-started/types-of-objects.html#footnotes",
    "title": "Types of Objects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThroughout this book we will actually be using the tibble data structure from the tidyverse package, which is an improved version of the data.frame.↩︎",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/file-management.html#working-directory",
    "href": "getting-started/file-management.html#working-directory",
    "title": "File management",
    "section": "",
    "text": "Whenever you are working in R, you are always working in a specific directory on your computer. This directory is called the working directory. You can check the current working directory with the getwd() (get working directory) function:\n\ngetwd()\n\nIf a file is inside of your working directory, you can easily read it by just providing the file name. Also, if you write a file without specifying a directory, it will be saved in this directory. Therefore, a good way to make data management in R easy, is by making a directory for your current project, and setting this as your working directory.\nOpen your file explorer (outside of RStudio) and create a directory somewhere with a name like “R_projects”. Then copy the fake_demo_data.csv file you downloaded earlier into this directory.\nNow, you can set this directory as your working directory in R. There are two ways to do this:\n\nManually: You can set the working directory with the setwd() function. For example, to set the working directory to /home/you/R_projects, you can run setwd(\"/home/you/R_projects\").\nUsing RStudio: In RStudio, you can set the working directory by clicking on Session in the menu bar, then Set Working Directory, and then Choose Directory. This will open a file explorer window where you can select the directory you want to set as the working directory.\n\nIf you did everything correctly, you should now be able to read the file using the following code:\n\nread_csv('fake_demo_data.csv')",
    "crumbs": [
      "Getting Started",
      "File management"
    ]
  },
  {
    "objectID": "2_data_preparation/inspect-and-clean.html#just-look",
    "href": "2_data_preparation/inspect-and-clean.html#just-look",
    "title": "Inspecting and Cleaning Data",
    "section": "Just …look",
    "text": "Just …look\nIt never hurts to just take a first general look at your data. Especially when you’re working with a new dataset, just looking at what columns you have, and what the first few rows look like, can give you a decent first impression. For a tibble with a few columns you can just print it to the console:\n\nd\n\nBut another nice way to look at the data is to use the View function in RStudio (or clicking on the name of the tibble in the Environment tab in the top-right corner).\n\nView(d)",
    "crumbs": [
      "Data preparation",
      "Inspecting and Cleaning Data"
    ]
  },
  {
    "objectID": "2_data_preparation/inspect-and-clean.html#footnotes",
    "href": "2_data_preparation/inspect-and-clean.html#footnotes",
    "title": "Inspecting and Cleaning Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr the more common version: “Sht in, sht out”.↩︎",
    "crumbs": [
      "Data preparation",
      "Inspecting and Cleaning Data"
    ]
  }
]