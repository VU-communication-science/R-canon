[
  {
    "objectID": "analysis/techniques/data-cleaning.html",
    "href": "analysis/techniques/data-cleaning.html",
    "title": "Data cleaning",
    "section": "",
    "text": "Required packages and data for this tutorial\n\n\n\nIn this tutorial we use the tidyverse and summarytools packages, and the simulated practice data.\n\nlibrary(tidyverse)\nlibrary(summarytools)\nd &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Data cleaning"
    ]
  },
  {
    "objectID": "analysis/techniques/data-cleaning.html#just-look",
    "href": "analysis/techniques/data-cleaning.html#just-look",
    "title": "Data cleaning",
    "section": "Just …look",
    "text": "Just …look\nIt never hurts to just take a first general look at your data. Especially when you’re working with a new dataset, just looking at what columns you have, and what the first few rows look like, can give you a decent first impression. For a tibble with a few columns you can just print it to the console:\n\nd\n\nBut another nice way to look at the data is to use the View function in RStudio (or clicking on the name of the tibble in the Environment tab in the top-right corner of RStudio).\n\nView(d)",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Data cleaning"
    ]
  },
  {
    "objectID": "analysis/techniques/data-cleaning.html#viewing-a-summary-of-every-column",
    "href": "analysis/techniques/data-cleaning.html#viewing-a-summary-of-every-column",
    "title": "Data cleaning",
    "section": "Viewing a summary of every column",
    "text": "Viewing a summary of every column\nViewing the top of your data is a good start, but there are many things that can go wrong that you won’t be able to spot. There can be missing values, incorrect data types, or outliers that are hard to spot just by looking at the data. So in addition you will want to view summaries of all the columns that you intend to use in your analysis.2\nOne way to get a quick summary is by just using the summary function in R. This is a generic function that you can use on many types of objects, and when you use it on a tibble (or regular data frame) it will give you a summary of each column.\n\nsummary(d)\n\nThe summary includes various usefull statistics, like the minimum and maximum values, the median, and the number of missing values (NA). However, this output can be a bit hard to read, especially when you have many columns. A great alternative is to use the summarytools package, which can create summaries in a nice table, including small graphs for each column to get a quick impression of the distribution.\n\nlibrary(summarytools)\ndfSummary(d)\n\nThis will print the summary in your console, but you can also render it in a more readable format with the view (With a small v) function.\n\ndfSummary(d) |&gt; view()\n\n\n\n\nData Frame Summary\nd\nDimensions: 600 x 17\n  Duplicates: 0\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n\n1\nid [numeric]\n\n\n\nMean (sd) : 300.5 (173.3)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 300.5 ≤ 600\n\n\nIQR (CV) : 299.5 (0.6)\n\n\n\n600 distinct values\n\n600 (100.0%)\n0 (0.0%)\n\n\n2\nage [numeric]\n\n\n\nMean (sd) : 51.3 (137.7)\n\n\nmin ≤ med ≤ max:\n\n\n17 ≤ 41 ≤ 1987\n\n\nIQR (CV) : 22 (2.7)\n\n\n\n52 distinct values\n\n595 (99.2%)\n5 (0.8%)\n\n\n3\nnp_subscription [character]\n\n\n\n1. no\n\n\n2. yes\n\n\n\n\n\n\n263\n(\n43.8%\n)\n\n\n337\n(\n56.2%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n4\nnews consumption [numeric]\n\n\n\nMean (sd) : 10.1 (3.2)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 11 ≤ 17\n\n\nIQR (CV) : 4 (0.3)\n\n\n\n18 distinct values\n\n600 (100.0%)\n0 (0.0%)\n\n\n5\nexperiment_group [character]\n\n\n\n1. control\n\n\n2. negative\n\n\n3. positive\n\n\n\n\n\n\n200\n(\n33.3%\n)\n\n\n200\n(\n33.3%\n)\n\n\n200\n(\n33.3%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n6\ntrust_t1 [numeric]\n\n\n\nMean (sd) : 3.9 (1)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 3.8 ≤ 7.6\n\n\nIQR (CV) : 1.4 (0.3)\n\n\n\n28 distinct values\n\n600 (100.0%)\n0 (0.0%)\n\n\n7\ntrust_t2 [numeric]\n\n\n\nMean (sd) : 3.8 (1.4)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 3.8 ≤ 8\n\n\nIQR (CV) : 1.8 (0.4)\n\n\n\n35 distinct values\n\n600 (100.0%)\n0 (0.0%)\n\n\n8\ntrust_t1_item1 [numeric]\n\n\n\nMean (sd) : 4.1 (1.1)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 4 ≤ 8\n\n\nIQR (CV) : 2 (0.3)\n\n\n\n\n\n\n1\n:\n5\n(\n0.8%\n)\n\n\n2\n:\n44\n(\n7.3%\n)\n\n\n3\n:\n130\n(\n21.7%\n)\n\n\n4\n:\n198\n(\n33.0%\n)\n\n\n5\n:\n160\n(\n26.7%\n)\n\n\n6\n:\n58\n(\n9.7%\n)\n\n\n7\n:\n4\n(\n0.7%\n)\n\n\n8\n:\n1\n(\n0.2%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n9\ntrust_t1_item2 [numeric]\n\n\n\nMean (sd) : 2.7 (1.5)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 3 ≤ 7\n\n\nIQR (CV) : 3 (0.5)\n\n\n\n\n\n\n1\n:\n159\n(\n26.5%\n)\n\n\n2\n:\n137\n(\n22.8%\n)\n\n\n3\n:\n133\n(\n22.2%\n)\n\n\n4\n:\n87\n(\n14.5%\n)\n\n\n5\n:\n61\n(\n10.2%\n)\n\n\n6\n:\n20\n(\n3.3%\n)\n\n\n7\n:\n3\n(\n0.5%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n10\ntrust_t1_item3 [numeric]\n\n\n\nMean (sd) : 6.7 (1.3)\n\n\nmin ≤ med ≤ max:\n\n\n2 ≤ 7 ≤ 10\n\n\nIQR (CV) : 2 (0.2)\n\n\n\n\n\n\n2\n:\n1\n(\n0.2%\n)\n\n\n3\n:\n2\n(\n0.3%\n)\n\n\n4\n:\n29\n(\n4.8%\n)\n\n\n5\n:\n76\n(\n12.7%\n)\n\n\n6\n:\n170\n(\n28.3%\n)\n\n\n7\n:\n159\n(\n26.5%\n)\n\n\n8\n:\n118\n(\n19.7%\n)\n\n\n9\n:\n37\n(\n6.2%\n)\n\n\n10\n:\n8\n(\n1.3%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n11\ntrust_t1_item4 [numeric]\n\n\n\nMean (sd) : 3.7 (1.1)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 4 ≤ 7\n\n\nIQR (CV) : 1 (0.3)\n\n\n\n\n\n\n1\n:\n11\n(\n1.8%\n)\n\n\n2\n:\n73\n(\n12.2%\n)\n\n\n3\n:\n176\n(\n29.3%\n)\n\n\n4\n:\n205\n(\n34.2%\n)\n\n\n5\n:\n111\n(\n18.5%\n)\n\n\n6\n:\n23\n(\n3.8%\n)\n\n\n7\n:\n1\n(\n0.2%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n12\ntrust_t1_item5 [numeric]\n\n\n\nMean (sd) : 4.6 (1.2)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 5 ≤ 9\n\n\nIQR (CV) : 1 (0.3)\n\n\n\n\n\n\n1\n:\n3\n(\n0.5%\n)\n\n\n2\n:\n19\n(\n3.2%\n)\n\n\n3\n:\n91\n(\n15.2%\n)\n\n\n4\n:\n149\n(\n24.8%\n)\n\n\n5\n:\n194\n(\n32.3%\n)\n\n\n6\n:\n108\n(\n18.0%\n)\n\n\n7\n:\n35\n(\n5.8%\n)\n\n\n9\n:\n1\n(\n0.2%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n13\ntrust_t2_item1 [numeric]\n\n\n\nMean (sd) : 4 (1.5)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 4 ≤ 8\n\n\nIQR (CV) : 2 (0.4)\n\n\n\n\n\n\n1\n:\n31\n(\n5.2%\n)\n\n\n2\n:\n71\n(\n11.8%\n)\n\n\n3\n:\n108\n(\n18.0%\n)\n\n\n4\n:\n160\n(\n26.7%\n)\n\n\n5\n:\n129\n(\n21.5%\n)\n\n\n6\n:\n73\n(\n12.2%\n)\n\n\n7\n:\n15\n(\n2.5%\n)\n\n\n8\n:\n13\n(\n2.2%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n14\ntrust_t2_item2 [numeric]\n\n\n\nMean (sd) : 2.7 (1.6)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 2 ≤ 9\n\n\nIQR (CV) : 3 (0.6)\n\n\n\n\n\n\n1\n:\n199\n(\n33.2%\n)\n\n\n2\n:\n112\n(\n18.7%\n)\n\n\n3\n:\n109\n(\n18.2%\n)\n\n\n4\n:\n95\n(\n15.8%\n)\n\n\n5\n:\n50\n(\n8.3%\n)\n\n\n6\n:\n22\n(\n3.7%\n)\n\n\n7\n:\n9\n(\n1.5%\n)\n\n\n8\n:\n3\n(\n0.5%\n)\n\n\n9\n:\n1\n(\n0.2%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n15\ntrust_t2_item3 [numeric]\n\n\n\nMean (sd) : 6.7 (1.7)\n\n\nmin ≤ med ≤ max:\n\n\n2 ≤ 7 ≤ 10\n\n\nIQR (CV) : 2 (0.2)\n\n\n\n\n\n\n2\n:\n3\n(\n0.5%\n)\n\n\n3\n:\n17\n(\n2.8%\n)\n\n\n4\n:\n31\n(\n5.2%\n)\n\n\n5\n:\n80\n(\n13.3%\n)\n\n\n6\n:\n141\n(\n23.5%\n)\n\n\n7\n:\n133\n(\n22.2%\n)\n\n\n8\n:\n101\n(\n16.8%\n)\n\n\n9\n:\n62\n(\n10.3%\n)\n\n\n10\n:\n32\n(\n5.3%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n16\ntrust_t2_item4 [numeric]\n\n\n\nMean (sd) : 3.6 (1.4)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 4 ≤ 8\n\n\nIQR (CV) : 1 (0.4)\n\n\n\n\n\n\n1\n:\n43\n(\n7.2%\n)\n\n\n2\n:\n83\n(\n13.8%\n)\n\n\n3\n:\n151\n(\n25.2%\n)\n\n\n4\n:\n179\n(\n29.8%\n)\n\n\n5\n:\n97\n(\n16.2%\n)\n\n\n6\n:\n33\n(\n5.5%\n)\n\n\n7\n:\n10\n(\n1.7%\n)\n\n\n8\n:\n4\n(\n0.7%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n17\ntrust_t2_item5 [numeric]\n\n\n\nMean (sd) : 4.5 (1.7)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 5 ≤ 9\n\n\nIQR (CV) : 3 (0.4)\n\n\n\n\n\n\n1\n:\n25\n(\n4.2%\n)\n\n\n2\n:\n50\n(\n8.3%\n)\n\n\n3\n:\n99\n(\n16.5%\n)\n\n\n4\n:\n118\n(\n19.7%\n)\n\n\n5\n:\n138\n(\n23.0%\n)\n\n\n6\n:\n106\n(\n17.7%\n)\n\n\n7\n:\n44\n(\n7.3%\n)\n\n\n8\n:\n10\n(\n1.7%\n)\n\n\n9\n:\n10\n(\n1.7%\n)\n\n\n\n\n600 (100.0%)\n0 (0.0%)\n\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.4.1)2024-09-04\n\n\n\n\n\n\n\n\n\nUsing summarytools in Quarto or RMarkdown\n\n\n\n\n\nIf you’re working in Quarto or RMarkdown, the normal way of using summarytools doesn’t create a nice table. Instead, what you need to do is tell Quarto to render the output as HTML. You can do this by setting the method argument to render.\n\nprint(dfSummary(iris), \n      method='render', \n      max.tbl.height = 500)\n\nNote that we also set the max.tbl.height argument to 500, to make sure that very large tables are put inside a scrollable window.\n\n\n\n\nWhat to look for in the summary\nThere are a few things in particular that you should look out for:\n\nMissing values: Are there any columns with a lot of missing values? If so, you might need to think about how to deal with these.\nData types: Are the data types of the columns correct? For example, are numbers stored as numbers, and not as text?\nOutliers: Are there any columns with very high or very low values? These might be errors in the data.\nDistribution: Are the values in the columns distributed in a way that you would expect? For example, if you have a column with ages, are there any values that are negative, or over 100?\n\nTaking this into account, most of the columns in our data look pretty good. Our trust variables (trust_t1 and trust_t2) are on a scale from 1 to 10, with a pretty normal distribution. We have a completely balanced distribution over the three experimental groups. The news consumption variable is a bit skewed, but nobody’s perfect. However, there is one column that stands out sorely: age.\nThe first sign that something is off is the histogram, which shows a very strange distribution where pretty much all observations are in the same bin on the left side. The reason for this becomes clear when we look at the maximum value, which is 1987. This is clearly an error in the data, as it is very unlikely that someone in our study is 1987 years old. What is much more likely is that at least one person entered their birth year instead of their age.\nTo see if this is the case, we can look at the unique values in the age column. Recall that we can refer to a vector in a data frame using the $ operator, like d$age. When we run the unique function on this vector, we see multiple several cases that indeed look like birth years.\n\nunique(d$age)\n\n [1]   48   29   52   37   19   25   40   59   30   34   35   57   65   62   39\n[16]   20   46   22 1967   31   60   44   49   55   58   51   33   42   41   64\n[31]   21   27   24   18   NA   36   32   26   45   38   47   63   43   56   23\n[46]   28   50 1970   54 1987   53   61   17\n\n\nFinally, one thing to look out for when inspecting the data is missing values, which are often coded as NA (not available) in R. In the summary we see that there are 5 missing values in the age column. This isn’t too bad, but we do need to be transparent about how we deal with these missing values in our analysis.",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Data cleaning"
    ]
  },
  {
    "objectID": "analysis/techniques/data-cleaning.html#selecting-columns",
    "href": "analysis/techniques/data-cleaning.html#selecting-columns",
    "title": "Data cleaning",
    "section": "Selecting columns",
    "text": "Selecting columns\nThe first thing we can do is to select only the columns that we need for our analysis, and where needed rename them. For the current example, let’s say that we just need the columns id, age and news consumption. While selecting them, we’ll immediately also rename the news consumption column to news_consumption, because spaces in column names can be a bit annoying to work with.\n\nd_clean &lt;- select(d, id, age, news_consumption = `news consumption`)\n\nNote that we assigned the result to a new tibble called d_clean. This is a good practice, because it keeps the original data frame intact, and you can always go back to it if you need to.",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Data cleaning"
    ]
  },
  {
    "objectID": "analysis/techniques/data-cleaning.html#mutating-columns",
    "href": "analysis/techniques/data-cleaning.html#mutating-columns",
    "title": "Data cleaning",
    "section": "Mutating columns",
    "text": "Mutating columns\nNext, we need to deal with the age column. When inspecting the data we saw that there were some birth years in there. Let’s consider some ways to address this.\n\nOption 1: Subtract the birth year from the year of the study\nThe easiest (and in this case best) way is to subtract the birth year from the year in which the study took place. We need to do this only for the birth years, and not for the actual ages, so we’ll use the if_else function. This let’s us say: if some condition is met, do this, else do that. The format is: if_else(condition, if_true, if_false).\nIn this case, our condition is that the number is a birth year, which we can do by checking if it’s unrealistically high, like over 200. When we can say: if the age is greater than 200, subtract it from the year of the study, otherwise just return the age.\n\nd_clean &lt;- mutate(d_clean, age = if_else(age &gt; 200, 2024 - age, age))\n\nNotice that this time we also used the d_clean tibble as input, and assigned the result back to d_clean to overwrite it. A common mistake would be to use d as input, but then we’d lose our previous clean up steps.\n\n\nOption 2: Recode specific values\nAnother option would be to recode the specific values that are wrong. This is more work, but it is a versatile technique that you can use in many situations.\nUsing the recode function, we can say what values we want to recode into what. The syntax is recode(column, current_value = new_value, ...).\n\nd_clean = mutate(d_clean, age = recode(age, `1987` = 37, `1970` = 51, `1967` = 54))\n\n\n\nThe nuclear option: just remove them\nIn this case we were able to correct the data, but sometimes that’s not an option. In that case you’d have to remove the rows with incorrect data (which is the same as selecting the rows that are correct).\n\nd_clean &lt;- filter(d_clean, age &lt; 200)",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Data cleaning"
    ]
  },
  {
    "objectID": "analysis/techniques/data-cleaning.html#filtering-rows",
    "href": "analysis/techniques/data-cleaning.html#filtering-rows",
    "title": "Data cleaning",
    "section": "Filtering rows",
    "text": "Filtering rows\nFinally, we’ll remove any rows that we do want to use. This can be because we do not want to include missing values in our analysis, but also because there might be data we do not want to use. Let’s say that we want to remove participants with missing values, and participants that are younger than 18.\nTo remove cases, we can use the filter function to select the cases we do want to keep. To filter out people below 17, we can use the age &gt;= 18 condition. For filtering out missing values, we can use the is.na function to check whether a value is missing, and then say that it should NOT (!) be missing: !is.na(age). We can then combine these two conditions with the & (AND) operator.\n\nd_clean &lt;- filter(d_clean, !is.na(age) & age &gt;= 18)",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Data cleaning"
    ]
  },
  {
    "objectID": "analysis/techniques/data-cleaning.html#footnotes",
    "href": "analysis/techniques/data-cleaning.html#footnotes",
    "title": "Data cleaning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr the more common version: “Sh*t in, sh*t out”.↩︎\nIf you have a very large dataset, you might first want to select only the columns that you need for your analysis. We’ll show you how to do this in the next section.↩︎",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Data cleaning"
    ]
  },
  {
    "objectID": "analysis/concepts/covariance-and-correlation.html",
    "href": "analysis/concepts/covariance-and-correlation.html",
    "title": "Covariance and correlation",
    "section": "",
    "text": "The covariance of two variables tells us something about the extent to which they change together. If one variable goes up, does the other variable go up as well? This information is the foundation for many statistical methods, because it enables us to quantify relationships between phenomena. For example, it allows us to measure: to what extent is a person’s social media use related to their feelings of loneliness?\nThe correlation is a standardized version of the covariance, which expresses the strength and direction of the relationship between two variables on a scale from -1 to 1. This makes it easier to interpret. Consider the following example:\n\nif the correlation between a person’s social media use and their feelings of loneliness is positive (&gt; 0), it means that people that spend more time on social media tend to feel more lonely.\nIf the correlation is negative (&lt; 0), it means that people that spend more time on social media tend to feel less lonely.\nIf the correlation is (close to) -1 or 1, it means that the relationship is strong.\nIf the correlation is (close to) 0, it means that the relationship is weak or absent.\n\nDeveloping a good intuition for covariance and correlation is important, because it helps you to understand relationships between variables, and to interpret the results of statistical tests. In this tutorial we’ll develop this intuition by looking at visualizations, and calculating the covariance and correlation ourselves.",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Covariance and correlation"
    ]
  },
  {
    "objectID": "analysis/concepts/covariance-and-correlation.html#the-difference-between-covariance-and-correlation",
    "href": "analysis/concepts/covariance-and-correlation.html#the-difference-between-covariance-and-correlation",
    "title": "Covariance and correlation",
    "section": "The difference between covariance and correlation",
    "text": "The difference between covariance and correlation\nIf we look at the plots above, we see that the covariance and correlation are closely related. The difference is that the correlation is standardized, while the covariance is not. This means that:\n\nThe covariance can be any number, while the correlation is always between -1 and 1.\nThe covariance depends on the scale of the variables.\n\nBy the scale we mean the units in which the variables are measured. For instance, we can measure social media use in hours or in minutes per day. This shouldn’t change the relation between social media use and feelings of loneliness, but it does change the covariance.\nTo illustrate this, let’s plot the positive correlation scenario from above three times:\n\nonce with the X variable (social media use) in hours\nonce with the X variable in minutes\nonce with both the X and Y variables standardized (i.e, turn them into z-scores).\n\n\n\n\n\n\n\n\n\n\nAs we can see, the direction and strength of the relation is the same in all three plots. The only thing that changed is the scale of the variables, which you can see in the X and Y axis. This doesn’t affect the correlation (\\(r = 0.887\\) in all three cases), but it does affect the covariance:\n\nFor the original data, the covariance is \\(1.287\\), as we saw before\nFor the data where we increased the scale of X by a factor 60 (to go from hours to minutes), the covariance increased to \\(76.683\\).\nIn the final plot we standardized both the X and Y variables. As a result, the covariance is now identical to the correlation (\\(r = 0.887, cov = 0.887\\)). This is what we mean when we say that the correlation is the standardized version of the covariance!",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Covariance and correlation"
    ]
  },
  {
    "objectID": "analysis/concepts/covariance-and-correlation.html#calculate-covariance",
    "href": "analysis/concepts/covariance-and-correlation.html#calculate-covariance",
    "title": "Covariance and correlation",
    "section": "Calculate covariance",
    "text": "Calculate covariance\nThe formula for the calculating the covariance between two variables \\(X\\) and \\(Y\\) in a sample1 is:\n\\[ cov(X, Y) = \\frac{\\sum (X_i - \\bar{X}) (Y_i - \\bar{Y})}{n-1} \\]\n\n\\(X_i\\) and \\(Y_i\\) are the individual values of \\(X\\) and \\(Y\\)\n\\(\\bar{X}\\) and \\(\\bar{Y}\\) are the means of \\(X\\) and \\(Y\\)\n\\(n\\) is the number of observations\n\\(\\sum\\) is the sum over all observations\n\nLet’s perform this calculation in steps, to make it easier to follow. First, calculate the means for x and y.\n\nd &lt;- mutate(d, x_mean = mean(x),\n               y_mean = mean(y))\n\nNow calculate the deviation from the mean for both variables. This is the \\(X_i - \\bar{X}\\) and \\(Y_i - \\bar{Y}\\) part of the formula.\n\nd &lt;- mutate(d, x_deviation = x - x_mean,\n               y_deviation = y - y_mean)\n\nThen multiply these deviations.\n\nd &lt;- mutate(d, deviation_product = x_deviation * y_deviation)\n\nHave a look at the intermediate results to see what we’ve done so far. Try to do the calculations in your head to see if you understand what’s going on.\n\nd\n\n\n  \n\n\n\nFinally, sum the deviation products and divide by \\(n-1\\) to get the covariance. Here we use the sum() function to calculate the sum of the deviation_product column, and the nrow() function to get the number of observations in the data d.\n\ndev_sum = sum(d$deviation_product)\nn = nrow(d)        \ndev_sum / (n - 1)\n\n[1] 22.5\n\n\nHere we use the summarize function, which allows us to calculate summarizing statistics functions, such as sum() and n() (which gives the number of observations).\nYou can check your answer by using the cov() function, which is R’s built-in function for calculating the covariance.\n\ncov(d$x, d$y)\n\n[1] 22.5",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Covariance and correlation"
    ]
  },
  {
    "objectID": "analysis/concepts/covariance-and-correlation.html#calculate-correlation",
    "href": "analysis/concepts/covariance-and-correlation.html#calculate-correlation",
    "title": "Covariance and correlation",
    "section": "Calculate correlation",
    "text": "Calculate correlation\nThe correlation is the standardized covariance. There are two ways to go about this:\n\nFirst standardize the variables, then calculate the covariance\nWe can repeat the covariance calculation, but this time standardize the variables first. This is also what we showed in the previous section, where we plotted the data with the variables standardized.\nTo standardize a variable, we subtract the mean and divide by the standard deviation (there is also the built in function scale() that can do this for you). Be carefull to use the parentheses correctly!\n\nd &lt;- mutate(d, x_z = (x - mean(x)) / sd(x),    \n               y_z = (y - mean(y)) / sd(y))\n\nNow you can repeat the covariance calculation, but this time with the standardized variables x_z and y_z instead of x and y. Here we won’t repeat the steps, but instead use the built in cov() function with the standardized variables. We’ll also calculate the correlation of x and y using the cor() function, so that you can see that it works.\n\ncov(d$x_z, d$y_z)\n\n[1] 0.9\n\ncor(d$x, d$y)\n\n[1] 0.9\n\n\n\n\nFirst calculate covariance, then standardize it\nThe other way to calculate the correlation is to first calculate the covariance, and then standardize it. For this we also need to know the variance of both x and y. Luckily, you already know how to calculate the variance, because this is the same as calculating the covariance of a variable with itself! We heartily invite you to try this yourself, but here we’ll just let R take care of the covariances for us. For this we again use the cov function, but this time we give it a tibble as input. R will then return the covariance matrix for all variables in the tibble.\n\nd |&gt;\n  select(x, y) |&gt;\n  cov()\n\n      x    y\nx 250.0 22.5\ny  22.5  2.5\n\n\nHere we see four values:\n\nThe covariance of x with x (the variance of x), which is 250.\nThe covariance of y with y (the variance of y), which is 2.5.\nThe covariance of x with y, which we already know is 22.5.\nThe covariance of y with x, which is the same as the covariance of x with y, because the covariance is symmetric.\n\nGiven this information, the correlation between x and y can be calculated as:\n\\[ r = \\frac{cov(X, Y)}{S_X S_Y} \\]\nWhere \\(S_X\\) is the standard deviation of \\(X\\), which you get by taking the square root of the variance. \\[ S_X = \\sqrt{var(X)} \\]\nSo the calculation is:\n\ncov_xy = 22.5\nSx = sqrt(250)\nSy = sqrt(2.5)\n\ncov_xy / (Sx * Sy)\n\n[1] 0.9\n\n\nWhich is the same as the correlation we calculated before!",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Covariance and correlation"
    ]
  },
  {
    "objectID": "analysis/concepts/covariance-and-correlation.html#footnotes",
    "href": "analysis/concepts/covariance-and-correlation.html#footnotes",
    "title": "Covariance and correlation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe formula is different depending on whether you are working with a sample or the entire population. Above we showed the formula for a sample, which is the most common case in practice. To calculate the covariance for the entire population, the only difference is that you would divide by \\(n\\) instead of \\(n-1\\): \\(\\frac{\\sum (X_i - \\bar{X}) (Y_i - \\bar{Y})}{n}\\).↩︎",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Covariance and correlation"
    ]
  },
  {
    "objectID": "getting-started/functions.html",
    "href": "getting-started/functions.html",
    "title": "Functions",
    "section": "",
    "text": "99% of what you do in R will involve using functions. A function in R is like a mini-program that you can use to perform specific tasks. It takes input, processes it, and gives you an output. For example, there are functions for:\n\nimporting data\ncomputing descriptive statistics\nperforming statistical tests\nvisualizing data\n\nA function in R has the form:\n\noutput &lt;- function_name(argument1, argument2, ...)`\n\n\nfunction_name is a name to indicate which function you want to use. It is followed by parentheses.\narguments are the input of the function, and are inserted within the parentheses.\noutput is anything that is returned by the function.\n\nFor example, the function c combines multiple values into a vector.\n\nx = c(1,2,3,4)\n\nNow, we can use the mean function to calculate the mean of these numbers:\n\nm &lt;- mean(x)\n\nThe calculated mean, 2.5, is now assigned to the name m:\n\nm\n\n[1] 2.5\n\n\n\n\nIn the c and mean functions above, all the arguments were required. To combine numbers into a vector, we needed to provide a list of numbers. To calculate a mean, we needed to provide a numeric vector.\nIn addition to the required arguments, a function can also have optional arguments, that give you more control over what a function does. For example, suppose we have a range of numbers that also contains a missing value. In R a missing value is called NA, which stands for Not Available:\n\nx_with_missing &lt;- c(1, 2, 3, NA, 4)\n\nNow, if we call the mean function, R will say that the mean is unknown, since the third value is unknown:\n\nmean(x_with_missing)\n\n[1] NA\n\n\nThis is statistically a very correct answer. But often, if some values happen to be missing in your data, you want to be able to calculate the mean just for the numbers that are not missing. Fortunately, the mean function has an optional argument na.rm (remove NAs) that you can set to TRUE (or to T, which is short for TRUE) to ignore the NAs:\n\nmean(x, na.rm=TRUE)\n\n[1] 2.5\n\n\nNotice that for the required argument, we directly provide the input x, but for the optional argument we include the argument name na.rm = TRUE. The reason is simply that there are other optional arguments, so we need to specify which one we’re using.\n\n\n\n\n\n\nHow do I know what arguments a function has?\n\n\n\n\n\nTo learn more about what a function does and what arguments it has, you can look it up in the ‘Help’ pane in the bottom right, or run ?function_name in R.\n\n?mean\n\nHere you can learn about the na.rm argument that we just used!\nIf you are just getting to know R, we recommend first finishing the rest of the Getting Started section. Then once you get the hang of things, have a look at the Use ?function help page tutorial.\n\n\n\n\n\n\nThere is another common way to use functions in R using the pipe syntax. With the pipe syntax, you can pipe the first argument into the function, instead of putting it inside the parentheses. As you will see below, this allows you to create a pipeline of functions, which is often easier to read. The pipe syntax uses the |&gt; operator, which is used as follows:\n\nargument1 |&gt; function_name(argument2, ...)\n\nFor example, the following two lines of code give identical output:\n\nmean(x_with_missing, na.rm=T)\n\n[1] 2.5\n\nx_with_missing |&gt; mean(na.rm=T)\n\n[1] 2.5\n\n\nNotice how our first argument, the required argument x_with_missing, is piped into the mean function. Inside the mean function, we only specify the second argument, the optional argument na.rm.\nSo why do we need this alternative way of doing the same thing? The reason is that when writing code, you shouldn’t just think about what the code does, but also about how easy the code is to read. This not only helps you prevent mistakes, but also makes your analysis transparent. As you’ll see later, you’ll encounter many cases where your analysis requires you to string together multiple functions. In these cases, pipes make your code much easier to read.\nFor example, imagine we would want to round the result (2.5) up to a round number (3). With the pipe syntax we can just add the round function to our pipeline.\n\nx_with_missing |&gt; \n  mean(na.rm=T) |&gt; \n  round()\n\nYou’ll see how powerful this can be later on, especially in the Data Management chapter. In order to prepare and clean up your data, you’ll often need to perform a series of functions in a specific order. The pipe syntax allows you to do this in a very readable way.",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "getting-started/functions.html#optional-arguments",
    "href": "getting-started/functions.html#optional-arguments",
    "title": "Functions",
    "section": "",
    "text": "In the c and mean functions above, all the arguments were required. To combine numbers into a vector, we needed to provide a list of numbers. To calculate a mean, we needed to provide a numeric vector.\nIn addition to the required arguments, a function can also have optional arguments, that give you more control over what a function does. For example, suppose we have a range of numbers that also contains a missing value. In R a missing value is called NA, which stands for Not Available:\n\nx_with_missing &lt;- c(1, 2, 3, NA, 4)\n\nNow, if we call the mean function, R will say that the mean is unknown, since the third value is unknown:\n\nmean(x_with_missing)\n\n[1] NA\n\n\nThis is statistically a very correct answer. But often, if some values happen to be missing in your data, you want to be able to calculate the mean just for the numbers that are not missing. Fortunately, the mean function has an optional argument na.rm (remove NAs) that you can set to TRUE (or to T, which is short for TRUE) to ignore the NAs:\n\nmean(x, na.rm=TRUE)\n\n[1] 2.5\n\n\nNotice that for the required argument, we directly provide the input x, but for the optional argument we include the argument name na.rm = TRUE. The reason is simply that there are other optional arguments, so we need to specify which one we’re using.\n\n\n\n\n\n\nHow do I know what arguments a function has?\n\n\n\n\n\nTo learn more about what a function does and what arguments it has, you can look it up in the ‘Help’ pane in the bottom right, or run ?function_name in R.\n\n?mean\n\nHere you can learn about the na.rm argument that we just used!\nIf you are just getting to know R, we recommend first finishing the rest of the Getting Started section. Then once you get the hang of things, have a look at the Use ?function help page tutorial.",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "getting-started/functions.html#pipe-syntax",
    "href": "getting-started/functions.html#pipe-syntax",
    "title": "Functions",
    "section": "",
    "text": "There is another common way to use functions in R using the pipe syntax. With the pipe syntax, you can pipe the first argument into the function, instead of putting it inside the parentheses. As you will see below, this allows you to create a pipeline of functions, which is often easier to read. The pipe syntax uses the |&gt; operator, which is used as follows:\n\nargument1 |&gt; function_name(argument2, ...)\n\nFor example, the following two lines of code give identical output:\n\nmean(x_with_missing, na.rm=T)\n\n[1] 2.5\n\nx_with_missing |&gt; mean(na.rm=T)\n\n[1] 2.5\n\n\nNotice how our first argument, the required argument x_with_missing, is piped into the mean function. Inside the mean function, we only specify the second argument, the optional argument na.rm.\nSo why do we need this alternative way of doing the same thing? The reason is that when writing code, you shouldn’t just think about what the code does, but also about how easy the code is to read. This not only helps you prevent mistakes, but also makes your analysis transparent. As you’ll see later, you’ll encounter many cases where your analysis requires you to string together multiple functions. In these cases, pipes make your code much easier to read.\nFor example, imagine we would want to round the result (2.5) up to a round number (3). With the pipe syntax we can just add the round function to our pipeline.\n\nx_with_missing |&gt; \n  mean(na.rm=T) |&gt; \n  round()\n\nYou’ll see how powerful this can be later on, especially in the Data Management chapter. In order to prepare and clean up your data, you’ll often need to perform a series of functions in a specific order. The pipe syntax allows you to do this in a very readable way.",
    "crumbs": [
      "Getting Started",
      "Functions"
    ]
  },
  {
    "objectID": "data-management/select-and-rename.html",
    "href": "data-management/select-and-rename.html",
    "title": "Select and rename",
    "section": "",
    "text": "Required packages and data for this tutorial\n\n\n\nIn this tutorial we use the tidyverse package and the simulated practice data.\n\nlibrary(tidyverse)\nd &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")\n\n\n\n\nSelecting columns with select\nOften you do not need to use all columns in your data, or you only need a subset of the columns for a specific analysis. You can do this with the select function.\nFirst, let’s see what columns are in our data using the colnames function, which returns the column names of a data frame:\n\ncolnames(d)\n\n [1] \"id\"               \"age\"              \"np_subscription\"  \"news consumption\"\n [5] \"experiment_group\" \"trust_t1\"         \"trust_t2\"         \"trust_t1_item1\"  \n [9] \"trust_t1_item2\"   \"trust_t1_item3\"   \"trust_t1_item4\"   \"trust_t1_item5\"  \n[13] \"trust_t2_item1\"   \"trust_t2_item2\"   \"trust_t2_item3\"   \"trust_t2_item4\"  \n[17] \"trust_t2_item5\"  \n\n\n\nSelecting specific columns\nThe simplest way of using select is to explicitly specify the columns you want to keep:\n\nds &lt;- select(d, age, np_subscription)\n\nThis will return a new tibble with only the columns id, age, and np_subscription. We assigned this new tibble to the variable ds (short for “data subset”). Sometimes you want to overwrite the original data frame with the new selection. You can do this by assigning the result to the same name as the input (d in this case):\n\nd &lt;- select(d, age, np_subscription)\n\nJust be carefull with this. One of the nice things about R is that you can have multiple versions of your data in different tibbles. It is often smart to at least keep the original (raw) data frame intact.\n\n\nSelecting a range of columns\nYou can also specify a range of columns using the syntax first_column:last_column. For example, to select all columns from experiment_group to trust_t2:\n\nselect(d, experiment_group:trust_t2)\n\nThis will return a new tibble with only the columns experiment_group, trust_t1, and trust_t2.\nNote that here we did not assign the result to anything. So in this case R will just print the result to the console, but not store it in a variable.\n\n\nSelecting and renaming columns\nWhen you select a column, you can also rename it using the syntax new_name = old_name. The following code selects the columns experiment_group, trust_t1, and trust_t2, and renames them to group, trust_before, and trust_after:\n\nselect(d, group = experiment_group, \n          trust_before = trust_t1, \n          trust_after = trust_t2)\n\n\n\nSelecting columns that have spaces in the name\nSometimes columns names have spaces in them. This is a bit annoying to work with in R, because you need to then tell R where a name starts and ends. You can do this by using backticks (reverse quotes) around the column name. In our practice data, we need this to select the news consumption column. It is then often smart to immediately rename the column to something without spaces, such as just replacing them with underscores:\n\nselect(d, news_consumption = `news consumption`)\n\n\n\nDropping columns\nInstead of selecting which column to keep, you can also specify which columns to drop. You can do this by adding a minus sign in front of the column name. The following code drops the columns np_subscription and trust_t1:\n\nselect(d, -np_subscription, -trust_t1)\n\nThis will return a new tibble with all columns except np_subscription and trust_t1.\n\n\n\nRenaming columns with rename\nSometimes you only want to rename columns without selecting or dropping any. You can do this with the rename function, which works similarly to how you rename columns with select:\n\nrename(d, group = experiment_group, \n          trust_before = trust_t1, \n          trust_after = trust_t2)\n\nIn this case, we do rename the columns, but without dropping all the other columns.\n\n\n\n\n Back to top",
    "crumbs": [
      "Data management",
      "Select and rename"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html",
    "href": "getting-started/types-of-objects.html",
    "title": "Types of Objects",
    "section": "",
    "text": "You can work with many types of data in R. Here are some of the most common types of objects you’ll encounter:\n\nNumeric: Numbers, like 5, 3.14, or -0.5.\nCharacter: Text, like \"Hello, world!\".\nFactor: Categorical data, like education_level or country.\n\nLet’s see how these types of objects work in practice.\n\nnumber &lt;- 5\nnumber\n\ncharacter &lt;- \"Hello, world!\"\ncharacter\n\nfactor &lt;- factor(c(1,2,2,3), labels = c(\"A\", \"B\", \"C\"))\nfactor\n\n\n\nThe type of an object determines what you can do with it. For example, you can perform mathematical operations on numeric objects, but not on character objects.\n\n10 + 10    # returns 20\n\"10\" + 10  # throws an error\n\n\n\n\nSometimes you have an object of the wrong type. For instance, your numeric data might have been read in as a character object.\n\nnumber &lt;- \"5\"    \n\nIf I want to perform mathematical operations on number, I need to first convert it to a numeric object. You can do this using the as.numeric function.\n\nnumber &lt;- as.numeric(number)\nnumber\nclass(number)    # numeric\n\nNote that you cannot always convert objects to a different type. Just use your common sense here.\n\nas.numeric(\"I am not a number\") \n\nWhen coercion is not possible, R will return NA (missing value) and give you a warning.",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#types-determine-what-you-can-do-with-an-object",
    "href": "getting-started/types-of-objects.html#types-determine-what-you-can-do-with-an-object",
    "title": "Types of Objects",
    "section": "",
    "text": "The type of an object determines what you can do with it. For example, you can perform mathematical operations on numeric objects, but not on character objects.\n\n10 + 10    # returns 20\n\"10\" + 10  # throws an error",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#you-can-coerce-objects-to-different-types",
    "href": "getting-started/types-of-objects.html#you-can-coerce-objects-to-different-types",
    "title": "Types of Objects",
    "section": "",
    "text": "Sometimes you have an object of the wrong type. For instance, your numeric data might have been read in as a character object.\n\nnumber &lt;- \"5\"    \n\nIf I want to perform mathematical operations on number, I need to first convert it to a numeric object. You can do this using the as.numeric function.\n\nnumber &lt;- as.numeric(number)\nnumber\nclass(number)    # numeric\n\nNote that you cannot always convert objects to a different type. Just use your common sense here.\n\nas.numeric(\"I am not a number\") \n\nWhen coercion is not possible, R will return NA (missing value) and give you a warning.",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#from-single-values-to-vectors",
    "href": "getting-started/types-of-objects.html#from-single-values-to-vectors",
    "title": "Types of Objects",
    "section": "From Single Values to Vectors",
    "text": "From Single Values to Vectors\nA vector is a collection of values of the same type (e.g., all numeric or all character). You create a vector by combining individual values using the c() (combine) function. Just like with single numbers, a vector has a type.\n\nnumbers &lt;- c(1, 2, 3, 4, 5)\nclass(numbers)\n\n[1] \"numeric\"\n\n\nNow, you can perform operations on the entire vector at once:\n\nnumbers * 10\n\n[1] 10 20 30 40 50\n\nsum(numbers)\n\n[1] 15",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#from-vectors-to-data-frames",
    "href": "getting-started/types-of-objects.html#from-vectors-to-data-frames",
    "title": "Types of Objects",
    "section": "From Vectors to Data Frames",
    "text": "From Vectors to Data Frames\nSingle vectors are still not very usefull for data analysis, since we’re often interested in relations between variables. This is where data frames come in.\nA data frame is a table with rows and columns, where each row is an observation and each column is a variable. You can think of a data frame as a collection of vectors, where each vector represents a column in the dataset. We can create a data frame by combining vectors using the data.frame() function.1\n\ncountry &lt;- c(\"NL\", \"NL\", \"BE\", \"BE\", \"DE\", \"DE\", \"FR\", \"FR\", \"UK\", \"UK\")\nheight  &lt;- c(176 , 165 , 172 , 160 , 180 , 170 , 175 , 165 , 185 , 175 )\n\nd &lt;- data.frame(country, height)\n\nNow, you can perform operations on the entire data frame at once! You can for instance perform a statistical test to see if the average height of people differs between countries. Just like with single values and vectors, we need to take the type of each column into account. For instance, we can’t perform a correlation analysis using the country column, because it’s a character vector.",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/types-of-objects.html#footnotes",
    "href": "getting-started/types-of-objects.html#footnotes",
    "title": "Types of Objects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThroughout this book we will actually be using the tibble data structure from the tidyverse package, which is an improved version of the data.frame.↩︎",
    "crumbs": [
      "Getting Started",
      "Types of Objects"
    ]
  },
  {
    "objectID": "getting-started/file-management.html",
    "href": "getting-started/file-management.html",
    "title": "File management",
    "section": "",
    "text": "If you’re working with data you need to be able to keep track of where you store it, especially if it concerns sensitive data such as personal information. In this section we’ll first explain and demonstrate a simple, recommended approach to managing files in R. Secondly, we provide some optional material that goes more in-depth on how your computer’s file system works, and we provide some additional techniques that give you more flexibility in managing files. This second part is optional, and if you’re just starting out with R we recommend skipping it for now.",
    "crumbs": [
      "Getting Started",
      "File management"
    ]
  },
  {
    "objectID": "getting-started/file-management.html#writing-a-file-to-your-working-directory",
    "href": "getting-started/file-management.html#writing-a-file-to-your-working-directory",
    "title": "File management",
    "section": "Writing a file to your working directory",
    "text": "Writing a file to your working directory\nThe following code writes a plain text file name hello_world.txt to your current working directory.\n\nwriteLines(\"Hello world!\", \"hello_world.txt\")\n\nIf you now open your file explorer and navigate to your working directory, you should see the file hello_world.txt there. Note that there is also a file explorer inside of RStudio, under the Files tab in the bottom-right window. If you click on the gear (settings) icon there, you can click Go To Working Directory.\nYou can also ask R to list all the files in your working directory.\n\nlist.files()                    ## show all files\nlist.files(pattern=\"*.txt\")     ## show files with .txt extension",
    "crumbs": [
      "Getting Started",
      "File management"
    ]
  },
  {
    "objectID": "getting-started/file-management.html#reading-a-file-from-your-working-directory",
    "href": "getting-started/file-management.html#reading-a-file-from-your-working-directory",
    "title": "File management",
    "section": "Reading a file from your working directory",
    "text": "Reading a file from your working directory\nReading files works the same way. Since there is now a file hello_world.txt in your working directory, you can read it with the following code:\n\nreadLines(\"hello_world.txt\")\n\n\n\n\n\n\n\nDirectories inside your working directory\n\n\n\n\n\nIn this tutorial we only show you how to work with files directly in your working directory, but you can also access directories. For example, if you have a directory called data inside your working directory, with a file called survey.csv in it, you can access it via the relative path data/survey.csv. This is very convenient in bigger projects, where you might want to organize things like data, results and visualizations in different directories.",
    "crumbs": [
      "Getting Started",
      "File management"
    ]
  },
  {
    "objectID": "getting-started/file-management.html#choosing-your-working-directory",
    "href": "getting-started/file-management.html#choosing-your-working-directory",
    "title": "File management",
    "section": "Choosing your working directory",
    "text": "Choosing your working directory\nWe highly recommend to always set your working directory to a specific location on your computer. For instance, in a directory called R_projects, or even a specific directory for your current project or assignment.\nIf you are working in a regular R script, you can set the working directory with the setwd() function. For example, to set the working directory to /home/you/R_projects, you can run setwd(\"/home/you/R_projects\"). If you prefer using a menu, you can also use RStudio: click on Session in the menu bar, then Set Working Directory, and then Choose Directory.\nIf you are working in a Quarto file, (as we explain here), the working directory is always the directory where the Quarto file is located (and you cannot change it). This is convenient, because it means that you just need to put the Quarto file and your data together in the directory that you want to work in.\nFinally, you can also create an RStudio project. In the top-right corner of your RStudio window, you can see a button labeled Project: (None). If you click it, you can create a new project, in a new or existing directory on your computer. RStudio will automatically set the working directory to this project directory, and in the top-right corner you will now see the name of the project instead of Project: (None).",
    "crumbs": [
      "Getting Started",
      "File management"
    ]
  },
  {
    "objectID": "getting-started/file-management.html#finding-the-absolute-path",
    "href": "getting-started/file-management.html#finding-the-absolute-path",
    "title": "File management",
    "section": "Finding the absolute path",
    "text": "Finding the absolute path\nSo how do you find the absolute path to the file? Here are several options.\n\nUsing file.choose()\nIf you run the code file.choose(), R will open a file explorer window.\n\nfile.choose()              ## find path of existing file\nfile.choose(new = TRUE)    ## create a path for a new file\n\nHere you can browse to the file, and when you select it, R will print the file path in the console window. This is easy, because you get a nice menu to browse through your files. But it’s also a bit cumbersome, because you have to do it every time you want to read a file. Also, you can’t select folder, only existing files.\n\n\nUsing the file explorer\nYou can also use your own file explorer (outside of RStudio). Navigate to a file or folder, and right click on it. You should then be able to select something like Properties (this depends on your operating system). Here you should be able to find the absolute path.\n\n\nUsing tab completion\nThere is one other nice trick that you can use to find files on your computer: tab completion. Whenever you are writing something in a code editor, you can often use the Tab key (above caps lock) to automatically complete it (like auto-complete on your phone). If you’ve ever seen programmers work really fast, it’s because they’re using tab completion all the time.\nTo use tab completion for file paths, put your text cursor between the quotes in readLines(\"\") (or any function for reading/writing files), and then press the Tab key. If there are multiple files that match the characters you’ve typed so far, RStudio will show you all the options. Keep typing to narrow down the options, and once you see the file or directory you want, press tab again to complete it. This takes some time to get used to, but it’s a very powerful trick.",
    "crumbs": [
      "Getting Started",
      "File management"
    ]
  },
  {
    "objectID": "data-management/the-tidyverse.html",
    "href": "data-management/the-tidyverse.html",
    "title": "The tidyverse",
    "section": "",
    "text": "Installing the tidyverse\nThe tidyverse is collection of R packages that makes it much easier to import, manage and visualize data. To use the tidyverse, you only need to open the tidyverse package, and it will automatically open all of the tidyverse R packages.\nLike any normal package, you need to first install it once:\n\ninstall.packages('tidyverse')\n\nThen in every script that you use the package, you open it with library:\n\nlibrary(tidyverse)\n\nWhen you first run this code, it will print a message that shows you all the packages that it opened for you. Some of the most important ones that we’ll we using are:\n\ntibble. An optimized way for structuring rectangular data (basically: a spreadsheet of rows and columns)\ndplyr. Functions for manipulating tibbles: select and rename columns, filter rows, mutate values, etc.\nreadr. Read data into R.\nggplot2. One of the best visualization tools out there. Check out the gallery\n\n\n\n\n\n\n\nWhat about the ‘Conflicts’?\n\n\n\n\n\nWhen opening the tidyverse, and when opening packages in general, you can get a Conflicts warning. A very common warning for the tidyverse is:\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nSo what does this mean, and should we be worried?\nSince anyone can write new packages for R, it can happen that two packages provide functions with the same name. In this example, we see that the filter function exists in both the dplyr package (which we opened by opening the tidyverse), and in the stats package (which is included in base R). So now R needs to decide which version of the function to use when you type filter(). In this case, it says that the dplyr::filter() masks stats::filter(), meaning that it will now use the dplyr version.\nIn practice, this will rarely be a problem, because you seldom need two versions of a function in the same script. But if you ever do, there is a simple solution. Instead of just using filter(), you can type dplyr::filter() to specifically use this version. In the following code, we use this notation to specifically open the help page for dplyr::filter and stats::filter.\n\n?dplyr::filter()\n?stats::filter()\n\n\n\n\n\n\n\n\n\n\nThe tidyverse versus base R\n\n\n\n\n\nMany of the things that the tidyverse allows you to do are also possible in base R (i.e. the basic installation of R). Base R also provides functions for importing, managing and visualizing data. So why do we need the tidyverse?\nThe tidyverse is an opinionated framework, which means that it doesn’t just enable you to do things, but also suggests how you should do things. The authors have thought long and hard about how to make data management easy, effective and intuitive (they have even written papers about it). This not only makes the tidyverse much easier and intuitive to learn, but also makes sure everyone writes their code in the same way, which improves transparency and shareability.\nThis is different from base R, which is designed to be a highly flexible programming language, that allows you to do almost anything. Accordingly, it is still worthwhile to learn base R at some point if you want to specialize more in computational research methods. But for our Communication Science program, and for many data science applications in general, you can do all your data management in the tidyverse.\n\n\n\n\n\nData management with the tidyverse\nThe tidyverse is built around the concept of tidy data (Wickham 2014). The main principles of tidy data are:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nThis type of data is also called a data frame, or a spreadsheet. What the tidyverse does is provide a set of tools that make it easy to work with this type of data. At the core of this is the tibble data structure. As a simple example, the following code creates a tibble containing respondents, their gender, and their height. We’ll call our tibble d (short for data).\n\nd &lt;- tibble(resp_id = c(  1,   2,   3,   4), \n            gender  = c(\"M\", \"M\", \"F\", \"F\"), \n            height  = c(176, 165, 172, 160))\n\nThe name d is now a tibble with 4 rows and 3 columns. Like any name in R, we can print it to see what it looks like:\n\nd\n\n# A tibble: 4 × 3\n  resp_id gender height\n    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1       1 M         176\n2       2 M         165\n3       3 F         172\n4       4 F         160\n\n\nThe vast majority of data that we work with in the social sciences can be structured in this way. The rows typically represent our units of analysis (e.g., respondents, participants, texts, etc.), and the columns represent the variables that we measure on these units. This makes it imperative for us to learn how we can manage this type of data effectively. We need to be able to select columns, filter rows, mutate values, and summarize data. Sometimes we also need to pivot the data, or join it with other data. In this chapter you will learn how to do all of this with the tidyverse.\n\n\n\n\n\n Back to topReferences\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.",
    "crumbs": [
      "Data management",
      "The tidyverse"
    ]
  },
  {
    "objectID": "data-management/pipes.html",
    "href": "data-management/pipes.html",
    "title": "Using the pipe syntax",
    "section": "",
    "text": "In the Functions tutorial we already mentioned that R also has a pipe syntax. This is a way to chain functions together, where the output of one function is the input of the next function. The syntax for the pipe is |&gt;, and it is used like this:\n\nargument1 |&gt; function(argument2)\n\nThe first argument of a funtion can be piped into it. Between the parentheses of the function we then only need to specify any additional arguments (if needed). Often the first argument of a function is the input data, which allows you to chain functions together.\n\ndata |&gt; \n   do_this() |&gt; \n   then_this() |&gt;\n   finally_this()\n\n\n\nThe tidyverse is designed to work really well with pipes. All of the functions for working with a tibble (like select, filter, arrange, etc.) have the first argument as the tibble itself, and the output is also a tibble. This means that you can chain these functions together to create a single pipeline for cleaning and preparing your data.\nFor example, the following code reads a csv file, selects columns, filters rows and finally arranges the data:\n\nlibrary(tidyverse)\n\npractice_data = read_csv(\"https://tinyurl.com/R-practice-data\")\npractice_data = select(practice_data, age, experiment_group, trust_t1)\npractice_data = filter(practice_data, age &gt;= 18)\npractice_data = arrange(practice_data, trust_t1)\n\nWith pipes, we can write the same code in a more readable way:\n\npractice_data = read_csv(\"https://tinyurl.com/R-practice-data\") %&gt;%\n  select(age, experiment_group, trust_t1) %&gt;%\n  filter(age &gt;= 18) %&gt;%\n  arrange(trust_t1)\n\n\n\n\n\n\n\nThe alternative pipe symbol %&gt;%\n\n\n\n\n\nThere is another pipe symbol in R: %&gt;%. In this book we will always use |&gt;, but it’s good to know about the existence of %&gt;%, because you might encounter it in other resources.\nBoth functions work almost in the same way. So why have two? The reason is simply that R keeps evolving, and the |&gt; pipe was only recently introduced.\nThe %&gt;% pipe was introduced in the magrittr package, and was made popular by the tidyverse. Because of this popularity, R decided that it would be a good idea to have a native pipe in the language itself, meaning that you don’t need to install a package to use it. This is why they introduced the |&gt; pipe.",
    "crumbs": [
      "Data management",
      "Using the pipe syntax"
    ]
  },
  {
    "objectID": "data-management/pipes.html#using-pipes-with-the-tidyverse",
    "href": "data-management/pipes.html#using-pipes-with-the-tidyverse",
    "title": "Using the pipe syntax",
    "section": "",
    "text": "The tidyverse is designed to work really well with pipes. All of the functions for working with a tibble (like select, filter, arrange, etc.) have the first argument as the tibble itself, and the output is also a tibble. This means that you can chain these functions together to create a single pipeline for cleaning and preparing your data.\nFor example, the following code reads a csv file, selects columns, filters rows and finally arranges the data:\n\nlibrary(tidyverse)\n\npractice_data = read_csv(\"https://tinyurl.com/R-practice-data\")\npractice_data = select(practice_data, age, experiment_group, trust_t1)\npractice_data = filter(practice_data, age &gt;= 18)\npractice_data = arrange(practice_data, trust_t1)\n\nWith pipes, we can write the same code in a more readable way:\n\npractice_data = read_csv(\"https://tinyurl.com/R-practice-data\") %&gt;%\n  select(age, experiment_group, trust_t1) %&gt;%\n  filter(age &gt;= 18) %&gt;%\n  arrange(trust_t1)\n\n\n\n\n\n\n\nThe alternative pipe symbol %&gt;%\n\n\n\n\n\nThere is another pipe symbol in R: %&gt;%. In this book we will always use |&gt;, but it’s good to know about the existence of %&gt;%, because you might encounter it in other resources.\nBoth functions work almost in the same way. So why have two? The reason is simply that R keeps evolving, and the |&gt; pipe was only recently introduced.\nThe %&gt;% pipe was introduced in the magrittr package, and was made popular by the tidyverse. Because of this popularity, R decided that it would be a good idea to have a native pipe in the language itself, meaning that you don’t need to install a package to use it. This is why they introduced the |&gt; pipe.",
    "crumbs": [
      "Data management",
      "Using the pipe syntax"
    ]
  },
  {
    "objectID": "getting-started/working-in-quarto.html",
    "href": "getting-started/working-in-quarto.html",
    "title": "Working in Quarto",
    "section": "",
    "text": "The classic way of working in R is to write your code in a script file. This is still a very common way of working, and it has many advantages. But today there are also great alternatives, that make it possible to integrate your code directly in your reporting.\nIn fact, the book that you are currently reading is written in one such alternatives, called Quarto. Quarto is a literate programming tool, which means that it allows you to write your code and your text in the same document. Any results and visualizations that you generate in your code can also be directly included in the output document. This is not only convenient, but also makes your analysis more transparent and reproducible. If you ever need to update your analysis, you can just rerun the code and the document will automatically update, including any tables, figures and statistial results that you generated.\nAnd despite all those features, it really isn’t that different from writing an R script. In some ways it is even a more convenient way of working.\nIn some courses in this program, you will be asked to write your assignments in Quarto. This section therefore provides a brief introduction to working in Quarto.\n\n\nYou can create and work with Quarto directly in RStudio. For assignments, you will often be given a template Quarto file, but here we’ll create one from scratch. To create a new Quarto file in RStudio, go to the menu bar and select File &gt; New File &gt; Quarto document.\nYou’ll get a menu where you can select some options, such as whether you want the output to be created in HTML, PDF or Word. We’ll stick with the default HTML, because this doesn’t require any additional software to be installed. For now, the only thing you should change is the Title. Just name it something like My First Quarto Document, and click Create.\nThe new file will open in the editor, and you’ll see that it already contains some text and code, that also doubles as a quick tutorial. Roughly speaking, there are three general components.\n\n\nAt the top of the file there is a header, which is enclosed by three dashes ---. Here you can specify some general settings for the document, such as the title, the author, and the output format. The default is:\n---\ntitle: \"My First Quarto Document\"\nformat: html\neditor: visual\n---\nThis means that the title of the document is “My First Quarto Document”, the output format is HTML, and the editor is the visual editor. You can add many more options to the header, but for now we’ll stick with the basics.\n\n\n\n\n\n\nVisual or Source editor\n\n\n\n\n\nThe Quarto editor can be source or visual. In visual mode, everything looks a bit more pretty. It doesn’t yet look as pretty as when you render the document (which we’ll show you in a bit), but it’s a bit easier to read. Also, in the toolbar you get some buttons to make text bold or italics, add codeblocks, insert images, create tables, etc.\nIn source mode, everything is just in plain text. Behind the scenes, Quarto uses a format called Markdown to format the text. Markdown is a type of markup language (like HTML). What this means is that any formatting that you do (e.g., bold, tables, lists) is done by writing the text in a specific way. For example, to make a word bold, you write it between two asterisks **like this**.\nWorking in source mode can be convenient if you’re already familiar with Markdown. If not, we recommend sticking with visual mode for now, and maybe forever.\n\n\n\n\n\n\nAfter the header, you can start writing your text. This works like a light version of Word. You can add heading, lists, tables, images, and more.\n\n\n\nAt some point you see a code chunk, which looks like:\n\n{r}\n1 + 1\n\nInside these code chunks you can write R code. When you render the document, the code will be executed, and the output will be shown in the document.\nWhile working on the document you can also run the code chunks separately (i.e. without rendering). You can do this like you would do in an R script, by selectin the code you want to run and hitting Run (or Ctrl/Cmd + Enter). But there is also a convenient option to run the entire code chunk at once by clicking on the run current chunk button in the top right of the code chunk. Here you also find a button to run all the chunks above the current one.\nTo add a new block, you can click on the Insert button in the toolbar, and select Executable Cell -&gt; R.\n\n\n\n\nWhen you’re done writing your document, or if you want to see a preview, you can render the document by clicking the Render button in the toolbar (or hitting Ctrl/Cmd + Shift + K). Try it now! If you did not yet save your Quarto file, R will first ask you to save it. You should then give it a name with the .qmd (Quarto Markdown) extension, for example my-first-quarto-document.qmd. This should produce an HTML file that opens in your browser (unless you changed something and broke it).\nThere is one big challenge when working with Quarto, which is also a blessing in disguise. If there is ANY error in your code, the entire document will not render. This can be frustrating, especially if you need to hand in an assignment in a few minutes. So whenever you’re working with Quarty, make sure to frequently render your document while you’re working on it, to catch any errors early on. It is much easier to trace back an error if you only made a few changes since the last time you rendered the document. If you only render the document at the end, you might have to go through the entire document to find the error. (though the error messages will tell you where to look)\nSo why is this a blessing in disguise? It forces you to write your code in a way that is clean and well-structured. When working in an R script, a common mistake to make is that some pieces of code depend on code that is further down in the script. For example, if you use a certain column on line 10, but you only create that column on line 20. This might not pose a problem while you’re working on it, but it will bite you when for whatever reason you need to rerun the entire script. Quarto forces you to write your code in a way that if you run the entire script from scratch, it will work.",
    "crumbs": [
      "Getting Started",
      "Working in Quarto"
    ]
  },
  {
    "objectID": "getting-started/working-in-quarto.html#creating-a-quarto-file",
    "href": "getting-started/working-in-quarto.html#creating-a-quarto-file",
    "title": "Working in Quarto",
    "section": "",
    "text": "You can create and work with Quarto directly in RStudio. For assignments, you will often be given a template Quarto file, but here we’ll create one from scratch. To create a new Quarto file in RStudio, go to the menu bar and select File &gt; New File &gt; Quarto document.\nYou’ll get a menu where you can select some options, such as whether you want the output to be created in HTML, PDF or Word. We’ll stick with the default HTML, because this doesn’t require any additional software to be installed. For now, the only thing you should change is the Title. Just name it something like My First Quarto Document, and click Create.\nThe new file will open in the editor, and you’ll see that it already contains some text and code, that also doubles as a quick tutorial. Roughly speaking, there are three general components.\n\n\nAt the top of the file there is a header, which is enclosed by three dashes ---. Here you can specify some general settings for the document, such as the title, the author, and the output format. The default is:\n---\ntitle: \"My First Quarto Document\"\nformat: html\neditor: visual\n---\nThis means that the title of the document is “My First Quarto Document”, the output format is HTML, and the editor is the visual editor. You can add many more options to the header, but for now we’ll stick with the basics.\n\n\n\n\n\n\nVisual or Source editor\n\n\n\n\n\nThe Quarto editor can be source or visual. In visual mode, everything looks a bit more pretty. It doesn’t yet look as pretty as when you render the document (which we’ll show you in a bit), but it’s a bit easier to read. Also, in the toolbar you get some buttons to make text bold or italics, add codeblocks, insert images, create tables, etc.\nIn source mode, everything is just in plain text. Behind the scenes, Quarto uses a format called Markdown to format the text. Markdown is a type of markup language (like HTML). What this means is that any formatting that you do (e.g., bold, tables, lists) is done by writing the text in a specific way. For example, to make a word bold, you write it between two asterisks **like this**.\nWorking in source mode can be convenient if you’re already familiar with Markdown. If not, we recommend sticking with visual mode for now, and maybe forever.\n\n\n\n\n\n\nAfter the header, you can start writing your text. This works like a light version of Word. You can add heading, lists, tables, images, and more.\n\n\n\nAt some point you see a code chunk, which looks like:\n\n{r}\n1 + 1\n\nInside these code chunks you can write R code. When you render the document, the code will be executed, and the output will be shown in the document.\nWhile working on the document you can also run the code chunks separately (i.e. without rendering). You can do this like you would do in an R script, by selectin the code you want to run and hitting Run (or Ctrl/Cmd + Enter). But there is also a convenient option to run the entire code chunk at once by clicking on the run current chunk button in the top right of the code chunk. Here you also find a button to run all the chunks above the current one.\nTo add a new block, you can click on the Insert button in the toolbar, and select Executable Cell -&gt; R.",
    "crumbs": [
      "Getting Started",
      "Working in Quarto"
    ]
  },
  {
    "objectID": "getting-started/working-in-quarto.html#rendering-the-document",
    "href": "getting-started/working-in-quarto.html#rendering-the-document",
    "title": "Working in Quarto",
    "section": "",
    "text": "When you’re done writing your document, or if you want to see a preview, you can render the document by clicking the Render button in the toolbar (or hitting Ctrl/Cmd + Shift + K). Try it now! If you did not yet save your Quarto file, R will first ask you to save it. You should then give it a name with the .qmd (Quarto Markdown) extension, for example my-first-quarto-document.qmd. This should produce an HTML file that opens in your browser (unless you changed something and broke it).\nThere is one big challenge when working with Quarto, which is also a blessing in disguise. If there is ANY error in your code, the entire document will not render. This can be frustrating, especially if you need to hand in an assignment in a few minutes. So whenever you’re working with Quarty, make sure to frequently render your document while you’re working on it, to catch any errors early on. It is much easier to trace back an error if you only made a few changes since the last time you rendered the document. If you only render the document at the end, you might have to go through the entire document to find the error. (though the error messages will tell you where to look)\nSo why is this a blessing in disguise? It forces you to write your code in a way that is clean and well-structured. When working in an R script, a common mistake to make is that some pieces of code depend on code that is further down in the script. For example, if you use a certain column on line 10, but you only create that column on line 20. This might not pose a problem while you’re working on it, but it will bite you when for whatever reason you need to rerun the entire script. Quarto forces you to write your code in a way that if you run the entire script from scratch, it will work.",
    "crumbs": [
      "Getting Started",
      "Working in Quarto"
    ]
  },
  {
    "objectID": "getting-started/using-rstudio.html",
    "href": "getting-started/using-rstudio.html",
    "title": "How to use RStudio",
    "section": "",
    "text": "Once you have installed R and RStudio, you can start by launching RStudio. If everything was installed correctly, RStudio will automatically launch R as well.\nThe first time you open RStudio, you will likely see three separate windows. The first thing you want to do is open an R Script (!!) to work in. To do so, go to the toolbar and select File -&gt; New File -&gt; R Script.\nYou will now see four windows split evenly over the four corners of your screen:\nWhile you can directly enter code into your console (bottom-left), you should always work with R scripts (top-left). This allows you to keep track of what you are doing and save every step.",
    "crumbs": [
      "Getting Started",
      "How to use RStudio"
    ]
  },
  {
    "objectID": "getting-started/using-rstudio.html#running-code-from-the-r-script",
    "href": "getting-started/using-rstudio.html#running-code-from-the-r-script",
    "title": "How to use RStudio",
    "section": "Running code from the R script",
    "text": "Running code from the R script\nCopy and paste the following example code into your R Script. For now, don’t bother understanding the syntax itself. Just focus on running it.\n\n3 + 3\n2 * 5\n6 / 2\n\"some text\"\n\"some more text\"\nsum(1,2,3,4,5)\n\nYou can run code by selecting the code and clicking on the Run button in the toolbar. However, we highly recommend getting used to using the keyboard shortcut, because this will greatly speed up your process. On Windows (and Linux) the shortcut is Ctrl + Enter. On Mac it’s Command + Enter.\nThere are two ways to run code:\n\nIf you select a specific piece of code (so that it is highlighted) you can run this specific selection. For example, select the first three lines (the three mathematical operations) and press Ctrl + Enter. This should then print the results for these three mathematical expressions. Note that you can also select a specific part on a line. Try selecting just the second 3 on the first line. This should just print the number 3.\nIf you haven’t made a selection, but your text cursor is somewhere on a line in your editor, you can press Ctrl + Enter to run the line where the cursor is at. This will also move the cursor to the next line, so you can walk through the code from top to bottom, running each line. Try starting on the first line, and pressing Ctrl + Enter six times, to run each line separately.",
    "crumbs": [
      "Getting Started",
      "How to use RStudio"
    ]
  },
  {
    "objectID": "getting-started/using-rstudio.html#using-rstudio-projects",
    "href": "getting-started/using-rstudio.html#using-rstudio-projects",
    "title": "How to use RStudio",
    "section": "Using RStudio projects",
    "text": "Using RStudio projects\nIt is best to put all your code in an RStudio project. This is essentially a folder on your computer in which you can store the R files and data for a project that you are working on. While you do not necessarily need a project to work with R, they are very convenient, and we strongly recommend using them.\nTo create a new project, go to the top-right corner of your RStudio window. Look for the button labeled Project: (None). Click on this button, and select New Project. Follow the instructions to create a new directory with a new project. Name the project “R introduction”.\nNow, open a new R script and immediately save it (select File -&gt; Save in the toolbar, or press ctrl-s). Name the file my_first_r_script.r. In the bottom-right corner, under the Files tab, you’ll now see the file added to the project. The extension .r indicates that the file is an R script.",
    "crumbs": [
      "Getting Started",
      "How to use RStudio"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "This is the home page of the Communication Science R Book. We are currently developing this book along with the methods courses of the program. If you were directed to specific chapters by your instructor, these chapters will be ready for you to read.\n\nGetting started\nData management\nAnalysis\nGood to know\n\n\n\n\n Back to top"
  },
  {
    "objectID": "getting-started/index.html",
    "href": "getting-started/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "In the VU Communication Science track, you’ll be using the R statistical software as a key tool for learning quantitative methods and data analysis techniques. R is a powerful, open-source programming language that is widely used in academia and industry for statistical analysis, visualization, text analysis, and more.\nThrough our courses, you’ll get hands-on experience with R, allowing you to analyze real-world data, perform statistical tests, create insightful visualizations, and develop reproducible research workflows. These skills will be invaluable as you move forward in your studies and future career, where data-driven decision-making plays a crucial role.\nR is also open-source, and therefore free to use for both academic and commercial purposes. This means you can continue to use R beyond your studies without any licensing costs, giving you the flexibility to apply your skills in various professional settings. The open-source nature of R also fosters a vibrant community of users and contributors who regularly share packages, tutorials, and resources. This community support makes it easier to find help, learn new techniques, and stay updated with the latest developments in data science and statistical analysis.\nIn this Getting started section, we’ll walk you through your first steps. We will cover how to install R, how to install and use the RStudio software for working with R, and the basics of the R syntax.\n\n\n\n Back to top",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "analysis/techniques/index.html",
    "href": "analysis/techniques/index.html",
    "title": "Techniques",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Analysis",
      "Techniques"
    ]
  },
  {
    "objectID": "analysis/concepts/index.html",
    "href": "analysis/concepts/index.html",
    "title": "Key concepts",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Analysis",
      "Key concepts"
    ]
  },
  {
    "objectID": "analysis/concepts/hypotheses.html",
    "href": "analysis/concepts/hypotheses.html",
    "title": "Hypotheses",
    "section": "",
    "text": "In order to use statistical methods to test theories, we first need to derive hypotheses from them. A hypothesis is a testable statement, that when proven true or false, provides evidence for or against the theory.\nFor example, consider the theory that playing violent video games leads to increased aggression in children. There is much disagreement about whether this theory is correct (Griffiths 1999), so it’s important to test it empirically. In order to do so, we need to derive hypotheses that we can conduct to rigorous statistical tests. Some examples of hypotheses are:\n\nThere is a relation “between arcade game use and teachers’ ratings of impulsiveness and aggresiveness”. (Lin and Lepper 1987, 81)\nPlaying violent video games leads to higher levels of heart rate and blood pressure compared to playing non-violent video games. (Lynch 1994)\nThere will be a “linear increase in aggressive affect after playing nonaggressive, moderately aggresive, and highly aggressive games”. (Scott 1995, 125)\n\nNotice that these hypotheses are more specific than the theory they are derived from. There are multiple ways to define and measure “playing violent video games” and “increased aggression”, and it could also be that the relation only holds for certain groups of people, or only for short periods of time. Hypotheses help us make (different aspects of) the theory more explicit, so that we can conduct empirical tests.\n\n\nIf an hypothesis is confirmed, it provides evidence in support of the theory. However, note that it does not necessarily prove the theory.\nFor example, if we find that children who play violent video games are more aggressive, it does not yet prove that the games caused the aggression, because there are other possible explanations for finding this relation. Perhaps people who are already aggressive are more likely to play violent video games, or there could be a confounding variable that causes both aggression and video game playing (e.g., gender, socioeconomic status, or parental involvement).\nHypotheses are thus the bridge between theory and statistical methods, and are essential for conducting rigorous research in communication science. They need to be specific enough to be testable, but close enough to the theory they are derived from to allow for meaningful conclusions.",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Hypotheses"
    ]
  },
  {
    "objectID": "analysis/concepts/hypotheses.html#evidence-neq-proof",
    "href": "analysis/concepts/hypotheses.html#evidence-neq-proof",
    "title": "Hypotheses",
    "section": "",
    "text": "If an hypothesis is confirmed, it provides evidence in support of the theory. However, note that it does not necessarily prove the theory.\nFor example, if we find that children who play violent video games are more aggressive, it does not yet prove that the games caused the aggression, because there are other possible explanations for finding this relation. Perhaps people who are already aggressive are more likely to play violent video games, or there could be a confounding variable that causes both aggression and video game playing (e.g., gender, socioeconomic status, or parental involvement).\nHypotheses are thus the bridge between theory and statistical methods, and are essential for conducting rigorous research in communication science. They need to be specific enough to be testable, but close enough to the theory they are derived from to allow for meaningful conclusions.",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Hypotheses"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "analysis/tests/index.html",
    "href": "analysis/tests/index.html",
    "title": "Statistical tests",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Analysis",
      "Statistical tests"
    ]
  },
  {
    "objectID": "getting-started/install-r-and-rstudio.html",
    "href": "getting-started/install-r-and-rstudio.html",
    "title": "Install R and RStudio",
    "section": "",
    "text": "To work with R, you will need to install two pieces of software.\nBoth programs can be downloaded for free, and are available for all main operating systems (Windows, macOS and Linux).",
    "crumbs": [
      "Getting Started",
      "Install R and RStudio"
    ]
  },
  {
    "objectID": "getting-started/install-r-and-rstudio.html#installing-r",
    "href": "getting-started/install-r-and-rstudio.html#installing-r",
    "title": "Install R and RStudio",
    "section": "Installing R",
    "text": "Installing R\nTo install R, you can download it from the CRAN (comprehensive R Archive Network) website. Do not be alarmed by the website’s 90’s aesthetics. The website is legit.",
    "crumbs": [
      "Getting Started",
      "Install R and RStudio"
    ]
  },
  {
    "objectID": "getting-started/install-r-and-rstudio.html#installing-rstudio",
    "href": "getting-started/install-r-and-rstudio.html#installing-rstudio",
    "title": "Install R and RStudio",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nRStudio can be downloaded from the posit.co website, which is the developer of RStudio. Make sure to pick the latest version available for your operating system.",
    "crumbs": [
      "Getting Started",
      "Install R and RStudio"
    ]
  },
  {
    "objectID": "analysis/concepts/causality.html",
    "href": "analysis/concepts/causality.html",
    "title": "Causality",
    "section": "",
    "text": "xkcd: correlation",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Causality"
    ]
  },
  {
    "objectID": "analysis/concepts/causality.html#visualizing-causation",
    "href": "analysis/concepts/causality.html#visualizing-causation",
    "title": "Causality",
    "section": "Visualizing causation",
    "text": "Visualizing causation\nTo develop a good intuition for the difference between correlation and causation, it helps to have a good mental model of what a causal relationship looks like. A good way to do this is to use Directed Acyclic Graphs (DAGs), which are a visual way to represent causal relationships. Simply put, these are graphs where the arrows indicate the direction of the causal relationship. Here are three DAGs that show different reasons for a correlation between X and Y. X represents what we believe to be the cause (e.g., screen time), and Y the effect (e.g., well-being). Z represents a confounding variable (e.g., parental involvement).\n\n\n\n\n\n\n\n\n\nThis shows why a correlation between X and Y is not enough to establish causation. Each of these DAGs shows a different reason for the correlation between X and Y, but only the first one supports our hypothesis that X causes Y. Our job as researchers is to figure out which of these DAGs is best supported by empirical evidence.",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Causality"
    ]
  },
  {
    "objectID": "analysis/concepts/causality.html#theoretical-foundation",
    "href": "analysis/concepts/causality.html#theoretical-foundation",
    "title": "Causality",
    "section": "Theoretical foundation",
    "text": "Theoretical foundation\nThe most important thing is to have a theoretical foundation for a causal relationship. This means that we have a good reason to believe that that a change in one variable will lead to a change in the other.\nIdeally, this theory should be based on a priori reasoning, meaning that you develop your theory before you collect data. This avoids the risks of post hoc (Latin for “after this”) reasoning, which is also known as HARKing (Hypothesizing After the Results are Known). The prime risk of post hoc reasoning is that you end up shaping a theory to fit the data, rather than using theory to guide your data collection and analysis. Many scientific journals therefore require you to pre-register your hypotheses before you start collecting data. This doesn’t mean that there is no room for exploration in your data, but it does mean that you should be transparent about what you were looking for based on a priori reasoning, and what you learned from post hoc reasoning about your data.\nTheory also helps us to identify possible confounding variables. Consider for example the gender gap in chess. Women are underrepresented in chess, with especially few women at the top levels (Chabris and Glickman 2006). A naive interpretation would be that this is because, as grandmaster Garry Kasparov said in 1989: “Men are hardwired to be better chess players than women”. To properly understand where this gap comes from, we need to build theory that takes alternative explanations into account. If you have time for a tangent, try reading this Slate Magazine article and see if you can identify some of the confounding variables that the author mentions.",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Causality"
    ]
  },
  {
    "objectID": "analysis/concepts/causality.html#if-possible-conduct-an-experiment",
    "href": "analysis/concepts/causality.html#if-possible-conduct-an-experiment",
    "title": "Causality",
    "section": "If possible, conduct an experiment",
    "text": "If possible, conduct an experiment\nThe gold standard for establishing causation is the randomized controlled experiment, in which the researcher manipulates the cause and observes the effect, while keeping all other variables constant. In order to manipulate the cause, the researcher creates different experimental conditions. The classic example comes from the field of medicine: you give some participants a medicine you want to test, where others get a placebo (i.e. a fake medicine that only looks the same). The people that got the real medicine are then the treatment group, and the people that got the placebo are the control group To test if the medicine works, you compare the treatment group to the control group.1\nThe experimental design helps to adress both confounding and directionality.\n\nBy randomly assigning people to the treatment or control group, the people in both groups are statistically identical. If the randomization is done correctly, any possible confounding variables are equally distributed between the two groups, and so any difference in the outcome can be attributed to the treatment.\nBy comparing the treatment group to the control group, you can account for confounding variables that have to do with the process of appyling the treatment. A well known example of this is the placebo effect, where people feel better just because they think they’re taking a medicine (control group), and not because of the actual medicine (treatment group). If the medicine works, the treatment group should see a stronger effect than the control group.\n\nThis ability to eliminate the influence of confounders makes experiments a powerful tool for establishing causation. However, in the field of communication science it is often not possible (or ethical) to conduct valid experiments. For many of the causes that we are interested in, it is simply not possible to manipulate it in a controlled setting that is still sufficiently similar to the real world.",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Causality"
    ]
  },
  {
    "objectID": "analysis/concepts/causality.html#for-observational-data-control-for-confounders",
    "href": "analysis/concepts/causality.html#for-observational-data-control-for-confounders",
    "title": "Causality",
    "section": "For observational data, control for confounders",
    "text": "For observational data, control for confounders\nIf we cannot conduct an experiment, we can to some extend control for confounding variables using statistical methods. This is not as powerful as an experiment, but it does allow us to establish some level of evidence for causation from observational data. In fact, many well supported causal theories in communication science have only been supported this way, such as the relationship between media coverage and public opinion (Mccombs and Shaw 1972). And even well established causal relations like “smoking causes cancer” rely heavily on evidence from observational studies. Given that many communication science theories cannot be tested with experiments, being able to gather evidence for causal relations from observational data is an essential part of the communication scientist’s toolkit.\nThe trick behind statistically controlling for confounders lies in multivariate analysis (i.e. analyzing more than two variables at the same time). If we are worried that a correlation between a dependent variable and independent variable is due to a third variable, we can add this confounding variable to the analysis. Using techniques like multiple regression, we can then measure the effect of the independent variable on the dependent variable while holding the confounding variable constant. We also call this controlling for the confounder.\nThe main limitation of this approach is that we can only control for the variables that we have measured. This puts an even greater burden on having a solid theoretical foundation for the relationships we test.",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Causality"
    ]
  },
  {
    "objectID": "analysis/concepts/causality.html#triangulation",
    "href": "analysis/concepts/causality.html#triangulation",
    "title": "Causality",
    "section": "Triangulation",
    "text": "Triangulation\nIt is generally the case that we cannot determine causality from a single study, and need to build a body of evidence from multiple studies that point in the same direction. An experiment might be able to establish causality, but only for a very specific context and short term effects. An observational study might be able to find correlations that are consistent across many different contexts and time periods, but it cannot establish causality. To build theory on complicated issues such as the relationship between screen time and well-being, we need to combine both types of studies.\nFor example, in their study on screen time and well-being, Twenge and Campbell presented valuable data on the correlation between these variables, but emphasized that based on their data they cannot determine the direction of the relationship (Twenge and Campbell 2018, 281). So to build a stronger case for the causal relationship, they also discussed literature that looked at this from other angles. One study they mention is an experiment that showed that people that took a one week break from Facebook showed higher well-being than people that continued using Facebook (Tromholt 2016). By itself this experiment does not prove that the problem is screen time, but if multiple similar experiments show that manipulating different forms of screen time has a positive effect on well-being, it strengthens the argument that the correlation between screen time and well-being is due to a causal relationship.",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Causality"
    ]
  },
  {
    "objectID": "analysis/concepts/causality.html#footnotes",
    "href": "analysis/concepts/causality.html#footnotes",
    "title": "Causality",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA similar example from communication science could be that you want to test the effictiveness of a new persuasion strategy. You could then show the treatment group a persuasive message that uses the new strategy, and the control group a message that doesn’t.↩︎",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Causality"
    ]
  },
  {
    "objectID": "data-management/index.html",
    "href": "data-management/index.html",
    "title": "Data management",
    "section": "",
    "text": "In this chapter we will introduce you to the basics of data management in R, using the powerful tidyverse framework. Data management is the process of importing, cleaning, transforming, and exporting data. This is an essential part of any data analysis project, where you will often have to deal with data that is messy, incomplete, or in the wrong format. Knowing your way around data management will save you a lot of time and frustration, and opens up new possibilities for analysis and visualization.\nOne of the greatest benefits of learning how to manage data in R is that it allows you to work with data in a structured and reproducible way. All the steps from importing the raw data to analyzing the final results are written in your script. If you made a mistake anywhere in the process, you can easily go back to fix it, and then rerun the script from the start. It is also common that you will need to rerun your analysis at a later time, for example when you get new data, or when someone (e.g., a client, a reviewer) asks you to make some changes. By the end of this chapter, you will be able to manage your data in a way that is transparent, reproducible, and efficient.",
    "crumbs": [
      "Data management"
    ]
  },
  {
    "objectID": "data-management/index.html#description-of-the-data",
    "href": "data-management/index.html#description-of-the-data",
    "title": "Data management",
    "section": "Description of the data",
    "text": "Description of the data\nThe practice data describes a fictional study about how entertainment media might influence people’s attitudes. In this study, 600 participants were shown holywood movies that portray journalism in different ways, and we measured their trust in journalists before (trust_t1) and after (trust_t2) the experiment.1\nThere were three types of movies (experiment_group): a control group that watched a neutral movie, a group that watched a movie with a positive portrayal of journalism, and a group that watched a movie with a negative portrayal of journalism. Participants were randomly assigned to one of these groups, with balanced group sizes (i.e., 200 participants per group). We also collected some demographic information (age), asked whether they have a newspaper subscription (np_subscription), and how much hours of news they consume per week (news consumption).\nSince trust is a complex construct, we measured it with five items that participants rated on a scale from 1 to 10. These items are based on the questions proposed by Strömbäck et al. (2020), but with some modifications. For example, we inversed the scale for one of the items, so that higher values indicate lower trust. The items are:\n\nJournalists are fair when covering the news\nJournalists are unbiased when covering the news\n(inversed) Journalists do not tell the whole story when covering the news\nJournalists are accurate when covering the news\nJournalists separate facts from opinion when covering the news\n\n\nVariables\n\n\n\n\n\n\n\n\nVariable name\nType\nDescription\n\n\n\n\nid\nnumeric\nUnique identifier for each participant\n\n\nage\nnumeric\nAge of the participant in years\n\n\npolitical_orientation\ncharacter\nPolitical orientation of the participant (left, center, right)\n\n\nnp_subscription\nfactor\nWhether the participant has a newspaper subscription (yes, no)\n\n\nnews consumption\nnumeric\nAverage number of hours per week the participant consumes news\n\n\nexperiment_group\nfactor\nType of movie shown: neutral (control group), positive (positive portrayal of journalism), negative (negative portrayal of journalism)\n\n\ntrust_t1\nnumeric\nTrust in journalists before the experiment (range: 1-10)\n\n\ntrust_t2\nnumeric\nTrust in journalists after the experiment (range: 1-10)\n\n\ntrust_t1_item[1-5]\nnumeric\nScale items for trust_t1\n\n\ntrust_t2_item[1-5]\nnumeric\nScale items for trust_t2",
    "crumbs": [
      "Data management"
    ]
  },
  {
    "objectID": "data-management/index.html#reading-the-data-into-r",
    "href": "data-management/index.html#reading-the-data-into-r",
    "title": "Data management",
    "section": "Reading the data into R",
    "text": "Reading the data into R\nThe following code reads the practice data into R. We will explain this code in the next tutorials, and include it at the top of every tutorial that uses it. Note that in order to run this code, you need to have installed the tidyverse package.\n\nlibrary(tidyverse)\nd &lt;- read_csv('https://tinyurl.com/R-practice-data')\n\nThis is what the full data looks like:",
    "crumbs": [
      "Data management"
    ]
  },
  {
    "objectID": "data-management/index.html#footnotes",
    "href": "data-management/index.html#footnotes",
    "title": "Data management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe t1 in trust_t1 stands for “time 1”. This is a common notation in studies where you measure the same construct at different points in time. Here t1 and t2 refer to the time before and after the experiment, respectively.↩︎",
    "crumbs": [
      "Data management"
    ]
  },
  {
    "objectID": "getting-started/packages.html",
    "href": "getting-started/packages.html",
    "title": "Packages",
    "section": "",
    "text": "In R, a package is a collection of functions, data, and documentation that extends the capabilities of R. You can think of packages kind of like apps on your phone: they provide additional functionality that you can use to perform specific tasks. Also, like apps, you can install and uninstall packages as needed, directly from within R.\nThere are thousands of R packages available, which enables you to use almost any existing data analysis technique. R is not just a tool for statistical analysis, but also for data visualization, data collection, articifial intelligence, and much more. If there is anything you need to do, there is a good chance that someone has already written a package for it.",
    "crumbs": [
      "Getting Started",
      "Packages"
    ]
  },
  {
    "objectID": "getting-started/packages.html#install-a-package",
    "href": "getting-started/packages.html#install-a-package",
    "title": "Packages",
    "section": "1. Install a package",
    "text": "1. Install a package\nMost packages are available on the Comprehensive R Archive Network (CRAN), which is the main repository for R packages. To install these packages, all you need to know is their name.\nFor example, there is a package called lubridate that makes it easier to work with dates and times in your data. To install this package, you can use the install.packages() function:\n\ninstall.packages(\"lubridate\")\n\n\n\n\n\n\n\nWhat if R asks tot ‘install from source’?\n\n\n\n\n\nWhen running install.packages() You sometimes get the message that There is a binary version available but the source version is later (we’re mainly seen this on Mac). You then get the question whether you want to install from source the package which needs compilation (Yes/no). To answer this question, you have to type “yes” or “no” in your R console.\nIt is then usually best to say NO. This will install a slightly older version of the package, but it will be much faster and easier to install. You often don’t need the latest version, so it’s not worth the extra hassle.\nIn case you’re curious, the reason for this is that the newest version has not been prepared for your system yet. They do have the source code, but it has not yet been compiled into a binary version that is ready to use. Think of the source code as a recipe, and the binary version as the ready-made dish. If you really want to have the newest version you can say “yes”, but you’ll have to cook it yourself! The main problem is that you will often need to install some extra software to do this, which can be a hassle. So unless you really need the newest version, it’s usually best to say “no”, and just install the older version that has already been prepared for your system.",
    "crumbs": [
      "Getting Started",
      "Packages"
    ]
  },
  {
    "objectID": "getting-started/packages.html#load-a-package",
    "href": "getting-started/packages.html#load-a-package",
    "title": "Packages",
    "section": "2. Load a package",
    "text": "2. Load a package\nOnce you have installed a package, it is not yet loaded into your current R session. Similar to when you install a new app on your phone, you need to open it every time you want to use it.\nTo use a package in your current R session, you can load it with the library() function:\n\nlibrary(lubridate)",
    "crumbs": [
      "Getting Started",
      "Packages"
    ]
  },
  {
    "objectID": "analysis/tests/correlation.html",
    "href": "analysis/tests/correlation.html",
    "title": "Correlation test",
    "section": "",
    "text": "The correlation test can be used to see if there is a relationship between two numeric variables. For example, we might want to know if there is a relationship between the amount of time people spend on social media and their feelings of loneliness.\nThe relation is expressed as a measure (the correlation coefficient) on a scale from -1 to 1, that tells us how strong the relationship is, and whether it is positive or negative. We also get a p-value that tells us if the relationship is significant. In this tutorial we focus on conducting the test. For more details on the concept of correlation and how to interpret the correlation coefficient \\(r\\), see the covariance and correlation section.\nWhat the correlation test does not tell us is why the relationship exists. If we find that there is a correlation between social media use and loneliness, it does not mean that social media use causes loneliness. For more detail on this, see the causality section.",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Correlation test"
    ]
  },
  {
    "objectID": "analysis/tests/correlation.html#between-two-variables-cor.test",
    "href": "analysis/tests/correlation.html#between-two-variables-cor.test",
    "title": "Correlation test",
    "section": "Between two variables: cor.test",
    "text": "Between two variables: cor.test\nIn the practice data we have variables for the news_consumption of the participants, and their trust_t1 in the news media (measured before the experiment). These are both numeric variables, so we can use the correlation test to see if there is a relationship between them.\nTo perform the correlation test using the cor.test function, we pass the two variables to the function. Recall that we can use the $ symbol to access variables in a data frame. So here we access the news_consumption and trust_t1 variables from the d data frame.\n\ncor.test(d$news_consumption, d$trust_t1)\n\n\n    Pearson's product-moment correlation\n\ndata:  d$news_consumption and d$trust_t1\nt = 2.6273, df = 598, p-value = 0.008827\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.02700997 0.18528348\nsample estimates:\n      cor \n0.1068234 \n\n\nThe output gives us the correlation coefficient under the sample estimates, which in our case is \\(r = 0.107\\). This means that the correlation is positive (&gt; 0), but quite weak. The test also reports a p-value (0.009), which tells us that the correlation is significant at the \\(99\\%\\) (&lt; 0.01) confidence level. Also note that the degrees of freedom are reported as 598, which is the number of observations minus 2.\n\n\n\n\n\n\ncor.test using the formula notation\n\n\n\n\n\nIt is also possible to use the formula notation in the cor.test function. However, this looks a bit weird, because in the correlation tests there are no dependent variables, since we are just looking at the relationship. The convention for formulas in R is to use the ~ symbol to separate the dependent and independent variables (dependent ~ independent_1 + independent_2 + ...). For the correlation test, we would just omit the dependent variable. So for the example above, the code would look like this:\n\ncor.test(~ news_consumption + trust_t1, data = d)",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Correlation test"
    ]
  },
  {
    "objectID": "analysis/tests/correlation.html#between-more-variables-tab_corr",
    "href": "analysis/tests/correlation.html#between-more-variables-tab_corr",
    "title": "Correlation test",
    "section": "Between more variables: tab_corr",
    "text": "Between more variables: tab_corr\nIf we need to see the correlations between multiple variables, we can use the tab_corr function from the sjPlot package. For example, let’s look at the correlations between the 5 items used to calculate the trust_t1 scale. To use tab_corr, we can simply pass a data frame with the variables we want to use. Here we use the select function to select all columns from trust_t1_item1 to trust_t1_item5.\n\nlibrary(sjPlot)\nd |&gt;\n    select(trust_t1_item1:trust_t1_item5) |&gt;\n    tab_corr()\n\n\n\n\n \ntrust_t1_item1\ntrust_t1_item2\ntrust_t1_item3\ntrust_t1_item4\ntrust_t1_item5\n\n\ntrust_t1_item1\n \n0.268***\n-0.730***\n0.724***\n0.848***\n\n\ntrust_t1_item2\n0.268***\n \n-0.273***\n0.252***\n0.289***\n\n\ntrust_t1_item3\n-0.730***\n-0.273***\n \n-0.673***\n-0.764***\n\n\ntrust_t1_item4\n0.724***\n0.252***\n-0.673***\n \n0.753***\n\n\ntrust_t1_item5\n0.848***\n0.289***\n-0.764***\n0.753***\n \n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nThis is great for getting a quick overview of the relationships between multiple variables. If you need to get the exact p-values, you can also use tab_corr(p.numeric = TRUE).",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Correlation test"
    ]
  },
  {
    "objectID": "analysis/tests/correlation.html#pearson-spearman-and-kendall",
    "href": "analysis/tests/correlation.html#pearson-spearman-and-kendall",
    "title": "Correlation test",
    "section": "Pearson, Spearman, and Kendall",
    "text": "Pearson, Spearman, and Kendall\nBy default the cor.test function uses the Pearson correlation coefficient. This assumes that the relationship between the variables is linear, and that the variables are (roughly) normally distributed. If this is not the case, you might want to instead use Spearman’s \\(\\rho\\) or Kendall’s \\(\\tau\\).\nTo use these alternative methods, you just need to specify the method argument in the cor.test function, or the corr.method argument in the tab_corr function.\n\ncor.test(d$news_consumption, d$trust_t1, method = \"spearman\")\ncor.test(d$news_consumption, d$trust_t1, method = \"kendall\")\n\nd |&gt; select(trust_t1_item1:trust_t1_item5) |&gt; tab_corr(corr.method=\"spearman\")\nd |&gt; select(trust_t1_item1:trust_t1_item5) |&gt; tab_corr(corr.method=\"kendall\")",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Correlation test"
    ]
  },
  {
    "objectID": "analysis/tests/correlation.html#footnotes",
    "href": "analysis/tests/correlation.html#footnotes",
    "title": "Correlation test",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen using tab_corr, you can use the p.numeric = TRUE argument to get the p-values. The degrees of freedom is always the number of observations minus 2, but be carefull not to count missing (NA) values.↩︎",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Correlation test"
    ]
  },
  {
    "objectID": "data-management/read-and-write.html",
    "href": "data-management/read-and-write.html",
    "title": "Read and write",
    "section": "",
    "text": "R can read files from many types of file formats. Here we will focus on the csv format, which is one of the most common formats for storing and sharing rectangular data (i.e., data in rows and columns). Once you know how to read a CSV file, you can easily read other file formats as well (e.g., Excel, SPSS).\n\n\n\n\n\n\n\n\nWhat is a CSV file?\n\n\n\n\n\nCSV stands for Comma Separated Values. It is a simple text file, that you can open in any text editor. In order to store a data frame (i.e. data in rows and colums), it simply read every line as a row, and separates the columns by a comma (or sometimes another symbol, like a semicolon).\nFor example, the following CSV file contains a data frame with three columns: resp_id, gender, and height. The first row contains the column names, and the following rows contain the data.\n\nresp_id,gender,height\n1,M,176\n2,M,165\n3,F,172\n4,F,160\n\nThe benefit of this simplicity is that any respectable spreadsheet or statistical software (e.g., Excel, Google sheets, SPSS, Stata) can read it. This makes CSV files a great way to share and store data.\nAnd just in case you’re worried, yes, CSV can also handle textual data. It uses some tricks to make sure that commas inside the text are not interpreted as column separators.\n\n\n\nTo show you how to work with CSV files, we’ll first import a dataset from the internet. Then we’ll show how to write this data to a CSV file on your computer, and how to read it back into R.\n\n\nTo read CSV files into R, you can use the read_csv from the tidyverse (more specifically from the readr package). If you provide a URL, it will download the file from the internet. Here we read the data and assign it to the name d (short for data). You can use any name you like, but since you’ll be referring to this data a lot, it’s convenient to keep it short.\n\nlibrary(tidyverse)\n\nd &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")\n\nMake sure to always check whether the data was imported correctly:\n\nd\n\nYou can also view the data in a larger spreadsheet-like view using the View function. Either click on the name (d) in the Environment tab in RStudio (top right panel), or use the View function:\n\nView(d)\n\nThis will open a new tab in RStudio that shows all the data. In the top menu bar you can also filter the data and search for specific values, or click on column names to sort the data.\n\n\n\nYou can use the write_csv function to write a tibble to a CSV file on your computer. If you just provide a file name, it will be saved in your current working directory.\n\nwrite_csv(d, \"practice_data.csv\")\n\n\n\n\nNow let’s read this file back into R. Since the file is in your working directory, you can just specify the file name:\n\nd2 &lt;- read_csv(\"practice_data.csv\")\n\nYou can check and verify that the data (d2) is indeed identical to the original data (d).\n\n\n\n\n\n\nCSV pitfalls to avoid\n\n\n\n\n\nThere are two important pitfalls to avoid when working with CSV files:\n\n\nWhen you download a CSV file from the internet, some computers might immediately ask you whether you want to open it in your default spreadsheet program (e.g., Excel, Numbers). Do not do this, but instead download the file directly to your computer. If you open the file and accidentally save it, it can overwrite the CSV file with a different format. Excel in particular has a habit of breaking CSV files this way.\n\n\n\nThere are different flavours of CSV files (for historic reasons). Even though we call them “comma separated values”, the separator is sometimes a semicolon or a tab. And depending on language, the decimal separator can be a comma or a dot. In particular, there are two most common versions of the CSV file. This is why tidyverse has two read_csv functions: read_csv and read_csv2. In general, you can just try read_csv first, and if it doesn’t work, try read_csv2.\n\n\n\n\n\n\n\n\nNow that you know how to read and write CSV files, reading other file formats is a piece of cake. It works almost the same way, but you just need to download a package that can read the file format.\nFor instance, to read an Excel file, you can use the readxl package, which provides the read_excel function. To read an SPSS file, you can use the haven package, which provides the read_sav function. You might have to take care of some additional details, such as the sheet name in the Excel file, or the variable labels in the SPSS file. But once you’ve got the hang of managing your data with the tidyverse, you’ll be able to handle any data frames formats that come your way.",
    "crumbs": [
      "Data management",
      "Read and write"
    ]
  },
  {
    "objectID": "data-management/read-and-write.html#csv-files",
    "href": "data-management/read-and-write.html#csv-files",
    "title": "Read and write",
    "section": "",
    "text": "What is a CSV file?\n\n\n\n\n\nCSV stands for Comma Separated Values. It is a simple text file, that you can open in any text editor. In order to store a data frame (i.e. data in rows and colums), it simply read every line as a row, and separates the columns by a comma (or sometimes another symbol, like a semicolon).\nFor example, the following CSV file contains a data frame with three columns: resp_id, gender, and height. The first row contains the column names, and the following rows contain the data.\n\nresp_id,gender,height\n1,M,176\n2,M,165\n3,F,172\n4,F,160\n\nThe benefit of this simplicity is that any respectable spreadsheet or statistical software (e.g., Excel, Google sheets, SPSS, Stata) can read it. This makes CSV files a great way to share and store data.\nAnd just in case you’re worried, yes, CSV can also handle textual data. It uses some tricks to make sure that commas inside the text are not interpreted as column separators.\n\n\n\nTo show you how to work with CSV files, we’ll first import a dataset from the internet. Then we’ll show how to write this data to a CSV file on your computer, and how to read it back into R.\n\n\nTo read CSV files into R, you can use the read_csv from the tidyverse (more specifically from the readr package). If you provide a URL, it will download the file from the internet. Here we read the data and assign it to the name d (short for data). You can use any name you like, but since you’ll be referring to this data a lot, it’s convenient to keep it short.\n\nlibrary(tidyverse)\n\nd &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")\n\nMake sure to always check whether the data was imported correctly:\n\nd\n\nYou can also view the data in a larger spreadsheet-like view using the View function. Either click on the name (d) in the Environment tab in RStudio (top right panel), or use the View function:\n\nView(d)\n\nThis will open a new tab in RStudio that shows all the data. In the top menu bar you can also filter the data and search for specific values, or click on column names to sort the data.\n\n\n\nYou can use the write_csv function to write a tibble to a CSV file on your computer. If you just provide a file name, it will be saved in your current working directory.\n\nwrite_csv(d, \"practice_data.csv\")\n\n\n\n\nNow let’s read this file back into R. Since the file is in your working directory, you can just specify the file name:\n\nd2 &lt;- read_csv(\"practice_data.csv\")\n\nYou can check and verify that the data (d2) is indeed identical to the original data (d).\n\n\n\n\n\n\nCSV pitfalls to avoid\n\n\n\n\n\nThere are two important pitfalls to avoid when working with CSV files:\n\n\nWhen you download a CSV file from the internet, some computers might immediately ask you whether you want to open it in your default spreadsheet program (e.g., Excel, Numbers). Do not do this, but instead download the file directly to your computer. If you open the file and accidentally save it, it can overwrite the CSV file with a different format. Excel in particular has a habit of breaking CSV files this way.\n\n\n\nThere are different flavours of CSV files (for historic reasons). Even though we call them “comma separated values”, the separator is sometimes a semicolon or a tab. And depending on language, the decimal separator can be a comma or a dot. In particular, there are two most common versions of the CSV file. This is why tidyverse has two read_csv functions: read_csv and read_csv2. In general, you can just try read_csv first, and if it doesn’t work, try read_csv2.",
    "crumbs": [
      "Data management",
      "Read and write"
    ]
  },
  {
    "objectID": "data-management/read-and-write.html#reading-other-file-formats-like-excel-and-spss",
    "href": "data-management/read-and-write.html#reading-other-file-formats-like-excel-and-spss",
    "title": "Read and write",
    "section": "",
    "text": "Now that you know how to read and write CSV files, reading other file formats is a piece of cake. It works almost the same way, but you just need to download a package that can read the file format.\nFor instance, to read an Excel file, you can use the readxl package, which provides the read_excel function. To read an SPSS file, you can use the haven package, which provides the read_sav function. You might have to take care of some additional details, such as the sheet name in the Excel file, or the variable labels in the SPSS file. But once you’ve got the hang of managing your data with the tidyverse, you’ll be able to handle any data frames formats that come your way.",
    "crumbs": [
      "Data management",
      "Read and write"
    ]
  },
  {
    "objectID": "getting-started/names-and-objects.html",
    "href": "getting-started/names-and-objects.html",
    "title": "Names and Objects",
    "section": "",
    "text": "In R, and in computer programming in general, the most essential operation is to assign objects to names. By object, we then broadly mean any piece of information. a single number, a text, a list of numbers, and even an entire data set.\nIn plain terms, assignment is how you make R remember things by assigning them to a name. To assign an object to a name, we use the arrow notation: name &lt;- value. For example:\n\nx &lt;- 2\n\n\n\n\n\n\n\nGood to know: you can also use x = 2\n\n\n\n\n\nInstead of using the arrow notation, you can also use the equal sign notation: name = object.\n\nx = 2\n\nWe will in general always use the arrow notation. But if you encounter the equal sign notation, just remember that it’s the same thing.\n\n\n\nBy running the code x &lt;- 2, you are saying: Assign the value 2 to the name x. Any objects that you assigned to names are stored in your Environment. You can see this environment in the top-right window, under the Environment tab. If you assigned 2 to x, you should see a table with in the left column the names (x) and in the right column a description of the object. For simply objects like numbers, this will just be the value (2).\nFrom hereon, when you use the name x in your code, it will refer to the value 2. So when we run the code x * 5 (x times 5) it will print the number 10\n\nx * 5\n\n[1] 10\n\n\n\n\n\n\n\n\nWhy does R print a [1] before the result?\n\n\n\n\n\nWhen running x * 5, R correctly prints the value 10. But why does it say [1] 10? The reason is that R always thinks of a number (or string) as a vector (i.e. list of values), that can have 1 or multiple values. The [1] indicates that 10 is the first (and only) value.\nIf you print a longer vector, you can see that R prints [...] at the start of each line, just to help you see the position of individual values. The following code generates a vector with numbers from 1 to 50\n\n1:50\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n\n\n\n\n\n\nNotice that when you ran the code x &lt;- 2, R didn’t print any values to the console (the bottom-left window). But when you ran x * 5, R did print the value 10. Basically, when you run code, and you DO NOT assign the result to a name, R will print the result to the console.\nSo the following code will NOT print the object (\"I will not be printed\") to the console, but will store it (you can see the name pop up in the Environment tab)\n\ni_will_be_remembered &lt;- \"I will not be printed\"\n\nAnd the following object (\"I will be printed\") will be printed to the console, but not stored in the Environment.\n\n\"I will be printed\"\n\n[1] \"I will be printed\"\n\n\n\n\n\nYou can assign any type of object to a name, and you can use any name, as long as it starts with a letter and doesn’t contain spaces or symbols (but underscores are OK)\n\na_number = 5\nmy_cats_name = \"Hobbes\"\n\nIf you run this code and check you Environment (top-right), you should now see these name-object pairs added.\n\n\n\nTill now we only directly assigned objects to names. This is convenient, but the power of assignment really shines when you use it to store results. For example, we can also do this.\n\nx = 5 + 10\n\nThis a very simple example, but just think for a second what this allows us to do. Since we can assign anything to a name, we can break down any complicated procedure into multiple steps! For now, the key lesson is just to wrap your head around the syntax for assigning objects to names. This is fundamental to everything you will be doing in R (and in programming in general).",
    "crumbs": [
      "Getting Started",
      "Names and Objects"
    ]
  },
  {
    "objectID": "getting-started/names-and-objects.html#assigning-versus-printing",
    "href": "getting-started/names-and-objects.html#assigning-versus-printing",
    "title": "Names and Objects",
    "section": "",
    "text": "Notice that when you ran the code x &lt;- 2, R didn’t print any values to the console (the bottom-left window). But when you ran x * 5, R did print the value 10. Basically, when you run code, and you DO NOT assign the result to a name, R will print the result to the console.\nSo the following code will NOT print the object (\"I will not be printed\") to the console, but will store it (you can see the name pop up in the Environment tab)\n\ni_will_be_remembered &lt;- \"I will not be printed\"\n\nAnd the following object (\"I will be printed\") will be printed to the console, but not stored in the Environment.\n\n\"I will be printed\"\n\n[1] \"I will be printed\"",
    "crumbs": [
      "Getting Started",
      "Names and Objects"
    ]
  },
  {
    "objectID": "getting-started/names-and-objects.html#assigning-different-types-of-objects",
    "href": "getting-started/names-and-objects.html#assigning-different-types-of-objects",
    "title": "Names and Objects",
    "section": "",
    "text": "You can assign any type of object to a name, and you can use any name, as long as it starts with a letter and doesn’t contain spaces or symbols (but underscores are OK)\n\na_number = 5\nmy_cats_name = \"Hobbes\"\n\nIf you run this code and check you Environment (top-right), you should now see these name-object pairs added.",
    "crumbs": [
      "Getting Started",
      "Names and Objects"
    ]
  },
  {
    "objectID": "getting-started/names-and-objects.html#assigning-results",
    "href": "getting-started/names-and-objects.html#assigning-results",
    "title": "Names and Objects",
    "section": "",
    "text": "Till now we only directly assigned objects to names. This is convenient, but the power of assignment really shines when you use it to store results. For example, we can also do this.\n\nx = 5 + 10\n\nThis a very simple example, but just think for a second what this allows us to do. Since we can assign anything to a name, we can break down any complicated procedure into multiple steps! For now, the key lesson is just to wrap your head around the syntax for assigning objects to names. This is fundamental to everything you will be doing in R (and in programming in general).",
    "crumbs": [
      "Getting Started",
      "Names and Objects"
    ]
  },
  {
    "objectID": "analysis/techniques/scale-construction.html",
    "href": "analysis/techniques/scale-construction.html",
    "title": "Scale construction",
    "section": "",
    "text": "A scale is a composite measure that combines multiple items into a single score. For example, in order to measure the complex construct “happiness”, you could ask people multiple questions related to happiness, such as “How often do you feel happy?”, “How satisfied are you with your life?”, and “How often do you feel joy?”.\nThe idea is that by combining multiple items into a single score, you can get a more reliable and valid measure of the underlying construct. If we would just ask people the single question “How happy are you?”, we might not get a very accurate measure of their happiness, because happiness is a complex and multi-faceted construct. By breaking it down into a multi-item scale, we can get a more nuanced and accurate measure.\nIf you’ve ever taken a survey, you’ve probably encountered scales before. For example, you might have been asked to rate your agreement with a series of statements on a scale from 1 to 5, where 1 means “strongly disagree” and 5 means “strongly agree”. This is also called a Likert scale, and it’s a common way to gather data on multiple items, with the goal of combining them into a single score for a complex construct.\nYou will also hear scales referred to as latent variables. The word latent means hidden or concealed, and it refers here to the fact that the construct we are trying to measure is not directly observable. We can only measure it propertly by looking at multiple observable indicators (items) that are related to the construct.\n\n\nTo create a scale, we combine multiple columns in your data (i.e. the variables for the scale items) into a single score. For instance, by taking the average of the values in these columns. However, before we can do that, we need to make sure that the scale is reliable and valid. This requires a few steps:\n\n\nFirst, you need to think carefully about which items to include in your scale, and this needs to be grounded in theory. There might also already be existing scales that you can use.\nFor example, in our practice data we have a construct called “trust in journalism”, which we measure with five items, based on the items proposed by Strömbäck et al. (2020). Participants were asked to rate their agreement with the following items on a scale from 1 to 10:\n\nJournalists are fair when covering the news\nJournalists are unbiased when covering the news\nJournalists do not tell the whole story when covering the news\nJournalists are accurate when covering the news\nJournalists separate facts from opinion when covering the news\n\nNote that item 3 is inversed, so that higher values indicate lower trust. Keep this in mind, because to create the scale we\n\n\n\nOnce you have collected your data, always check whether everything is in order. In the Inspect and clean chapter we looked at how to do this. Here we just use the dfSummary function from the summarytools package to get a quick overview of our data.\nFirst though, let’s load our data and select the columns that we’re interested in.\n\nlibrary(tidyverse)\nd = read_csv('https://tinyurl.com/R-practice-data')\n\nIn the practice data we have two scales: trust_t1 and trust_t2, with each having five items (trust_t1_item1 to trust_t1_item5 and trust_t2_item1 to trust_t2_item5). For this tutorial we’ll just focus on trust_t1. The following code selects the five items for trust_t1, and then uses the dfSummary function to get a summary of these columns.\n\nlibrary(summarytools)\n\nd |&gt; \n    select(trust_t1_item1:trust_t1_item5) |&gt; \n    dfSummary() |&gt;\n    view()\n\nThis looks good. There are no missing values, all values are within the expected range (1-10), and the distributions look reasonable.\n\n\n\nThe idea behind a scale is that the items are related to each other, because they all measure the same underlying construct. A good way to check this is by looking at the correlations between the items.\nFor this we’ll be using the sjPlot package, which has a function tab_corr (tabulate correlations) that creates a nice table with the correlations between all columns in a data frame. We again use this on the five items for trust_t1. In tab_corr we also set p.numeric=TRUE to show the p-values for the correlations as numbers (instead of stars).\n\nlibrary(sjPlot)\nd |&gt; \n    select(trust_t1_item1:trust_t1_item5) |&gt; \n    tab_corr(p.numeric=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n \ntrust_t1_item1\ntrust_t1_item2\ntrust_t1_item3\ntrust_t1_item4\ntrust_t1_item5\n\n\ntrust_t1_item1\n \n0.268\n(&lt;.001)\n-0.730\n(&lt;.001)\n0.724\n(&lt;.001)\n0.848\n(&lt;.001)\n\n\ntrust_t1_item2\n0.268\n(&lt;.001)\n \n-0.273\n(&lt;.001)\n0.252\n(&lt;.001)\n0.289\n(&lt;.001)\n\n\ntrust_t1_item3\n-0.730\n(&lt;.001)\n-0.273\n(&lt;.001)\n \n-0.673\n(&lt;.001)\n-0.764\n(&lt;.001)\n\n\ntrust_t1_item4\n0.724\n(&lt;.001)\n0.252\n(&lt;.001)\n-0.673\n(&lt;.001)\n \n0.753\n(&lt;.001)\n\n\ntrust_t1_item5\n0.848\n(&lt;.001)\n0.289\n(&lt;.001)\n-0.764\n(&lt;.001)\n0.753\n(&lt;.001)\n \n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nHere we see that the correlations between the items is mostly quite strong, and all significant at the 0.001 level. The only notable exception in terms of strength is that the correlations of item2 to the other items is much lower. This suggests that our items indeed measure are common underlying construct, but item2 (about bias) might measure a somewhat different aspect of trust in journalism.\nOne very important thing to notice is that the correlation of trust_t1_item3 with the other items is negative! So when the score on trust_t1_item3 goes up, the scores on the other items tend to go down. This makes complete sense if we remember that trust_t1_item3 is inversed, so that higher values indicate lower trust.\n\n\n\n\n\n\nFactor analysis\n\n\n\n\n\nAnother common way to check whether the items in your scale are related is by using factor analysis. This is a statistical technique that can help you identify the underlying factors that explain the correlations between the items. We’ll cover factor analysis in a later tutorial.\n\n\n\n\n\n\nIn the correlation analysis we saw that the third item (trust_t1_item3) is negatively correlated with the other items. This is all according to plan, since we inversed the scale for this item. But to create a single construct, we need to make sure that all items have the same directionality. So we need to invert the values for trust_t1_item3.\nSince we have a scale from 1 to 10, we can inverse the value by subtracking it from 11 (11 - 1 = 10, 11 - 2 = 9, …, 11 - 10 = 1).\n\nd &lt;- d |&gt; \n    mutate(trust_t1_item3_inv = 11 - trust_t1_item3)\n\nNotice that we do not overwrite the original column, but create a new column trust_t1_item3_inv (inversed). Overwriting the original column is possible, but dangerous and not transparent. Creating a new column prevents you from accidentally running the inversion multiple times, and messing up your analysis.\n\n\n\n\n\n\nGeneral formula for inversing\n\n\n\n\n\nSince we had a scale from 1 to 10, we could invert the values by subtracting from 11. Similarly, if you have a scale from 1 to 7, you could invert the values by subtracting from 8. So for any scale starting from 1, the formula you can use is:\n\\[ \\text{new value} = \\text{max value} + 1 - \\text{old value} \\]\nHowever, if your scale does not start from 1, this doesn’t work! (try it out for a scale from 0 to 10, and you’ll see why). The more general formula therefore is:\n\\[ \\text{new value} = \\text{max value} + \\text{min value} - \\text{old value} \\]\nNote that in this case you need to use the minimum and maximum possible values of your scale, and NOT the actual minimum and maximum values in your data! So if your scale goes from 1 to 7, you would use 1 and 7 in the formula, even if the minimum and maximum values in your data are 1.5 and 6.5.\n\n\n\n\n\n\nThe reliability of a scale is a measure of how consistent the items in the scale are. There are different ways to calculate reliability, but one of the most common is Cronbach’s alpha.\nCronbach’s alpha ranges from 0 to 1, where higher values indicate higher reliability. A common rule of thumb is that a value of 0.7 or higher is acceptable, but this can vary depending on the context. As with any rule of thumb, don’t blindly follow it, but think about what makes sense in your specific situation.\nTo calculate Cronbach’s alpha, we can use the psych package, which has a function alpha that calculates the alpha for the columns on an input data frame. So we’ll do the same thing as above, but note that this time our select statement looks different, because we need to include the inversed item.\n\nlibrary(psych)\n\nd |&gt; \n    select(trust_t1_item1,  \n           trust_t1_item2, \n           trust_t1_item3_inv, \n           trust_t1_item4, \n           trust_t1_item5) |&gt; \n    alpha()\n\n\nReliability analysis   \nCall: alpha(x = select(d, trust_t1_item1, trust_t1_item2, trust_t1_item3_inv, \n    trust_t1_item4, trust_t1_item5))\n\n  raw_alpha std.alpha G6(smc) average_r S/N  ase mean   sd median_r\n      0.85      0.86    0.87      0.56 6.3 0.01  3.9 0.99      0.7\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.83  0.85  0.87\nDuhachek  0.83  0.85  0.87\n\n Reliability if an item is dropped:\n                   raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r\ntrust_t1_item1          0.78      0.80    0.80      0.50  4.0   0.0153 0.0642\ntrust_t1_item2          0.92      0.92    0.90      0.75 11.9   0.0052 0.0034\ntrust_t1_item3_inv      0.79      0.81    0.82      0.52  4.4   0.0151 0.0785\ntrust_t1_item4          0.80      0.82    0.83      0.53  4.5   0.0143 0.0778\ntrust_t1_item5          0.77      0.79    0.78      0.49  3.8   0.0162 0.0598\n                   med.r\ntrust_t1_item1      0.48\ntrust_t1_item2      0.74\ntrust_t1_item3_inv  0.51\ntrust_t1_item4      0.51\ntrust_t1_item5      0.47\n\n Item statistics \n                     n raw.r std.r r.cor r.drop mean  sd\ntrust_t1_item1     600  0.88  0.89  0.89   0.80  4.1 1.1\ntrust_t1_item2     600  0.56  0.52  0.31   0.30  2.7 1.5\ntrust_t1_item3_inv 600  0.85  0.86  0.82   0.75  4.3 1.3\ntrust_t1_item4     600  0.83  0.85  0.80   0.74  3.7 1.1\ntrust_t1_item5     600  0.90  0.91  0.92   0.83  4.6 1.2\n\nNon missing response frequency for each item\n                      1    2    3    4    5    6    7 8 9 miss\ntrust_t1_item1     0.01 0.07 0.22 0.33 0.27 0.10 0.01 0 0    0\ntrust_t1_item2     0.26 0.23 0.22 0.14 0.10 0.03 0.00 0 0    0\ntrust_t1_item3_inv 0.01 0.06 0.20 0.26 0.28 0.13 0.05 0 0    0\ntrust_t1_item4     0.02 0.12 0.29 0.34 0.18 0.04 0.00 0 0    0\ntrust_t1_item5     0.00 0.03 0.15 0.25 0.32 0.18 0.06 0 0    0\n\n\n\n\n\n\n\n\nCronbach’s alpha with 3 digits\n\n\n\n\n\nBy default, alpha only shows two digits for Cronbach’s alpha. If you want to see more digits, you can use the print function with the digits argument.\n\nd |&gt; \n    select(trust_t1_item1, trust_t1_item2, trust_t1_item3_inv, trust_t1_item4, trust_t1_item5) |&gt; \n    alpha() |&gt;\n    print(digits=3)\n\n\nReliability analysis   \nCall: alpha(x = select(d, trust_t1_item1, trust_t1_item2, trust_t1_item3_inv, \n    trust_t1_item4, trust_t1_item5))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean    sd median_r\n     0.848     0.863   0.867     0.557 6.3 0.0104 3.89 0.995    0.699\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt    0.828 0.848 0.866\nDuhachek 0.828 0.848 0.868\n\n Reliability if an item is dropped:\n                   raw_alpha std.alpha G6(smc) average_r   S/N alpha se   var.r\ntrust_t1_item1         0.782     0.800   0.798     0.501  4.01  0.01532 0.06423\ntrust_t1_item2         0.921     0.923   0.904     0.749 11.92  0.00524 0.00336\ntrust_t1_item3_inv     0.791     0.814   0.823     0.522  4.38  0.01512 0.07847\ntrust_t1_item4         0.801     0.818   0.826     0.529  4.49  0.01427 0.07779\ntrust_t1_item5         0.770     0.791   0.784     0.487  3.79  0.01621 0.05985\n                   med.r\ntrust_t1_item1     0.481\ntrust_t1_item2     0.742\ntrust_t1_item3_inv 0.507\ntrust_t1_item4     0.510\ntrust_t1_item5     0.473\n\n Item statistics \n                     n raw.r std.r r.cor r.drop mean   sd\ntrust_t1_item1     600 0.875 0.888 0.886  0.799 4.10 1.15\ntrust_t1_item2     600 0.556 0.518 0.314  0.301 2.71 1.46\ntrust_t1_item3_inv 600 0.854 0.856 0.818  0.748 4.34 1.33\ntrust_t1_item4     600 0.829 0.847 0.804  0.736 3.67 1.09\ntrust_t1_item5     600 0.900 0.909 0.919  0.830 4.64 1.24\n\nNon missing response frequency for each item\n                       1     2     3     4     5     6     7     8     9 miss\ntrust_t1_item1     0.008 0.073 0.217 0.330 0.267 0.097 0.007 0.002 0.000    0\ntrust_t1_item2     0.265 0.228 0.222 0.145 0.102 0.033 0.005 0.000 0.000    0\ntrust_t1_item3_inv 0.013 0.062 0.197 0.265 0.283 0.127 0.048 0.003 0.002    0\ntrust_t1_item4     0.018 0.122 0.293 0.342 0.185 0.038 0.002 0.000 0.000    0\ntrust_t1_item5     0.005 0.032 0.152 0.248 0.323 0.180 0.058 0.000 0.002    0\n\n\nNote that this way of setting the nr of digits is specific to the psych package.\n\n\n\nThis gives quite a lot of output. These are the most important parts to consider:\n\nCronbach’s alpha: At the top we have a row that says raw_alpha, std.alpha, etc. Here we are just interested in the raw_alpha, which is the value of Cronbach’s alpha. In this case it’s 0.85, which is already very good.\nReliability if an item is dropped: This part shows you what would happen to the raw_alpha (and the other reliability measures) if you would drop one of the items from the scale. In our data we see that if item2 would be dropped, the raw_alpha would go up to 0.92 (from 0.85). In other words, if we would use a 4-item scale with item2 dropped, the scale would be more reliable.\n\nItem statistics: This part shows you some usefull statistics about the items, like the mean and standard deviation (sd). More importantly, it also shows several scores for item-total correlation (the r in raw.r, std.r, r.cor and r.drop stands for correlation). This indicate how strongly the item is correlated to the total scale (i.e. the combination of the other items). The recommended correlation measure to look at is the r.cor (correlation corrected for item overlap). In our data we see that item5 has the strongest correlation with the total scale, whereas item2 has the weakets. Notice how this is in line with the Reliability if an item is dropped part: if we would drop item2, the scale would become more reliable. (Think about why that makes sense!)\n\n\n\n\nIf the reliability of your scale is too low, you might want to consider removing some items (if you have enough items to spare). Above we saw that if we would drop item2, the reliability of the scale would go up to 0.92. You can verify that this is indeed what happens:\n\nd |&gt; \n    select(trust_t1_item1, \n           trust_t1_item3_inv, \n           trust_t1_item4, \n           trust_t1_item5) |&gt; \n    alpha()\n\nYou can verify that the raw_alpha is now 0.92. So now we can choose between either using the 4-item scale with an alpha of 0.92, or keeping the 5-item scale with an alpha of 0.85. This is a judgement call, and depends on the context of your research. You might also consider why an items is not working as well as the others (e.g. is it measuring a different aspect of the construct?, or might there be a problem with the item itself, such as confusing formulation?). Generally speaking, if you’re alpha is on the lower end (&lt; 0.7) and you have items to spare (&gt; 3), it’s a good idea to remove some items.\n\n\n\nFinally, once you have a reliable scale, you can calculate the scale score. This is usually done by taking the average of the items in the scale. The simplest way to do so is to just add up the items and divide by the number of items. Let’s do this for the 5-item scale (\\(\\alpha = 0.85\\)) that we tested above (mind the parentheses!):\n\nd &lt;- d |&gt; \n    mutate(trust_t1_scale = (trust_t1_item1 + trust_t1_item2 + trust_t1_item3_inv + trust_t1_item4 + trust_t1_item5) / 5)\n\nSo now we have a new column trust_t1_scale that contains the scale score.\nRemember that in the practice data we already had a column trust_t1 for the 5-item scale. So this is how that column was created. You can verify this by checking whether the new column is identical to the old one:\n\nidentical(d$trust_t1, d$trust_t1_scale)\n\n[1] TRUE",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Scale construction"
    ]
  },
  {
    "objectID": "analysis/techniques/scale-construction.html#create-scale",
    "href": "analysis/techniques/scale-construction.html#create-scale",
    "title": "Scale construction",
    "section": "",
    "text": "To create a scale, we combine multiple columns in your data (i.e. the variables for the scale items) into a single score. For instance, by taking the average of the values in these columns. However, before we can do that, we need to make sure that the scale is reliable and valid. This requires a few steps:\n\n\nFirst, you need to think carefully about which items to include in your scale, and this needs to be grounded in theory. There might also already be existing scales that you can use.\nFor example, in our practice data we have a construct called “trust in journalism”, which we measure with five items, based on the items proposed by Strömbäck et al. (2020). Participants were asked to rate their agreement with the following items on a scale from 1 to 10:\n\nJournalists are fair when covering the news\nJournalists are unbiased when covering the news\nJournalists do not tell the whole story when covering the news\nJournalists are accurate when covering the news\nJournalists separate facts from opinion when covering the news\n\nNote that item 3 is inversed, so that higher values indicate lower trust. Keep this in mind, because to create the scale we\n\n\n\nOnce you have collected your data, always check whether everything is in order. In the Inspect and clean chapter we looked at how to do this. Here we just use the dfSummary function from the summarytools package to get a quick overview of our data.\nFirst though, let’s load our data and select the columns that we’re interested in.\n\nlibrary(tidyverse)\nd = read_csv('https://tinyurl.com/R-practice-data')\n\nIn the practice data we have two scales: trust_t1 and trust_t2, with each having five items (trust_t1_item1 to trust_t1_item5 and trust_t2_item1 to trust_t2_item5). For this tutorial we’ll just focus on trust_t1. The following code selects the five items for trust_t1, and then uses the dfSummary function to get a summary of these columns.\n\nlibrary(summarytools)\n\nd |&gt; \n    select(trust_t1_item1:trust_t1_item5) |&gt; \n    dfSummary() |&gt;\n    view()\n\nThis looks good. There are no missing values, all values are within the expected range (1-10), and the distributions look reasonable.\n\n\n\nThe idea behind a scale is that the items are related to each other, because they all measure the same underlying construct. A good way to check this is by looking at the correlations between the items.\nFor this we’ll be using the sjPlot package, which has a function tab_corr (tabulate correlations) that creates a nice table with the correlations between all columns in a data frame. We again use this on the five items for trust_t1. In tab_corr we also set p.numeric=TRUE to show the p-values for the correlations as numbers (instead of stars).\n\nlibrary(sjPlot)\nd |&gt; \n    select(trust_t1_item1:trust_t1_item5) |&gt; \n    tab_corr(p.numeric=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n \ntrust_t1_item1\ntrust_t1_item2\ntrust_t1_item3\ntrust_t1_item4\ntrust_t1_item5\n\n\ntrust_t1_item1\n \n0.268\n(&lt;.001)\n-0.730\n(&lt;.001)\n0.724\n(&lt;.001)\n0.848\n(&lt;.001)\n\n\ntrust_t1_item2\n0.268\n(&lt;.001)\n \n-0.273\n(&lt;.001)\n0.252\n(&lt;.001)\n0.289\n(&lt;.001)\n\n\ntrust_t1_item3\n-0.730\n(&lt;.001)\n-0.273\n(&lt;.001)\n \n-0.673\n(&lt;.001)\n-0.764\n(&lt;.001)\n\n\ntrust_t1_item4\n0.724\n(&lt;.001)\n0.252\n(&lt;.001)\n-0.673\n(&lt;.001)\n \n0.753\n(&lt;.001)\n\n\ntrust_t1_item5\n0.848\n(&lt;.001)\n0.289\n(&lt;.001)\n-0.764\n(&lt;.001)\n0.753\n(&lt;.001)\n \n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nHere we see that the correlations between the items is mostly quite strong, and all significant at the 0.001 level. The only notable exception in terms of strength is that the correlations of item2 to the other items is much lower. This suggests that our items indeed measure are common underlying construct, but item2 (about bias) might measure a somewhat different aspect of trust in journalism.\nOne very important thing to notice is that the correlation of trust_t1_item3 with the other items is negative! So when the score on trust_t1_item3 goes up, the scores on the other items tend to go down. This makes complete sense if we remember that trust_t1_item3 is inversed, so that higher values indicate lower trust.\n\n\n\n\n\n\nFactor analysis\n\n\n\n\n\nAnother common way to check whether the items in your scale are related is by using factor analysis. This is a statistical technique that can help you identify the underlying factors that explain the correlations between the items. We’ll cover factor analysis in a later tutorial.\n\n\n\n\n\n\nIn the correlation analysis we saw that the third item (trust_t1_item3) is negatively correlated with the other items. This is all according to plan, since we inversed the scale for this item. But to create a single construct, we need to make sure that all items have the same directionality. So we need to invert the values for trust_t1_item3.\nSince we have a scale from 1 to 10, we can inverse the value by subtracking it from 11 (11 - 1 = 10, 11 - 2 = 9, …, 11 - 10 = 1).\n\nd &lt;- d |&gt; \n    mutate(trust_t1_item3_inv = 11 - trust_t1_item3)\n\nNotice that we do not overwrite the original column, but create a new column trust_t1_item3_inv (inversed). Overwriting the original column is possible, but dangerous and not transparent. Creating a new column prevents you from accidentally running the inversion multiple times, and messing up your analysis.\n\n\n\n\n\n\nGeneral formula for inversing\n\n\n\n\n\nSince we had a scale from 1 to 10, we could invert the values by subtracting from 11. Similarly, if you have a scale from 1 to 7, you could invert the values by subtracting from 8. So for any scale starting from 1, the formula you can use is:\n\\[ \\text{new value} = \\text{max value} + 1 - \\text{old value} \\]\nHowever, if your scale does not start from 1, this doesn’t work! (try it out for a scale from 0 to 10, and you’ll see why). The more general formula therefore is:\n\\[ \\text{new value} = \\text{max value} + \\text{min value} - \\text{old value} \\]\nNote that in this case you need to use the minimum and maximum possible values of your scale, and NOT the actual minimum and maximum values in your data! So if your scale goes from 1 to 7, you would use 1 and 7 in the formula, even if the minimum and maximum values in your data are 1.5 and 6.5.\n\n\n\n\n\n\nThe reliability of a scale is a measure of how consistent the items in the scale are. There are different ways to calculate reliability, but one of the most common is Cronbach’s alpha.\nCronbach’s alpha ranges from 0 to 1, where higher values indicate higher reliability. A common rule of thumb is that a value of 0.7 or higher is acceptable, but this can vary depending on the context. As with any rule of thumb, don’t blindly follow it, but think about what makes sense in your specific situation.\nTo calculate Cronbach’s alpha, we can use the psych package, which has a function alpha that calculates the alpha for the columns on an input data frame. So we’ll do the same thing as above, but note that this time our select statement looks different, because we need to include the inversed item.\n\nlibrary(psych)\n\nd |&gt; \n    select(trust_t1_item1,  \n           trust_t1_item2, \n           trust_t1_item3_inv, \n           trust_t1_item4, \n           trust_t1_item5) |&gt; \n    alpha()\n\n\nReliability analysis   \nCall: alpha(x = select(d, trust_t1_item1, trust_t1_item2, trust_t1_item3_inv, \n    trust_t1_item4, trust_t1_item5))\n\n  raw_alpha std.alpha G6(smc) average_r S/N  ase mean   sd median_r\n      0.85      0.86    0.87      0.56 6.3 0.01  3.9 0.99      0.7\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.83  0.85  0.87\nDuhachek  0.83  0.85  0.87\n\n Reliability if an item is dropped:\n                   raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r\ntrust_t1_item1          0.78      0.80    0.80      0.50  4.0   0.0153 0.0642\ntrust_t1_item2          0.92      0.92    0.90      0.75 11.9   0.0052 0.0034\ntrust_t1_item3_inv      0.79      0.81    0.82      0.52  4.4   0.0151 0.0785\ntrust_t1_item4          0.80      0.82    0.83      0.53  4.5   0.0143 0.0778\ntrust_t1_item5          0.77      0.79    0.78      0.49  3.8   0.0162 0.0598\n                   med.r\ntrust_t1_item1      0.48\ntrust_t1_item2      0.74\ntrust_t1_item3_inv  0.51\ntrust_t1_item4      0.51\ntrust_t1_item5      0.47\n\n Item statistics \n                     n raw.r std.r r.cor r.drop mean  sd\ntrust_t1_item1     600  0.88  0.89  0.89   0.80  4.1 1.1\ntrust_t1_item2     600  0.56  0.52  0.31   0.30  2.7 1.5\ntrust_t1_item3_inv 600  0.85  0.86  0.82   0.75  4.3 1.3\ntrust_t1_item4     600  0.83  0.85  0.80   0.74  3.7 1.1\ntrust_t1_item5     600  0.90  0.91  0.92   0.83  4.6 1.2\n\nNon missing response frequency for each item\n                      1    2    3    4    5    6    7 8 9 miss\ntrust_t1_item1     0.01 0.07 0.22 0.33 0.27 0.10 0.01 0 0    0\ntrust_t1_item2     0.26 0.23 0.22 0.14 0.10 0.03 0.00 0 0    0\ntrust_t1_item3_inv 0.01 0.06 0.20 0.26 0.28 0.13 0.05 0 0    0\ntrust_t1_item4     0.02 0.12 0.29 0.34 0.18 0.04 0.00 0 0    0\ntrust_t1_item5     0.00 0.03 0.15 0.25 0.32 0.18 0.06 0 0    0\n\n\n\n\n\n\n\n\nCronbach’s alpha with 3 digits\n\n\n\n\n\nBy default, alpha only shows two digits for Cronbach’s alpha. If you want to see more digits, you can use the print function with the digits argument.\n\nd |&gt; \n    select(trust_t1_item1, trust_t1_item2, trust_t1_item3_inv, trust_t1_item4, trust_t1_item5) |&gt; \n    alpha() |&gt;\n    print(digits=3)\n\n\nReliability analysis   \nCall: alpha(x = select(d, trust_t1_item1, trust_t1_item2, trust_t1_item3_inv, \n    trust_t1_item4, trust_t1_item5))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean    sd median_r\n     0.848     0.863   0.867     0.557 6.3 0.0104 3.89 0.995    0.699\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt    0.828 0.848 0.866\nDuhachek 0.828 0.848 0.868\n\n Reliability if an item is dropped:\n                   raw_alpha std.alpha G6(smc) average_r   S/N alpha se   var.r\ntrust_t1_item1         0.782     0.800   0.798     0.501  4.01  0.01532 0.06423\ntrust_t1_item2         0.921     0.923   0.904     0.749 11.92  0.00524 0.00336\ntrust_t1_item3_inv     0.791     0.814   0.823     0.522  4.38  0.01512 0.07847\ntrust_t1_item4         0.801     0.818   0.826     0.529  4.49  0.01427 0.07779\ntrust_t1_item5         0.770     0.791   0.784     0.487  3.79  0.01621 0.05985\n                   med.r\ntrust_t1_item1     0.481\ntrust_t1_item2     0.742\ntrust_t1_item3_inv 0.507\ntrust_t1_item4     0.510\ntrust_t1_item5     0.473\n\n Item statistics \n                     n raw.r std.r r.cor r.drop mean   sd\ntrust_t1_item1     600 0.875 0.888 0.886  0.799 4.10 1.15\ntrust_t1_item2     600 0.556 0.518 0.314  0.301 2.71 1.46\ntrust_t1_item3_inv 600 0.854 0.856 0.818  0.748 4.34 1.33\ntrust_t1_item4     600 0.829 0.847 0.804  0.736 3.67 1.09\ntrust_t1_item5     600 0.900 0.909 0.919  0.830 4.64 1.24\n\nNon missing response frequency for each item\n                       1     2     3     4     5     6     7     8     9 miss\ntrust_t1_item1     0.008 0.073 0.217 0.330 0.267 0.097 0.007 0.002 0.000    0\ntrust_t1_item2     0.265 0.228 0.222 0.145 0.102 0.033 0.005 0.000 0.000    0\ntrust_t1_item3_inv 0.013 0.062 0.197 0.265 0.283 0.127 0.048 0.003 0.002    0\ntrust_t1_item4     0.018 0.122 0.293 0.342 0.185 0.038 0.002 0.000 0.000    0\ntrust_t1_item5     0.005 0.032 0.152 0.248 0.323 0.180 0.058 0.000 0.002    0\n\n\nNote that this way of setting the nr of digits is specific to the psych package.\n\n\n\nThis gives quite a lot of output. These are the most important parts to consider:\n\nCronbach’s alpha: At the top we have a row that says raw_alpha, std.alpha, etc. Here we are just interested in the raw_alpha, which is the value of Cronbach’s alpha. In this case it’s 0.85, which is already very good.\nReliability if an item is dropped: This part shows you what would happen to the raw_alpha (and the other reliability measures) if you would drop one of the items from the scale. In our data we see that if item2 would be dropped, the raw_alpha would go up to 0.92 (from 0.85). In other words, if we would use a 4-item scale with item2 dropped, the scale would be more reliable.\n\nItem statistics: This part shows you some usefull statistics about the items, like the mean and standard deviation (sd). More importantly, it also shows several scores for item-total correlation (the r in raw.r, std.r, r.cor and r.drop stands for correlation). This indicate how strongly the item is correlated to the total scale (i.e. the combination of the other items). The recommended correlation measure to look at is the r.cor (correlation corrected for item overlap). In our data we see that item5 has the strongest correlation with the total scale, whereas item2 has the weakets. Notice how this is in line with the Reliability if an item is dropped part: if we would drop item2, the scale would become more reliable. (Think about why that makes sense!)\n\n\n\n\nIf the reliability of your scale is too low, you might want to consider removing some items (if you have enough items to spare). Above we saw that if we would drop item2, the reliability of the scale would go up to 0.92. You can verify that this is indeed what happens:\n\nd |&gt; \n    select(trust_t1_item1, \n           trust_t1_item3_inv, \n           trust_t1_item4, \n           trust_t1_item5) |&gt; \n    alpha()\n\nYou can verify that the raw_alpha is now 0.92. So now we can choose between either using the 4-item scale with an alpha of 0.92, or keeping the 5-item scale with an alpha of 0.85. This is a judgement call, and depends on the context of your research. You might also consider why an items is not working as well as the others (e.g. is it measuring a different aspect of the construct?, or might there be a problem with the item itself, such as confusing formulation?). Generally speaking, if you’re alpha is on the lower end (&lt; 0.7) and you have items to spare (&gt; 3), it’s a good idea to remove some items.\n\n\n\nFinally, once you have a reliable scale, you can calculate the scale score. This is usually done by taking the average of the items in the scale. The simplest way to do so is to just add up the items and divide by the number of items. Let’s do this for the 5-item scale (\\(\\alpha = 0.85\\)) that we tested above (mind the parentheses!):\n\nd &lt;- d |&gt; \n    mutate(trust_t1_scale = (trust_t1_item1 + trust_t1_item2 + trust_t1_item3_inv + trust_t1_item4 + trust_t1_item5) / 5)\n\nSo now we have a new column trust_t1_scale that contains the scale score.\nRemember that in the practice data we already had a column trust_t1 for the 5-item scale. So this is how that column was created. You can verify this by checking whether the new column is identical to the old one:\n\nidentical(d$trust_t1, d$trust_t1_scale)\n\n[1] TRUE",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Scale construction"
    ]
  },
  {
    "objectID": "data-management/mutate.html",
    "href": "data-management/mutate.html",
    "title": "Mutate and recode",
    "section": "",
    "text": "Required packages and data for this tutorial\n\n\n\nIn this tutorial we use the tidyverse package and the simulated practice data.\n\nlibrary(tidyverse)\nd &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")",
    "crumbs": [
      "Data management",
      "Mutate and recode"
    ]
  },
  {
    "objectID": "data-management/mutate.html#creating-new-variables",
    "href": "data-management/mutate.html#creating-new-variables",
    "title": "Mutate and recode",
    "section": "Creating new variables",
    "text": "Creating new variables\nTo create a new variable you use the following syntax:\n\nmutate(d, new_variable = expression)\n\nThe expression can be anything that returns a valid column. For example, in the practice data we have the columns trust_t1 and trust_t2, which represent trust in journalists before and after the experiment. We can create a new variable trust_change that represents the change in trust from before to after the experiment.\n\nd &lt;- mutate(d, trust_change = trust_t2 - trust_t1)\n\nselect(d, trust_t1, trust_t2, trust_change)",
    "crumbs": [
      "Data management",
      "Mutate and recode"
    ]
  },
  {
    "objectID": "data-management/mutate.html#mutate-existing-variables",
    "href": "data-management/mutate.html#mutate-existing-variables",
    "title": "Mutate and recode",
    "section": "Mutate existing variables",
    "text": "Mutate existing variables\nTo mutate an existing variable, you can simply overwrite the column with the same name. For example, let’s say that we want to standardize the trust_change variable that we just made. We can standardize a variable with the scale function, so we can use that inside of mutate.\n\nd &lt;- mutate(d, trust_change = scale(trust_change))\n\nNow the trust_change variable is standardized, which means that it has a mean of 0 and a standard deviation of 1. A nice way to get a quick overview of the distribution of a single variable is to plot a histogram.\n\nhist(d$trust_change)",
    "crumbs": [
      "Data management",
      "Mutate and recode"
    ]
  },
  {
    "objectID": "data-management/mutate.html#recode-with-case_match",
    "href": "data-management/mutate.html#recode-with-case_match",
    "title": "Mutate and recode",
    "section": "Recode with case_match",
    "text": "Recode with case_match\nThe case_match function is a simple way to recode specific values into new values. For example, in our practice data we have a column with the experimental groups, which are control, positive, and negative. Let’s say we want to clarify that positive means positive_movie, and negative means negative_movie. We could then use case_match to change these values.\n\nd &lt;- mutate(d, experiment_group = case_match(experiment_group,\n                                            \"positive\" ~ \"positive_movie\",\n                                            \"negative\" ~ \"negative_movie\",\n                                            .default = experiment_group))\n\nHere we say: overwrite the experiment_group column with output of the case_match function. Inside the case_match function, we specify three things:\n\nThe column we want to recode (experiment_group).\nThe conditions for recoding the values. We have two conditions:\n\nIf value is \"positive\", recode into \"positive_movie\".\nIf value is \"negative\", recode into \"negative_movie\".\n\nWe specify a .default value for values that are not matched in the conditions. Here we say that in that case we want to use the current value of the experiment_group column.\n\nIf you check the unique values of the experiment_group column, you will see that positive and negative have been changed to positive_movie and negative_movie, and that control remains the same.\n\nunique(d$experiment_group)\n\n[1] \"negative_movie\" \"control\"        \"positive_movie\"",
    "crumbs": [
      "Data management",
      "Mutate and recode"
    ]
  },
  {
    "objectID": "data-management/mutate.html#more-flexible-recoding-with-case_when",
    "href": "data-management/mutate.html#more-flexible-recoding-with-case_when",
    "title": "Mutate and recode",
    "section": "More flexible recoding with case_when",
    "text": "More flexible recoding with case_when\nThe case_match function is great if you need to recode many values, but sometimes you need more flexibility. For example, if we want to recode the age variable into categories (e.g., &lt;= 20, 20-30), it would be really tiresome to recode every individual age value. With the case_when function, we can specify the conditions using logical expressions. Each condition is evaluated in order, and the first one that is TRUE is used.\n\nd &lt;- mutate(d, age_category = case_when(\n  age &lt; 20 ~ \"&lt;= 20\",\n  age &lt; 30 ~ \"20-30\",\n  age &lt; 40 ~ \"30-40\",\n  age &lt; 50 ~ \"40-50\",\n  age &lt; 60 ~ \"50-60\",\n  .default = \"&gt;= 60\"\n))\n\ntable(d$age_category)\n\n\n&lt;= 20 &gt;= 60 20-30 30-40 40-50 50-60 \n   12    78   124   138   129   119",
    "crumbs": [
      "Data management",
      "Mutate and recode"
    ]
  },
  {
    "objectID": "data-management/mutate.html#binary-cases-with-if_else",
    "href": "data-management/mutate.html#binary-cases-with-if_else",
    "title": "Mutate and recode",
    "section": "Binary cases with if_else",
    "text": "Binary cases with if_else\nIf you only have two categories, you can use the if_else function. You could technically also use case_when for this, but if_else is more concise and easier to read. The syntax for if_else is:\n\nif_else(condition, value_if_true, value_if_false)\n\nA common use case is that sometimes you want to perform an operation only on a subset of the data. For example, in our data there are a few participants that accidentally entered their birthyear instead of their age. To correct this, we can use if_else to set the age to 2024 - birthyear, but only if the number the participants entered is above 1000 (which is only the case if it’s a birthyear).\n\nd &lt;- mutate(d, age = if_else(age &gt; 1000, 2024 - age, age))\n\nSo this reads: if the age is above 1000, return 2024 - age, otherwise return the current age.",
    "crumbs": [
      "Data management",
      "Mutate and recode"
    ]
  },
  {
    "objectID": "good-to-know/function-documentation.html",
    "href": "good-to-know/function-documentation.html",
    "title": "Function Documentation",
    "section": "",
    "text": "One of the things that can new users in R can find overwhelming, is that they think they need to learn all functions by heart. This is not the case! Aside from a handfull of functions that you will use all the time, you will often need to look up how to use a function. Rather than learning everything by heart, you therefore need to learn some tricks for how to quickly look up information about functions.\nOne of the most important tricks is to use the built-in help system in R. You can quickly access documentation for any function using the ? symbol. This is a powerful tool that can help you understand how to use functions, what arguments they require, and what they return.\n\n\nTo view the documentation for a specific function, you simply need to type ? followed by the function name. For example, if you want to learn more about the mean() function, you would type:\n\n?mean\n\nThis will open the help page, often in the bottom right pane of RStudio.\n\n\n\nThe help page for a function is divided into several sections. The most important sections are:\n\n\nA brief description of what the function does. For the mean() function, the description is: Generic Function for the (Arithmetic) Mean.\nBy generic function, they mean that the function can have multiple implementations. When you think of the mean, you are probably thinking of the mean of a vector of numbers.\n\nx = c(1,2,3,4)\nmean(x)\n\n[1] 2.5\n\n\nBut you can do more! For example, you can also calculate the mean of a vector of Date values:\n\ndates = as.Date(c(\"2021-01-01\", \"2021-01-03\"))\nmean(dates)\n\n[1] \"2021-01-02\"\n\n\n\n\n\nThe syntax of the function, including all the arguments it takes. For example, the mean() function has the following usage:\n\nmean(x, ...)\n\n## Default S3 method:\nmean(x, trim = 0, na.rm = FALSE, ...)\n\nThe first part tells you that the most basic way to use the function is to provide an argument called x. What x is, is explained in the arguments section that we discuss below.\nThe ... at the end means that the function can take additional arguments. This is because the mean function is a generic function. Depending on the type of input you provide (e.g., numbers, dates), some arguments might not be relevant.\nThe second part tells you that the default method for the mean function has two additional arguments in addition to x: trim, and na.rm. Note an important difference with the x argument: these arguments have default values (0 and FALSE, respectively). This means that these arguments are optional. If you don’t specify them, the function will use these default values.\nFor example, notice that the na.rm argument is set to FALSE by default. As we can see in the Arguments section, this means that the function will not remove missing values by default. (NA stands for Not Available, and is used in R to indicate missing values, so na.rm is short for remove NAs).\n\nx_with_missing &lt;- c(1, 2, 3, NA, 4)\nmean(x_with_missing)\n\n[1] NA\n\n\nIf we want to remove missing values, we can set na.rm to TRUE:\n\nmean(x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\n\n\n\n\n\n\nTo name or not to name your arguments\n\n\n\n\n\nNotice how in the code above we specify the argument name na.rm = TRUE to indicate that we want to use this optional argument. For the x argument we don’t need to specify the argument name, because it’s the first argument and the function knows that the first argument is x. Generally speaking, if you don’t specify the argument name, R will assume that you are providing the arguments in the order that they are listed in the usage section. Let’s think a bit about when we should and should not use argument names!\nYou could decide to always use argument names:\n\nmean(x=x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\nThis is fine, and sometimes you might want to do this for sake of clarity. But it’s also often unnecessary. For the mean function, it is obvious that the first argument is the input over which you want to calculate the mean, so you don’t need to specify the argument name.\nOn the opposite end of the spectrum, you could decide to never use argument names:\n\nmean(x_with_missing, 0, TRUE)\n\n[1] 2.5\n\n\nHere the three arguments follow the order in the usage section: x, trim, na.rm.\nThis has two obvious downsides:\n\nIt is not obvious what the 0 and TRUE arguments are. The reader might thus have to look up the function documentation.\nWe now also need to specify the trim argument, because it comes before na.rm in the usage section.\n\nSo in general, it is often good to use argument names for optional arguments, like na.rm. For required arguments, like x, it is often not necessary. Arguably the best way to use the mean function with na.rm is therefore:\n\nmean(x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\n\n\n\n\n\n\nA description of all the arguments that the function takes. This should cover all the arguments that are listed in the usage section.\nFor example, the mean() function explains that the x argument is can be a numeric vector, but also something like a logical or date vector. For the na.rm argument it explains that if set to TRUE, missing values will be removed before calculating the mean.\n\n\n\nThe value section explains what the function returns (i.e. the output).\n\n\n\nThe examples section shows you how to use the function. Honestly, this is often the most useful part of the help page. If you are not sure how to use a function, a great way to learn is to look at the examples. Usually, you can directly copy-paste these examples into your script and run them to see how the function works.",
    "crumbs": [
      "Good to Know",
      "Function Documentation"
    ]
  },
  {
    "objectID": "good-to-know/function-documentation.html#how-to-use-the-symbol",
    "href": "good-to-know/function-documentation.html#how-to-use-the-symbol",
    "title": "Function Documentation",
    "section": "",
    "text": "To view the documentation for a specific function, you simply need to type ? followed by the function name. For example, if you want to learn more about the mean() function, you would type:\n\n?mean\n\nThis will open the help page, often in the bottom right pane of RStudio.",
    "crumbs": [
      "Good to Know",
      "Function Documentation"
    ]
  },
  {
    "objectID": "good-to-know/function-documentation.html#how-to-read-the-help-page",
    "href": "good-to-know/function-documentation.html#how-to-read-the-help-page",
    "title": "Function Documentation",
    "section": "",
    "text": "The help page for a function is divided into several sections. The most important sections are:\n\n\nA brief description of what the function does. For the mean() function, the description is: Generic Function for the (Arithmetic) Mean.\nBy generic function, they mean that the function can have multiple implementations. When you think of the mean, you are probably thinking of the mean of a vector of numbers.\n\nx = c(1,2,3,4)\nmean(x)\n\n[1] 2.5\n\n\nBut you can do more! For example, you can also calculate the mean of a vector of Date values:\n\ndates = as.Date(c(\"2021-01-01\", \"2021-01-03\"))\nmean(dates)\n\n[1] \"2021-01-02\"\n\n\n\n\n\nThe syntax of the function, including all the arguments it takes. For example, the mean() function has the following usage:\n\nmean(x, ...)\n\n## Default S3 method:\nmean(x, trim = 0, na.rm = FALSE, ...)\n\nThe first part tells you that the most basic way to use the function is to provide an argument called x. What x is, is explained in the arguments section that we discuss below.\nThe ... at the end means that the function can take additional arguments. This is because the mean function is a generic function. Depending on the type of input you provide (e.g., numbers, dates), some arguments might not be relevant.\nThe second part tells you that the default method for the mean function has two additional arguments in addition to x: trim, and na.rm. Note an important difference with the x argument: these arguments have default values (0 and FALSE, respectively). This means that these arguments are optional. If you don’t specify them, the function will use these default values.\nFor example, notice that the na.rm argument is set to FALSE by default. As we can see in the Arguments section, this means that the function will not remove missing values by default. (NA stands for Not Available, and is used in R to indicate missing values, so na.rm is short for remove NAs).\n\nx_with_missing &lt;- c(1, 2, 3, NA, 4)\nmean(x_with_missing)\n\n[1] NA\n\n\nIf we want to remove missing values, we can set na.rm to TRUE:\n\nmean(x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\n\n\n\n\n\n\nTo name or not to name your arguments\n\n\n\n\n\nNotice how in the code above we specify the argument name na.rm = TRUE to indicate that we want to use this optional argument. For the x argument we don’t need to specify the argument name, because it’s the first argument and the function knows that the first argument is x. Generally speaking, if you don’t specify the argument name, R will assume that you are providing the arguments in the order that they are listed in the usage section. Let’s think a bit about when we should and should not use argument names!\nYou could decide to always use argument names:\n\nmean(x=x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\nThis is fine, and sometimes you might want to do this for sake of clarity. But it’s also often unnecessary. For the mean function, it is obvious that the first argument is the input over which you want to calculate the mean, so you don’t need to specify the argument name.\nOn the opposite end of the spectrum, you could decide to never use argument names:\n\nmean(x_with_missing, 0, TRUE)\n\n[1] 2.5\n\n\nHere the three arguments follow the order in the usage section: x, trim, na.rm.\nThis has two obvious downsides:\n\nIt is not obvious what the 0 and TRUE arguments are. The reader might thus have to look up the function documentation.\nWe now also need to specify the trim argument, because it comes before na.rm in the usage section.\n\nSo in general, it is often good to use argument names for optional arguments, like na.rm. For required arguments, like x, it is often not necessary. Arguably the best way to use the mean function with na.rm is therefore:\n\nmean(x_with_missing, na.rm=TRUE)\n\n[1] 2.5\n\n\n\n\n\n\n\n\nA description of all the arguments that the function takes. This should cover all the arguments that are listed in the usage section.\nFor example, the mean() function explains that the x argument is can be a numeric vector, but also something like a logical or date vector. For the na.rm argument it explains that if set to TRUE, missing values will be removed before calculating the mean.\n\n\n\nThe value section explains what the function returns (i.e. the output).\n\n\n\nThe examples section shows you how to use the function. Honestly, this is often the most useful part of the help page. If you are not sure how to use a function, a great way to learn is to look at the examples. Usually, you can directly copy-paste these examples into your script and run them to see how the function works.",
    "crumbs": [
      "Good to Know",
      "Function Documentation"
    ]
  },
  {
    "objectID": "good-to-know/function-documentation.html#try-using-tab-completion-everywhere",
    "href": "good-to-know/function-documentation.html#try-using-tab-completion-everywhere",
    "title": "Function Documentation",
    "section": "Try using tab completion everywhere",
    "text": "Try using tab completion everywhere\nWell ok, not everywhere. But you might be surprised how often it can help you. It can even help you find files on your computer. If you use tab completion between quotes, RStudio will show you all the files in your working directory that match the characters you’ve typed so far. So you can use this inside functions like read_csv to quickly find the file you want to read.\n\nlibrary(tidyverse)\nread_csv(\"\")\n\nTry it out!",
    "crumbs": [
      "Good to Know",
      "Function Documentation"
    ]
  },
  {
    "objectID": "data-management/filter-and-arrange.html",
    "href": "data-management/filter-and-arrange.html",
    "title": "Filter and Arrange",
    "section": "",
    "text": "Required packages and data for this tutorial\n\n\n\nIn this tutorial we use the tidyverse package and the simulated practice data.\n\nlibrary(tidyverse)\nd &lt;- read_csv(\"https://tinyurl.com/R-practice-data\")",
    "crumbs": [
      "Data management",
      "Filter and Arrange"
    ]
  },
  {
    "objectID": "data-management/filter-and-arrange.html#the-condition-is-a-logical-expression",
    "href": "data-management/filter-and-arrange.html#the-condition-is-a-logical-expression",
    "title": "Filter and Arrange",
    "section": "The condition is a logical expression",
    "text": "The condition is a logical expression\nThe condition in filter can be any logical expression. A logical expression is simply a statement that is either TRUE or FALSE. When we use a logical expression in the filter function, we are asking R to evaluate this expression for each row in the tibble. Each row for which the expression evaluates to TRUE is then included in the subset.\nIf you know a bit about how logical expressions work, you will have great control over what rows are included in your subset. Here is an overview of the most important operators for logical expressions.\n\nComparison operators\nComparison operators are used to compare two values.\n\n== equal to\n!= not equal to\n&gt; greater than\n&gt;= greater than or equal to\n&lt; less than\n&lt;= less than or equal to\n%in% is in a list of values (second value must be a list or vector)\n\nExample:\n\n5 &gt; 1    # TRUE:  5 is greater than 1\n\n[1] TRUE\n\n5 &lt; 1    # FALSE: 5 is less than 1\n\n[1] FALSE\n\n\"A\" %in% c(\"A\", \"B\", \"C\")  # TRUE: \"A\" is in the list\n\n[1] TRUE\n\n\"A\" %in% c(\"B\", \"C\", \"D\")  # FALSE: \"A\" is not in the list\n\n[1] FALSE\n\n\n\n\nLogical operators\nLogical operators are used to combine multiple conditions.\n\n& and\n| or\n! not\n\nExample:\n\n5 &gt; 1 | 5 &lt; 1   # TRUE: 5 is greater than 1 OR 5 is less than 1\n\n[1] TRUE\n\n5 &gt; 1 & 5 &lt; 1   # FALSE: 5 is greater than 1 AND 5 is less than 1\n\n[1] FALSE\n\n!5 &lt; 1          # TRUE: it is not the case that 5 is smaller than 1\n\n[1] TRUE\n\n\n\n\nUsing equations\nYou can also use equations in your conditions. For example, to select all rows where the absolute difference between trust_t1 and trust_t2 is greater than 2:\n\nfilter(d, abs(trust_t2 - trust_t1) &gt; 2)\n\n\n\nParentheses\nFor complex conditions, you can use parentheses to group conditions, similar to how you would in a mathematical expression. For example, say that you want to inspect surprising cases where trust in journalists decreased after watching the positive movie, or increased after watching the negative movie.\n\nfilter(d, (experiment_group == 'positive' & trust_t2 &lt; trust_t1) |\n          (experiment_group == 'negative' & trust_t2 &gt; trust_t1))",
    "crumbs": [
      "Data management",
      "Filter and Arrange"
    ]
  },
  {
    "objectID": "analysis/techniques/controlling.html",
    "href": "analysis/techniques/controlling.html",
    "title": "Controlling for confounders",
    "section": "",
    "text": "In the section about causality we discussed the difference between correlation and causation, and what techniques we can use to distinguish between the two. One of those techniques is to use multivariate analysis techniques, such as multiple regression. In this tutorial we will show you how to use multiple regression to control for confounding variables, and how this helps us deal with spurious correlations. As example data we’ll be using a famous example of a spurious correlation: the number of storks and the birth rate of a country.\n\n\nThere is a famous example of confounding, that points at the surprisingly high correlation between the number of storks and the number of newborn babies across European countries (\\(\\rho = 0.62\\)). Based on this empirical finding, could one actually argue that the might be some truth to the old folk tale that storks deliver babies? The answer to this questions is obviously no, but it does raise an important issue: how can we distinguish between correlation and causation? The example of storks and babies is sufficiently ridiculous to make it clear that this is a spurious correlation, but how can we be sure in more realistic cases?\nOne of the techniques in our tool belt is to control for confounding variables in a multiple regression model. Let’s see how this works in practice using the storks and babies data from the article.\n\nlibrary(tidyverse)\nlibrary(sjPlot)\n\nstorks &lt;- tibble(\n storks = c(100,300,1,5000,9,140,3300,2500,4,5000,5,30000,1500,5000,8000,150,25000),\n birth_rate = c(83,87,118,117,59,774,901,106,188,124,551,610,120,367,439,82,1576),\n area_km2 = c(29,84,31,111,43,544,357,132,42,93,301,312,92,237,505,41,779) \n)\n\n\n\n\nThe data we have here is about the number of storks, the birth_rate, and the area_km2 of the country. Let’s first have a look at the correlations.\n\ntab_corr(storks)\n\n\n\n\n \nstorks\nbirth_rate\narea_km2\n\n\nstorks\n \n0.620**\n0.579*\n\n\nbirth_rate\n0.620**\n \n0.922***\n\n\narea_km2\n0.579*\n0.922***\n \n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nAs reported in the article, the correlation between the number of storks and the birth rate is high at \\(r = 0.62\\). But we also see that the area of the country is correlated with both the number of storks and the birth rate (\\(r = 0.58\\) and \\(r = 0.92\\) respectively). This is a good indication that the area of the country is likely a confounding variable.\n\n\n\nNow let’s see what happens when we put the number of storks into a regression model. Our dependent variable is the birth_rate, our independent variable is the storks, and we’ll control for the area_km2. The variables are standardized first, so that you can see the relation to the correlation coefficients. We’ll run two models: without and with the control variable.\n\n##  standardize all variables\nstorks_z &lt;- storks |&gt; scale() |&gt; as_tibble()\n\nm1 &lt;- lm(birth_rate ~ storks,            data = storks_z)\nm2 &lt;- lm(birth_rate ~ storks + area_km2, data = storks_z)\ntab_model(m1, m2)\n\n\n\n\n \nbirth_rate\nbirth_rate\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n-0.00\n-0.42 – 0.42\n1.000\n0.00\n-0.21 – 0.21\n1.000\n\n\nstorks\n0.62\n0.19 – 1.05\n0.008\n0.13\n-0.13 – 0.39\n0.304\n\n\narea km2\n\n\n\n0.85\n0.59 – 1.11\n&lt;0.001\n\n\nObservations\n17\n17\n\n\nR2 / R2 adjusted\n0.385 / 0.344\n0.862 / 0.842\n\n\n\n\n\n\n\nIn the first model we see a positive effect of the number of storks on the birth rate (\\(\\beta = 0.62\\)). The effect is significant (p &lt; 0.01) and the standardized coefficient is the same as the correlation coefficient. So here you see firsthand a spurious effect.\nIn the second model, where we control for the area of the country, the effect of the number of storks on the birth rate is no longer significant (p = 0.304). Instead, we see a very strong effect of the area of the country on the birth rate (\\(\\beta = 0.85\\), p &lt; 0.001). So here you see the power of multiple regression in action: by controlling for area_km2, the spurious effect of storks on birth_rate disappears.",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Controlling for confounders"
    ]
  },
  {
    "objectID": "analysis/techniques/controlling.html#do-storks-deliver-babies",
    "href": "analysis/techniques/controlling.html#do-storks-deliver-babies",
    "title": "Controlling for confounders",
    "section": "",
    "text": "There is a famous example of confounding, that points at the surprisingly high correlation between the number of storks and the number of newborn babies across European countries (\\(\\rho = 0.62\\)). Based on this empirical finding, could one actually argue that the might be some truth to the old folk tale that storks deliver babies? The answer to this questions is obviously no, but it does raise an important issue: how can we distinguish between correlation and causation? The example of storks and babies is sufficiently ridiculous to make it clear that this is a spurious correlation, but how can we be sure in more realistic cases?\nOne of the techniques in our tool belt is to control for confounding variables in a multiple regression model. Let’s see how this works in practice using the storks and babies data from the article.\n\nlibrary(tidyverse)\nlibrary(sjPlot)\n\nstorks &lt;- tibble(\n storks = c(100,300,1,5000,9,140,3300,2500,4,5000,5,30000,1500,5000,8000,150,25000),\n birth_rate = c(83,87,118,117,59,774,901,106,188,124,551,610,120,367,439,82,1576),\n area_km2 = c(29,84,31,111,43,544,357,132,42,93,301,312,92,237,505,41,779) \n)",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Controlling for confounders"
    ]
  },
  {
    "objectID": "analysis/techniques/controlling.html#correlation-analysis",
    "href": "analysis/techniques/controlling.html#correlation-analysis",
    "title": "Controlling for confounders",
    "section": "",
    "text": "The data we have here is about the number of storks, the birth_rate, and the area_km2 of the country. Let’s first have a look at the correlations.\n\ntab_corr(storks)\n\n\n\n\n \nstorks\nbirth_rate\narea_km2\n\n\nstorks\n \n0.620**\n0.579*\n\n\nbirth_rate\n0.620**\n \n0.922***\n\n\narea_km2\n0.579*\n0.922***\n \n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nAs reported in the article, the correlation between the number of storks and the birth rate is high at \\(r = 0.62\\). But we also see that the area of the country is correlated with both the number of storks and the birth rate (\\(r = 0.58\\) and \\(r = 0.92\\) respectively). This is a good indication that the area of the country is likely a confounding variable.",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Controlling for confounders"
    ]
  },
  {
    "objectID": "analysis/techniques/controlling.html#multiple-regression-analysis",
    "href": "analysis/techniques/controlling.html#multiple-regression-analysis",
    "title": "Controlling for confounders",
    "section": "",
    "text": "Now let’s see what happens when we put the number of storks into a regression model. Our dependent variable is the birth_rate, our independent variable is the storks, and we’ll control for the area_km2. The variables are standardized first, so that you can see the relation to the correlation coefficients. We’ll run two models: without and with the control variable.\n\n##  standardize all variables\nstorks_z &lt;- storks |&gt; scale() |&gt; as_tibble()\n\nm1 &lt;- lm(birth_rate ~ storks,            data = storks_z)\nm2 &lt;- lm(birth_rate ~ storks + area_km2, data = storks_z)\ntab_model(m1, m2)\n\n\n\n\n \nbirth_rate\nbirth_rate\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n-0.00\n-0.42 – 0.42\n1.000\n0.00\n-0.21 – 0.21\n1.000\n\n\nstorks\n0.62\n0.19 – 1.05\n0.008\n0.13\n-0.13 – 0.39\n0.304\n\n\narea km2\n\n\n\n0.85\n0.59 – 1.11\n&lt;0.001\n\n\nObservations\n17\n17\n\n\nR2 / R2 adjusted\n0.385 / 0.344\n0.862 / 0.842\n\n\n\n\n\n\n\nIn the first model we see a positive effect of the number of storks on the birth rate (\\(\\beta = 0.62\\)). The effect is significant (p &lt; 0.01) and the standardized coefficient is the same as the correlation coefficient. So here you see firsthand a spurious effect.\nIn the second model, where we control for the area of the country, the effect of the number of storks on the birth rate is no longer significant (p = 0.304). Instead, we see a very strong effect of the area of the country on the birth rate (\\(\\beta = 0.85\\), p &lt; 0.001). So here you see the power of multiple regression in action: by controlling for area_km2, the spurious effect of storks on birth_rate disappears.",
    "crumbs": [
      "Analysis",
      "Techniques",
      "Controlling for confounders"
    ]
  },
  {
    "objectID": "analysis/tests/linear-regression.html",
    "href": "analysis/tests/linear-regression.html",
    "title": "Linear regression",
    "section": "",
    "text": "Regression analysis is a statistical technique for predicting a dependent variable based on one or more independent variables. We use predict in a statistical sense, meaning that we are estimating how well we can guess the dependent variable given knowledge of the independent variable(s). The reason we want to do this is usually not to make actual predictions, but to understand the relationship between the variables. If one variable can predict another, it tells us something about how they might be related.\nLet’s first look a how this prediction works for a single independent variable. This is also called simple linear regression. We have a dependent variable Y and an independent variable X, and we want to predict Y based on X. The goal is to find a mathematical formula that best describes their relationship. You can visually think of this as a scatterplot, where we draw a regression line such that the distance between the line and the data points is as small as possible.\n\n\n\n\n\n\n\n\n\nThis model can be described by a classic formula, called the regression equation. We’ve coloured the different parts of the equation according to the colours used in the visualization.\n\\[ \\Large  \\color{Green} Y_i = \\color{Blue} b_0 + b_1 X_i + \\color{Red} \\epsilon_i \\]\n\n\\(\\color{Green} Y_i\\) represents the real values of the dependent variable (\\(Y_1, Y_2, ... Y_n\\)) that we want to predict.\n\\(\\color{Blue} b_0 + b_1 X_i\\) represents our prediction of y. It has two coefficients:\n\n\\(\\color{Blue} b_0\\) is the intercept. This is the value of Y when X is zero.\n\\(\\color{Blue} b_1\\) is the slope. This tells us how much Y changes for every unit increase in X.\n\n\\(\\color{Red} \\epsilon_i\\) represents the error. This is the distance between the predicted and the real value of Y.\n\nFor example, at \\(X = 10\\), the blue line predicting Y passes through the point \\(Y = 10.45\\). However, the real value of Y at this point is 8.66. So the error \\(\\epsilon_i\\) at this point is \\(8.66 - 10.45 = -1.79\\).\nThe point of regression analysis is to find the values of \\(b_0\\) (intercept) and \\(b_1\\) (slope) that minimize the error. More specifically, it minimizes the sum of the squared errors (SSE), which is why this method is called least squares regression.\n\\[  SSE = \\large \\sum_{i=1}^{n} {\\color{red} \\epsilon_i^2} \\]\nTry it yourself! Change the values of the intercept and slope in the interactive widget below, and see how the regression line changes. The goal is to get the lowest possible SSE, which means that the line is as close as possible to the data points.\n\nviewof intercept = Inputs.range(\n  [-8, 6], \n  {value: 3, step: 0.001, label: \"Intercept\"}\n)\nviewof slope = Inputs.range(\n  [0.5,1.8], \n  {value: 0.5, step: 0.001, label: \"Slope\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx_line = Array.from({ length: 100 }, (_, i) =&gt; i-5);\ny_line = x_line.map(x =&gt; slope * x + intercept);\nmutable error = 0\n\nupdate_error = {\n  let sse = 0\n  x.forEach((d, i) =&gt; {\n    sse += Math.pow(y[i] - (slope * x[i] + intercept), 2)\n  })\n  mutable error = Number(sse.toFixed(2))\n}\n\n// Now, plot the regression line\nPlot.plot({\n  marks: [\n    Plot.lineY(y_line, {x: x_line, stroke: \"blue\"}),\n    Plot.dot(x, {x: x, y: y, fill: \"darkgreen\"}),\n    Plot.text([{intercept,slope}], {x: 10, y: 3, text: d =&gt; `prediction = ${d.intercept.toFixed(2)} + ${d.slope.toFixed(2)} * X`, fill: \"black\", fontSize: 16, fill: \"blue\", textAnchor: \"start\"}),\n    Plot.text([error], {x: 10, y: 0, text: d =&gt; `SSE = ${d}`, fill: \"black\", fontSize: 16, fill: \"red\", textAnchor: \"start\"})\n  ],\n  x: {\n    label: \"X\",\n    domain: [-5, 20]\n  },\n  y: {\n    label: \"Y\",\n    domain: [-5,22]\n  },\n  height: 300\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpoiler\n\n\n\n\n\nThe optimal values for the intercept and slope are \\(b_0 = 0.930\\) and \\(b_1 = 0.952\\). This should give you an SSE of \\(160.24\\).\n\n\n\n\n\nThe primary goal of regression analysis is to understand the relationship between variables. It allows us to test hypotheses of the sort:\n\nOlder people have more trust in journalists\nPeople on the political left have more trust in journalists than people on the political right\n\nYou might be wondering: can’t we already do the first one with a correlation test, and the second one with a t-test? The answer is yes, and one benefit of regression analysis is that it can do both these things in the same model. But more importantly, regression analysis can test both of these things at the same time, and account for the fact that age and political orientation are related to each other! If we know that younger people are more likely to be on the political left, then in order to test the effect of age on trust, we need to somehow account for political orientation.\nIn this tutorial we will show you how to use regression analysis to predict a person’s trust in journalists based on age and political orientation. The model we will be working towards looks like this:\n\\[ \\large  \\color{Green} Y_i = \\color{Blue} b_0 + b_1 X_{age} + b_2 X_{political\\_left} + b_3 X_{political\\_right} + \\color{Red} \\epsilon_i \\]\nDon’t worry if this doesn’t make sense yet! We’ll take it step by step.\n\nWe’ll first show you how to do a simple linear regression, where we predict trust based on age.\nThen we’ll show you how to include a categorical variable in the model, by predicting trust based on political orientation.\nFinally, we’ll show you how to include both age and political orientation in the model at the same time.",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Linear regression"
    ]
  },
  {
    "objectID": "analysis/tests/linear-regression.html#regression-with-numerical-independent-variable",
    "href": "analysis/tests/linear-regression.html#regression-with-numerical-independent-variable",
    "title": "Linear regression",
    "section": "Regression with numerical independent variable",
    "text": "Regression with numerical independent variable\nLet’s start with the example of predicting trust in journalists based on age. For this we’ll be using the trust_t11 variable as the dependent variable, and the age variable as the independent variable. We can use the lm() function to create a linear regression model. Inside the lm() function we specify the formula for the regression, which has the format dependent ~ independent.\n\nm &lt;- lm(trust_t1 ~ age, data = d) \n\nThe standard way to inspect the results is using the summary(m) function. This gives you all the information you need, but not in a very nice and readable format. We will therefore be using the tab_model() function from the sjPlot package to get a nice (APA ready) table of the results.\n\ntab_model(m)\n\n\n\n \ntrust_t1\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n2.28\n2.04 – 2.51\n&lt;0.001\n\n\nage\n0.04\n0.03 – 0.04\n&lt;0.001\n\n\nObservations\n595\n\n\nR2 / R2 adjusted\n0.261 / 0.260\n\n\n\n\n\n\n\n\n\n\n\n\nBackup plan if tab_model doesn’t work\n\n\n\n\n\nThe tab_model function by default shows the regression table in your viewer pane. If this for some reason doesn’t work, you can also use the use.viewer=F argument to show the table in your default web browser. This has the additional benefit that you can directly copy-paste the table into most text editors.\n\ntab_model(m, use.viewer=F)\n\n\n\n\nThe output of the regression analysis gives us values for the two coefficients: the (intercept) and the age variable. We get the coefficient estimates, and also the p-values that tell us if these coefficients are statistically significant. With tab_model we also get the confidence interval.\nThe coefficient for age tells us how much trust_t1 changes for every unit increase in age. Our age variable is in years, so the coefficient 0.04 tells us that for every year of age, trust in journalists increases on average by 0.04 points.\nTo get a better view of what this means in practice, it can be helpful to plot the regression line. The sjPlot package has a function plot_model() that can do this for us. Here we visualize the prediction (type = \"pred\") for the age variable (terms = \"age\").\n\nplot_model(m, type = \"pred\", terms = \"age\")\n\n\n\n\n\n\n\n\nHere the regression line shows the predicted values. At age 20 the predicted trust is around 3.05. For every year of age the predicted trust increases by 0.04 points, so by the time a person is 40 years old, the predicted trust is around 3.85. Visualizing effects like this can give readers (and yourself) a better understanding of how strong the effect really is.",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Linear regression"
    ]
  },
  {
    "objectID": "analysis/tests/linear-regression.html#footnotes",
    "href": "analysis/tests/linear-regression.html#footnotes",
    "title": "Linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe use the trust_t1 variable here, because for the current analysis we are not interested in the effect of the experimental group. Since trust_t1 is measured before the experiment, it is not influenced by the experiment.↩︎\nYou can also use a character variable directly, but the factor type is more appropriate for categorical variables. It also let’s you specify the order of the categories, which is important for regression analysis because it let’s you specify the reference category.↩︎\nRemember that this is a simulated dataset, so in this case we know the true relationships between the variables. But these types of complex relationships are very common in real data.↩︎",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Linear regression"
    ]
  },
  {
    "objectID": "analysis/tests/correlation.html#spearmans-rho-and-kendalls-tau",
    "href": "analysis/tests/correlation.html#spearmans-rho-and-kendalls-tau",
    "title": "Correlation test",
    "section": "Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\)",
    "text": "Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\)\nTo use the Spearman’s \\(\\rho\\) or Kendall’s \\(\\tau\\) correlation, you need to specify the method argument in the cor.test function, or the corr.method argument in the tab_corr function.\n\ncor.test(d$news_consumption, d$trust_t1, method = \"spearman\")\ncor.test(d$news_consumption, d$trust_t1, method = \"kendall\")\n\nd |&gt; select(trust_t1_item1:trust_t1_item5) |&gt; tab_corr(corr.method=\"spearman\")\nd |&gt; select(trust_t1_item1:trust_t1_item5) |&gt; tab_corr(corr.method=\"kendall\")",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Correlation test"
    ]
  },
  {
    "objectID": "analysis/tests/linear-regression.html#regression-with-binary-independent-variable",
    "href": "analysis/tests/linear-regression.html#regression-with-binary-independent-variable",
    "title": "Simple linear regression",
    "section": "Regression with binary independent variable",
    "text": "Regression with binary independent variable\nThe independent variable in a regression model can also be binary. This allows us to include categorical variables in the model, similar to the t-test and ANOVA. Let’s see what this looks like when we predict trust_t1 based on the np_subscription variable, which indicates if a person has a newspaper subscription. This is coded as a character vector with values yes and no. In order to use this variable in the regression model, we need to convert it to a binary variable. R can actually do this for us, but we’ll first show you how to do it yourself to understand what’s happening.\n\nd = mutate(d, has_subscription = if_else(np_subscription == \"yes\", 1, 0))\n\nselect(d, np_subscription, has_subscription)\n\n# A tibble: 600 × 2\n   np_subscription has_subscription\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 no                             0\n 2 no                             0\n 3 yes                            1\n 4 yes                            1\n 5 no                             0\n 6 no                             0\n 7 yes                            1\n 8 yes                            1\n 9 yes                            1\n10 no                             0\n# ℹ 590 more rows\n\n\nAnd now we can use this variable in the regression model.\n\nm &lt;- lm(trust_t1 ~ has_subscription, data = d)\n\ntab_model(m)\n\n\n\n\n \ntrust_t1\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n3.68\n3.56 – 3.80\n&lt;0.001\n\n\nhas subscription\n0.37\n0.22 – 0.53\n&lt;0.001\n\n\nObservations\n600\n\n\nR2 / R2 adjusted\n0.035 / 0.033\n\n\n\n\n\n\n\no Take some time to think why this makes sense. By converting the np_subscription variable to a binary variable, we can use it in the regression model.\nNow, if we use this variable in the regression model, our formula will look like this: \\[ trust\\_t1 = intercept + slope_{has} \\times age \\]\nWhen we include this variable in the regression model, R automatically converts it to a binary variable with values 0 and 1.\nWe do this by creating a new variable that has the value 1 if the person has a subscription, and 0 if they don’t. This type of variable is also called a dummy variable.\n\nd &lt;- mutate(d, has_subscription = if_else(np_subscription == \"yes\", 1, 0))\n\nWhen we include this variable in the regression model, R automatically converts it to a binary variable with values 0 and 1.\n\nm &lt;- lm(trust_t1 ~ has_subscription, data = d)\n\ntab_model(m)\n\n\n\n\n \ntrust_t1\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n3.68\n3.56 – 3.80\n&lt;0.001\n\n\nhas subscription\n0.37\n0.22 – 0.53\n&lt;0.001\n\n\nObservations\n600\n\n\nR2 / R2 adjusted\n0.035 / 0.033\n\n\n\n\n\n\nplot_model(m, type = \"pred\", terms = \"has_subscription\", show.data = T, jitter=T)\n\n\n\n\n\n\n\n\nJust like before, we get an intercept and a slope. The interpretation is mostly the same, but we need to keep in mind that the np_subscription variable is binary.\nNotice that the label for the np_subscription variable is np_subscription [yes]. This tells us that R",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Simple linear regression"
    ]
  },
  {
    "objectID": "analysis/tests/linear-regression.html#regression-with-categorical-independent-variable",
    "href": "analysis/tests/linear-regression.html#regression-with-categorical-independent-variable",
    "title": "Linear regression",
    "section": "Regression with categorical independent variable",
    "text": "Regression with categorical independent variable\nThe independent variable in a regression model can also be categorical. This allows us to include categorical variables in the model, similar to the t-test and ANOVA. Let’s see what this looks like when we predict trust_t1 based on the political_orientation variable, which has the values left, right and center.\nWe cannot directly use the values “left”, “right” and “center” in the regression equation, but we create so-called dummy variables. A dummy variable is a binary variable (i.e., it can only take the values 0 and 1) that represents whether something is the case or not. So if we want to test whether people that lean to the left have more trust in journalists, we can create a dummy variable that is 1 for people with orientation left, and 0 for all other people (i.e., right and center).\n\nd = mutate(d, political_left = if_else(political_orientation == \"left\", 1, 0))\n\nselect(d, political_orientation, political_left)\n\nNow we can use this variable in the regression model.\n\nm &lt;- lm(trust_t1 ~ political_left, data = d)\n\ntab_model(m)\n\n\n\n \ntrust_t1\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n3.83\n3.73 – 3.93\n&lt;0.001\n\n\npolitical left\n0.21\n0.03 – 0.38\n0.019\n\n\nObservations\n600\n\n\nR2 / R2 adjusted\n0.009 / 0.008\n\n\n\n\n\n\nThe interpretation of the results is almost identical to the previous example with the age variable. The only thing we need to keep in mind is that the political_left variable is binary, so it can only take the values 0 and 1. Accordingly, we don’t say that the trust in journalists increases by 0.21 points for every unit increase in political_left. Instead, we just say that people on the political left (political_left = 1) have on average a trust score that is 0.21 points higher than people that are not on the political left (political_left = 0).\n\n\nCategories with more than two levels\nIn the previous example we created the dummy variable ourselves, but we can also let R do this for us. This is especially convenient if we have more than two categories. In addition, this also has the benefit that lm knows that the variable is categorical, which enables us (among other things) to visualize the model propertly.\nIf an independent variable is of the character or factor type, R will automatically create dummy variables for us.\nWe first need to make sure that our variable is of the factor type2. The difference between a character and a factor is that in a factor we explicitly tell R what the categories are, and what the order of the categories is. The order is important, because the first category will be the reference category (more on this later) in the regression model. To transform our political_orientation variable to a factor we can use the factor() function, and provide the levels argument to specify the order of the categories.\n\nd &lt;- mutate(d, political_orientation = factor(political_orientation, levels=c('center','left','right')))\n\nNow, when we use the political_orientation variable in the regression model, R will automatically create the dummy variables for us.\n\nm &lt;- lm(trust_t1 ~ political_orientation, data = d)\ntab_model(m)\n\n\n\n \ntrust_t1\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n3.91\n3.77 – 4.05\n&lt;0.001\n\n\npolitical orientation[left]\n0.12\n-0.07 – 0.32\n0.220\n\n\npolitical orientation[right]\n-0.17\n-0.36 – 0.03\n0.100\n\n\nObservations\n600\n\n\nR2 / R2 adjusted\n0.014 / 0.010\n\n\n\n\n\n\nNotice that there are now two coefficients for the political_orientation variable: one for left and one for right. Why 2, and not 3? This is because when we have a categorical variable with k levels, we only need k-1 coefficients to represent all the levels. If we know that a person is not in the left category, and not in the right category, then we know that the person must be in the center category.\n\n\n\npolitical_orientation\nis_left\nis_right\n\n\n\n\nleft\n1\n0\n\n\nright\n0\n1\n\n\ncenter\n0\n0\n\n\n\nThe center category is now the reference category. That is, the center category is the category that all other categories are compared to. So the political_orientation [left] coefficient tells us that trust is for people in the left category is 0.12 points higher than for people in the center category, but this difference is not significant (p = 0.220) The political_orientation [right] coefficient tells us that trust is for people in the right category is 0.17 points lower than for people in the center category, but this difference is also not significant (p = 0.100).\n\n\n\n\n\n\nDetermining what reference category to use\n\n\n\n\n\nIn the current model we cannot directly compare the left and right categories, because the center category is the reference category. This makes it important to choose a reference category that makes sense for your research question. To determine the reference category, you can change the order of the levels in the factor() function. The first level will always be the reference category. So in the following example, the left category is the reference category.\n\nd2 &lt;- mutate(d, political_orientation = factor(political_orientation, levels=c('left','center','right')))\n\nlm(trust_t1 ~ political_orientation, data = d2) |&gt;\n    tab_model()\n\n\n\n \ntrust_t1\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n4.04\n3.89 – 4.18\n&lt;0.001\n\n\npolitical orientation[center]\n-0.12\n-0.32 – 0.07\n0.220\n\n\npolitical orientation[right]\n-0.29\n-0.49 – -0.09\n0.004\n\n\nObservations\n600\n\n\nR2 / R2 adjusted\n0.014 / 0.010\n\n\n\n\n\n\n\n\n\nWhen we visualize the prediction for a categorical variable, we get a plot that shows the predicted value for each category, with the confidence interval.\n\nplot_model(m, type = \"pred\", terms = \"political_orientation\")\n\n\n\n\n\n\n\n\nHere we see that (in our simulated data) people on the political left have the highest trust in journalists, followed by people in the center, and then people on the political right. However, based on the model we know that the difference between left and center is not significant, and neither is the difference between center and right.",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Linear regression"
    ]
  },
  {
    "objectID": "analysis/concepts/mean_and_variance.html",
    "href": "analysis/concepts/mean_and_variance.html",
    "title": "Communication Science R Book",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Analysis",
      "Key concepts",
      "Mean and Variance"
    ]
  },
  {
    "objectID": "analysis/tests/linear-regression.html#what-can-we-do-with-it",
    "href": "analysis/tests/linear-regression.html#what-can-we-do-with-it",
    "title": "Linear regression",
    "section": "",
    "text": "The primary goal of regression analysis is to understand the relationship between variables. It allows us to test hypotheses of the sort:\n\nOlder people have more trust in journalists\nPeople on the political left have more trust in journalists than people on the political right\n\nYou might be wondering: can’t we already do the first one with a correlation test, and the second one with a t-test? The answer is yes, and one benefit of regression analysis is that it can do both these things in the same model. But more importantly, regression analysis can test both of these things at the same time, and account for the fact that age and political orientation are related to each other! If we know that younger people are more likely to be on the political left, then in order to test the effect of age on trust, we need to somehow account for political orientation.\nIn this tutorial we will show you how to use regression analysis to predict a person’s trust in journalists based on age and political orientation. The model we will be working towards looks like this:\n\\[ \\large  \\color{Green} Y_i = \\color{Blue} b_0 + b_1 X_{age} + b_2 X_{political\\_left} + b_3 X_{political\\_right} + \\color{Red} \\epsilon_i \\]\nDon’t worry if this doesn’t make sense yet! We’ll take it step by step.\n\nWe’ll first show you how to do a simple linear regression, where we predict trust based on age.\nThen we’ll show you how to include a categorical variable in the model, by predicting trust based on political orientation.\nFinally, we’ll show you how to include both age and political orientation in the model at the same time.",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Linear regression"
    ]
  },
  {
    "objectID": "analysis/tests/linear-regression.html#regression-with-multiple-independent-variables",
    "href": "analysis/tests/linear-regression.html#regression-with-multiple-independent-variables",
    "title": "Linear regression",
    "section": "Regression with multiple independent variables",
    "text": "Regression with multiple independent variables\nNow, we can finally get to the most interesting part: using multiple independent variables in the regression model. This is called multiple regression, and it allows us to test the effect of multiple variables on the dependent variable at the same time. The formula for using multiple independent variables is: dependent ~ independent1 + independent2 + ....\nWe’ll be using both the age and political_orientation variables. As we mentioned earlier, by including both variables in the model we can test the effect of age on trust while controlling for political_orientation, and vice versa. To show you how this works, we’ll create two models, and show them side by side by plugging them both into the tab_model() function.\n\nm1 &lt;- lm(trust_t1 ~ political_orientation, data = d)\nm2 &lt;- lm(trust_t1 ~ political_orientation + age, data = d)\n\ntab_model(m1, m2)\n\n\n\n \ntrust_t1\ntrust_t1\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n3.91\n3.77 – 4.05\n&lt;0.001\n2.11\n1.87 – 2.35\n&lt;0.001\n\n\npolitical orientation[left]\n0.12\n-0.07 – 0.32\n0.220\n0.26\n0.09 – 0.42\n0.003\n\n\npolitical orientation[right]\n-0.17\n-0.36 – 0.03\n0.100\n-0.42\n-0.59 – -0.25\n&lt;0.001\n\n\nage\n\n\n\n0.04\n0.04 – 0.05\n&lt;0.001\n\n\nObservations\n600\n595\n\n\nR2 / R2 adjusted\n0.014 / 0.010\n0.330 / 0.327\n\n\n\n\n\n\nIn the first model, we did not see any effect of having a left or right political orientation compared to the center (reference) category. But once we include the age variable in the model, both the left and right categories show do show a significant effect on trust!\nThe reason for this is that the age variable is related to both the political_orientation variable and the trust variable.3 Younger people have less trust in journalists, but are also more likely to be on the political left. Therefore, if we do not control for age, the positive effect of being on the political left is suppressed by the negative effect of being younger.\nIf this sounds confusing, don’t worry, because you’re in good company! Thinking about how the effect of independent variables on the dependent variable depends on the relation between the independent variables can make your head spin. But for better or worse, this is how the world works, and this is why we need sophisticated statistical tools to help us understand it.",
    "crumbs": [
      "Analysis",
      "Statistical tests",
      "Linear regression"
    ]
  }
]