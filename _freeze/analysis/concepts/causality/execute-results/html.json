{
  "hash": "7bf74357839f812978932c3e7e943228",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Causality\nsubtitle: How to find evidence for causal relations?\norder: 3\n---\n\n\n\n\n\n\n![xkcd: correlation](https://imgs.xkcd.com/comics/correlation.png)\n\n\n# Testing causal relations\n\nIn communication science we are often concerned with **causal relations**. \nConsider the following two examples:\n\n- When news media focus heavily on certain issues, it enhances the public's perception of their importance [@mccombs72].\n- More screen time among children and adolescents is associated with lower psychological well-being [@twenge18].\n\nResearch into such causal relations is important, because by understanding causal mechanisms, we can develop strategies to change the outcomes for the better.\nIf more screen time causes lower psychological well-being, we should advice parents to limit children's screen time.\n\nIn order to determine whether we should really incur the wrath of our children by taking away their smartphones, we need to test our theory with empirical data.\nAs usual, we can [derive hypotheses](../concepts/hypotheses.qmd) that predict relationships between variables, and then test these hypotheses with **statistical methods**.\nHowever, when testing causal relations, we need to particularly careful!\n\nA common mistake is to assume that a correlation between two variables implies a causal relationship.\nEven if we find strong evidence that people that have more screen time also have lower psychological well-being, this does not prove that screen time causes lower well-being!\nThere is a famous saying in statistics: **correlation does not imply causation**.\nIn this tutorial you will learn why this is the case, and how to deal with it.\n\n\n# Why correlation does not imply causation\n\n**Causation** implies a cause-effect relationship, where a change in one variable (the cause) leads to a change in the other (the effect). \nThe cause does not need to be the *only* cause of the effect (there are many things that affect well-being), nor does it *always* need to lead to the effect (some people with high screen time will be happier than other with low screen time).\nIt is enough that the cause increases the likelihood of the effect.\nIf we were to somehow manipulate the cause, we would expect the likelihood of the effect to change.\n\n[Correlation](../concepts/covariance-and-correlation.qmd), on the other hand, only indicates an association or relationship between two variables.\nIt makes sense why people can confuse this for a causal relationship.\nIf we see that people that that are often on their phone are overall less happy, it is easy to assume that the phone is the cause of their unhappiness.\nBut in order to establish whether a correlation is due to a causal relationship, we need address three issues:\n\n* **Confounding**: A correlation could be due to a third variable that causes both of them to change. \n    Both screen time and psychological well-being could for instance be influenced by parental involvement.\n    If the relation between two variables is not because they are causally related, but because they are both influenced by a third variable, we call this a **spurious correlation**.\n    The third variable that causes the correlation is called a **confounding variable**.\n* **Directionality**: A correlation does not tell us which variable causes the other.\n    It could be that more screen time causes lower psychological well-being, but it could also be that children with lower psychological well-being spend more time on screens.\n    If this is the case, taking away their phone would not help!\n* **Coincidence**: A correlation is also **spurious** if it is just a coincidence.\n    This is something we can address with statistical tests, but it bears keeping in mind. (See the box below for some famous examples of spurious correlations.)\n    \n::: {.callout-tip title=\"Famous spurious correlations\" collapse=\"true\"}\nA [famous example](http://www.brixtonhealth.com/storksBabies.pdf) is that across European countries the number of storks is quite strongly correlated with the number of newborn babies ($\\rho = 0.62$). A naive interpretation would be that this provides evidence for the folk theory that storks deliver babies. The real reason in this case is due to a confounding variable: the size of the country. Larger countries simply have more storks and more babies.\n\nThere is also a website called [Spurious Correlations](https://www.tylervigen.com/spurious-correlations), that is dedicated to finding coincidental spurious correlations, such as the correlation between the number of people who drowned by falling into a pool and the number of films Nicolas Cage appeared in.\n:::\n\n## Visualizing causation \n\nTo develop a good intuition for the difference between correlation and causation, it helps to have a good mental model of what a causal relationship looks like.\nA good way to do this is to use **Directed Acyclic Graphs** (DAGs), which are a visual way to represent causal relationships.\nSimply put, these are graphs where the arrows indicate the direction of the causal relationship.\nHere are three DAGs that show different reasons for a correlation between X and Y.\nX represents what we believe to be the cause (e.g., screen time), and Y the effect (e.g., well-being).\nZ represents a confounding variable (e.g., parental involvement).\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](causality_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThis shows why a correlation between X and Y is not enough to establish causation.\nEach of these DAGs shows a different reason for the correlation between X and Y, but only the first one supports our hypothesis that X causes Y.\nOur job as researchers is to figure out which of these DAGs is best supported by empirical evidence.\n\n# How to (better) establish causation\n\nLet's first acknowledge that establishing causation is hard (see @pearl09).\nBut this should not deter us from doing our best to build evidence for causal relationships!\nLuckily, we have a number of strategies at our disposal to build evidence for causation.\n\n\n## Theoretical foundation\n\nThe most important thing is to have a **theoretical foundation** for a causal relationship.\nThis means that we have a good reason to believe that that a change in one variable will lead to a change in the other.\n\nIdeally, this theory should be based on **a priori** reasoning, meaning that you develop your theory before you collect data.\nThis avoids the risks of **post hoc** (Latin for \"after this\") reasoning, which is also known as **HARKing** (Hypothesizing After the Results are Known).\nThe prime risk of post hoc reasoning is that you end up shaping a theory to fit the data, rather than using theory to guide your data collection and analysis.\nMany scientific journals therefore require you to pre-register your hypotheses before you start collecting data.\nThis doesn't mean that there is no room for exploration in your data, but it does mean that you should be transparent about what you were looking for based on **a priori** reasoning, and what you learned from **post hoc** reasoning about your data.\n\nTheory also helps us to identify possible confounding variables.\nConsider for example the **gender gap in chess**.\nWomen are underrepresented in chess, with especially few women at the top levels [@christopher07].\nA naive interpretation would be that this is because, as grandmaster Garry Kasparov said in 1989: \"Men are hardwired to be better chess players than women\". \nTo properly understand where this gap comes from, we need to build theory that takes alternative explanations into account.\nIf you have time for a tangent, try reading [this Slate Magazine article](https://slate.com/technology/2020/12/why-are-the-best-chess-players-men.html) and see if you can identify some of the confounding variables that the author mentions.\n\n\n\n## If possible, conduct an experiment\n\nThe gold standard for establishing causation is the **randomized controlled experiment**, in which the researcher *manipulates the cause* and observes the effect, while keeping all other variables constant.\nIn order to manipulate the cause, the researcher creates different experimental conditions.\nThe classic example comes from the field of medicine: you give some participants a medicine you want to test, where others get a placebo (i.e. a fake medicine that only looks the same).\nThe people that got the real medicine are then the *treatment group*, and the people that got the placebo are the *control group*\nTo test if the medicine works, you compare the treatment group to the control group.[^1]\n\n[^1]: A similar example from communication science could be that you want to test the effictiveness of a new persuasion strategy.\nYou could then show the treatment group a persuasive message that uses the new strategy, and the control group a message that doesn't. \n\nThe experimental design helps to adress both **confounding** and **directionality**.\n\n* By randomly assigning people to the treatment or control group, the people in both groups are *statistically identical*. If the randomization is done correctly, any possible confounding variables are equally distributed between the two groups, and so any difference in the outcome can be attributed to the treatment. \n* By comparing the treatment group to the control group, you can account for confounding variables that have to do with the process of appyling the treatment. A well known example of this is the *placebo effect*, where people feel better just because they *think* they're taking a medicine (control group), and not because of the actual medicine (treatment group). If the medicine works, the treatment group should see a stronger effect than the control group. \n\nThis ability to eliminate the influence of confounders makes experiments a powerful tool for establishing causation.\nHowever, in the field of communication science it is often not possible (or ethical) to conduct valid experiments.\nFor many of the causes that we are interested in, it is simply not possible to manipulate it in a controlled setting that is still sufficiently similar to the real world.\n\n## For observational data, control for confounders\n\nIf we cannot conduct an experiment, we can to some extend control for confounding variables using **statistical methods**.\nThis is not as powerful as an experiment, but it does allow us to establish some level of evidence for causation from observational data.\nIn fact, many well supported causal theories in communication science have only been supported this way, such as the relationship between media coverage and public opinion [@mccombs72].\nAnd even well established causal relations like \"smoking causes cancer\" rely heavily on evidence from observational studies.\nGiven that many communication science theories cannot be tested with experiments, being able to gather evidence for causal relations from observational data is an essential part of the communication scientist's toolkit.\n\nThe trick behind statistically controlling for confounders lies in **multivariate analysis** (i.e. analyzing more than two variables at the same time).\nIf we are worried that a correlation between a **dependent variable** and **independent variable** is due to a third variable, we can add this **confounding variable** to the analysis.\nUsing techniques like [multiple regression](../tests/multiple-regression_WIP.qmd), we can then measure the effect of the independent variable on the dependent variable while holding the confounding variable constant.\nWe also call this **controlling for** the confounder.\n\nThe main limitation of this approach is that we can only control for the variables that we have measured.\nThis puts an even greater burden on having a solid theoretical foundation for the relationships we test.\n\n## Triangulation\n\nIt is generally the case that we cannot determine causality from a single study, and need to build a body of evidence from multiple studies that point in the same direction.\nAn experiment might be able to establish causality, but only for a very specific context and short term effects.\nAn observational study might be able to find correlations that are consistent across many different contexts and time periods, but it cannot establish causality.\nTo build theory on complicated issues such as the relationship between screen time and well-being, we need to combine both types of studies.\n\nFor example, in their study on screen time and well-being, Twenge and Campbell presented valuable data on the correlation between these variables, but emphasized that based on their data they cannot determine the direction of the relationship [@twenge18, p. 281].\nSo to build a stronger case for the causal relationship, they also discussed literature that looked at this from other angles.\nOne study they mention is an experiment that showed that people that took a one week break from Facebook showed higher well-being than people that continued using Facebook [@tromholt16].\nBy itself this experiment does not prove that the problem is screen time, but if multiple similar experiments show that manipulating different forms of screen time has a positive effect on well-being, it strengthens the argument that the correlation between screen time and well-being is due to a causal relationship.\n\n\n\n\n<!-- An an example, consider the *agenda-setting theory* [@mccombs72], which posits that the media influence the public's perception of the importance of issues.\nInitially, the primary evidence for this theory came from a study that showed a correlation between the amount of media coverage of an issue and the public's perception of its importance.\nAt the time, news media were one of the few sources of information for the public, so *on theoretical grounds* it was reasonable to assume that the media were the cause of the public's perception.\nBut today, with the rise of social media and other information sources, the relationship between media coverage and public opinion is more complex.\nTo test the theory today, we would need to control for other sources of information, such as social media, to see if the relationship between media coverage and public opinion still holds.\n\nThis also emphasizes that statistics are never a substitute for theory.\nStatistics allow us to build stronger empirical evidence for our theories, but we need theory to guide us in what to measure and how to interpret the results. -->\n",
    "supporting": [
      "causality_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}