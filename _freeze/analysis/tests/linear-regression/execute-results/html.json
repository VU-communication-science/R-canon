{
  "hash": "df9b1be09043920879b8e3749be2d0ea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Linear regression\nsubtitle: \"Predicting one variable from another\"\nformat:\n  html:\n    df-print: kable\n---\n\n\n\n\n# What is regression analysis?\n\nRegression analysis is a statistical technique for *predicting* a `dependent variable` (DV) based on one or more `independent variables` (IVs)\nWe use *predict* in a statistical sense, meaning that we are estimating how well we can guess the dependent variable given knowledge of the independent variable(s).\nThe reason we want to do this is usually not to make actual predictions, but to understand the world better.\nIf we can model how a variable is predicted by one or more other variables, we can make inferences about the relationship between these variables.\n\nRegression analysis allows us to test hypotheses of the sort:\n\n* Older people have more trust in journalists\n* People on the political left have more trust in journalists than people on the political right\n\nIndividually, you could test these hypotheses with a correlation test and a t-test, respectively.\nRegression analysis can do both these things in the same model.\nBut **more importantly**, it can measure the effect of **multiple** independent variables at once, and control for relations between the independent variables.\nIf we know that younger people are more likely to be on the political left, then in order to test the effect of political orientation on trust, we need to somehow account for age.\n\nIn this tutorial we first discuss how regression analysis works under the hood. \nWe won't go deep into the math, but a broad understanding of the regression equation and the $R^2$ value is helpful for understanding when and how to use regression analysis.\nSecond, we'll show you how to use regression analysis in practice, where we go over a number of examples.\nFinally, we'll talk about when you can use regression analysis, and what the assumptions are that you need to meet.\n\n\n# How does it work? \n\nWhile you don't need to understand the math behind regression analysis to use it, it is helpful to have a basic understanding of how it works.\nIn **simple linear regression** we have a dependent variable `Y` and an independent variable `X`, and we want to predict `Y` based on `X`.\nThe goal is to find a mathematical formula that best describes their relationship.\nYou can visually think of this as a scatterplot, where we draw a `regression line`  such that the distance between the line and the data points is as small as possible.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-1-1.png){width=100%}\n:::\n:::\n\n\n\n\nThis visualization corresponds to the **regression equation**.\nWe've coloured the different parts of the equation according to the colours used in the visualization. \n\n\n$$ \\Large  \\color{Green} Y_i = \\color{Blue} b_0 + b_1 X_i + \\color{Red} \\epsilon_i $$\n\n* $\\color{Green} Y_i$ represents the real values of the dependent variable ($Y_1, Y_2, ... Y_n$) that we want to predict.\n* $\\color{Blue} b_0 + b_1 X_i$ represents our prediction of y. It has two coefficients: \n  * **intercept** $\\color{Blue} b_0$: The value of Y when X is zero.\n  * **slope** $\\color{Blue} b_1$: How much Y changes for every unit increase in X.\n* $\\color{Red} \\epsilon_i$ represents the **residual**. This is the distance between the <span style=\"color:blue;\">predicted</span> and the <span style=\"color:green\">real</span> value of Y.[^error]\n\n[^error]: For example, at $X = 10$, the blue line predicting Y passes through the point $Y = 10.45$.\nHowever, the real value of Y at this point is 8.66. \nSo the residual $\\epsilon_i$ at this point is $8.66 - 10.45 = -1.79$.\n\nBased on this equation, we can learn two important things about the relationship between X and Y:\n\n1. **The effect of X on Y**: The slope $b_1$ tells us how much Y changes for every unit increase in X.\n2. **The model fit**: Based on the residual $\\epsilon_i$ we can calculate how well the model fits the data.\n\n### The effect of X on Y\n\nBased on the coefficients of the regression model, we can make inferences about the relationship between the variables.\nIn the example, we see that there is a positive relationship between `X` and `Y`, because the slope is positive.\nIf X goes up, then Y tends to go up as well. \nThis relation is expressed by the coefficient $b_1$, which in our example is 0.95.\nFor every unit increase in X, Y increases by 0.95.\n\nWe can also interpret the intercept ($b_0$), which is the value of Y when X is zero.\nHowever, this is not always meaningful.\nFor example, if the independent variable is `age`, then the intercept would be the predicted value of Y for a person that is 0 years old.\nThis is why the intercept is often not interpreted. \n\n### Model fit\n\nBased on the coefficient of the independent variable we can say something about the relation between X and Y.\nHowever, this is not the full story.\nBased on the coefficients we can make a prediction of Y, but we don't yet know how good this prediction is.\nThis is why we also report the **coefficent of determination** ($R^2$), which tells us how well the model fits the data.\n\nWe can calculate this based on the residual $\\epsilon_i$.\nThe residuals tell us how bad our prediction is for each data point.\nTo get a single score for how bad our prediction is for all data points, we can calculate the **residual sum of squares** ($SS_R$).\nAs the name implies, we take the sum of the squared residuals.\n\n$$  SS_R = \\large \\sum_{i=1}^{n} {\\color{red} \\epsilon_i^2} $$\n\nYou can think of the $SS_R$ as the variance of the real values of Y around the predicted values of Y.\nIf our model is perfect, then the $SS_R$ would be zero. \nBut how do we know how not-perfect our model is? how high is too high?\nFor this, we can look at the **total sum of squares** ($SS_T$), which is the sum of the squared differences between the real values of Y and the mean of Y.\n\n$$ SS_T = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2 $$\n\nThe $SS_T$ is the **total variance** in Y.\nNotice that the calculation is similar to the $SS_R$, but instead of the predicted values of Y, we use the mean of Y.\nSo now we can use this to see how much better our prediction is than just using the mean of Y.\nBy subtracting the variance that we did not explain ($SS_R$) from the total variance ($SS_T$), we get the variance that we did explain, called the **model sum of squares** ($SS_M$).\n$$ SS_M = SS_T - SS_R $$\n\nAnd now we can calculate the $R^2$ value: the proportion of the total variance that has been explained by our model.\n$$ R^2 = \\frac{SS_M}{SS_T} $$\n\nThe $R^2$ is a value between 0 and 1, that tells us how much of the variance in the dependent variable is explained by the independent variable(s).\nIn our example the $R^2$ is 0.79, which means that 79% of the variance in Y can be explained by the variance in X.\nThis is **very high**, especially for social science data.\nWhen we're trying to predict human behaviour, we're usually already happy if we can explain a few percent of the variance.\n\n::: {.callout-tip title=\"Interactive widget for building intuition\" collapse=\"true\"}\n\nTry for yourself how the regression line changes when you change the values of the intercept and slope!\nThe goal is to get the lowest possible SSE (and the highest possible R2), which means that the line is as close as possible to the data points.\n\nNote that because you're manually tweaking the intercept and slope, you might actually create a model that is worse than the mean of Y, which is why you can get a negative $R^2$.\nIn a regression model this is not possible, because it will always find a model that is at least as good as the mean of Y.\n\n\n\n\n\n```{ojs}\n//| echo: false\nviewof intercept = Inputs.range(\n  [-8, 6], \n  {value: 3, step: 0.001, label: \"b0 Intercept\"}\n)\nviewof slope = Inputs.range(\n  [0.5,1.8], \n  {value: 0.5, step: 0.001, label: \"b1 Slope\"}\n)\n```\n\n```{ojs}\n//| echo: false\nx_line = Array.from({ length: 100 }, (_, i) => i-5);\ny_line = x_line.map(x => slope * x + intercept);\nmutable SSR = 0\nmutable R2 = 0\n\nSST = {\n  let sst = 0\n  let mean = 0\n  y.forEach((d, i) => {\n    mean += y[i] / y.length\n  })\n  y.forEach((d, i) => {\n    sst += Math.pow(y[i] - mean, 2)\n  })\n  return(Number(sst.toFixed(2)))\n}\n\nupdate_SSR = {\n  let ssr = 0\n  x.forEach((d, i) => {\n    ssr += Math.pow(y[i] - (slope * x[i] + intercept), 2)\n  })\n  mutable SSR = Number(ssr.toFixed(2))\n  mutable R2 = Number((1 - SSR / SST).toFixed(2))\n}\n\n// Now, plot the regression line\nPlot.plot({\n  marks: [\n    Plot.lineY(y_line, {x: x_line, stroke: \"blue\"}),\n    Plot.dot(x, {x: x, y: y, fill: \"darkgreen\"}),\n    Plot.text([{intercept,slope}], {x: 10, y: 3, text: d => `prediction = ${d.intercept.toFixed(2)} + ${d.slope.toFixed(2)} * X`, fill: \"black\", fontSize: 16, fill: \"blue\", textAnchor: \"start\"}),\n    Plot.text([{SSR, R2}], {x: 10, y: 0, text: d => `R2 = ${d.R2} = ${(SST - d.SSR).toFixed(2)} / ${SST}`, fill: \"black\", fontSize: 16, fill: \"black\", textAnchor: \"start\"})\n  ],\n  x: {\n    label: \"X\",\n    domain: [-5, 20]\n  },\n  y: {\n    label: \"Y\",\n    domain: [-5,22]\n  },\n  height: 300\n})\n\n```\n\n\n\n\n::: {.callout-tip title=\"Spoiler\" collapse=\"true\"}\n\nThe optimal values for the intercept and slope are $b_0 = 0.930$ and $b_1 = 0.952$.\nThis should give you an $R^2$ of $0.79$.\n\n\n:::\n:::\n\n\n\n# How to use it\n\nWe'll show you how to use regression analysis in several steps:\n\n* First, we'll look at **simple linear regression** with a single numeric independent variable.\n* Then, we'll perform a **multiple regression** with two numeric independent variables.\n* In the third step we'll show you how to use a **categorical independent variable** in the regression model.\n* We conclude with a **multiple regression** model that includes both numeric and categorical independent variables.\n\nWe'll be using our standard (simulated) practice data, and a bit of `tidyverse` to load and clean the data.\nWe'll also be using the `sjPlot` package, which can create nice tables and plots of regression models.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(sjPlot)\n\nd <- read_csv(\"https://tinyurl.com/R-practice-data\") |>\n        mutate(age = if_else(age > 100, 2024 - age, age)) |>\n        select(age, political_orientation, trust_t1, \n               news_consumption = `news consumption`)\n\nd\n```\n:::\n\n\n\n\n\n## Simple linear regression \n\nWe start with the example of predicting `trust` in journalists based on `age`.\nFor this we'll be using the `trust_t1`[^1] variable as the dependent variable, and the `age` variable as the independent variable.\nWe can use the `lm()` function to create a linear regression model.\nInside the `lm()` function we specify the formula for the regression, which has the format `dependent ~ independent`. \n\n[^1]: We use the `trust_t1` variable here, because for the current analysis we are not interested in the effect of the experimental group. Since `trust_t1` is measured before the experiment, it is not influenced by the experiment.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(trust_t1 ~ age, data = d) \n```\n:::\n\n\n\n\nThe *standard* way to inspect the results is using the `summary(m)` function.\nThis gives you all the information you need, but not in a very nice and readable format.\nWe will therefore be using the `tab_model()` function from the `sjPlot` package to get a nice (APA ready) table of the results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab_model(m)\n```\n\n::: {.cell-output-display  html-table-processing=none}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">trust_t1</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">2.21</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.98&nbsp;&ndash;&nbsp;2.43</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">age</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.04</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.04&nbsp;&ndash;&nbsp;0.05</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">595</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.281 / 0.280</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n\n\n::: {.callout-caution title=\"Backup plan if tab_model doesn't work\" collapse=\"true\"}\n\nThe `tab_model` function by default shows the regression table in your `viewer` pane.\nIf this for some reason doesn't work, you can also use the `use.viewer=F` argument to show the table in your default web browser.\nThis has the additional benefit that you can directly copy-paste the table into most text editors.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab_model(m, use.viewer=F)\n```\n:::\n\n\n\n:::\n\nThe output of the regression analysis gives us values for two coefficients: the **(intercept)** and the **age** variable (like $b_0$ and $b_1$ in the regression equation).\nWe get the coefficient **estimates** and **p-values** that tell us if these coefficients are statistically significant.\nWith `tab_model` we also get the **confidence interval**. \n\n* The coefficient for the `intercept` tells us the predicted value of `trust_t1` when `age` is zero.\nThis is not very meaningful in this case, because we don't have any people with an age of zero in our data.\n* The coefficient for `age` tells us how much `trust_t1` changes for every *unit increase* in `age`. Our `age` variable is in years, so the coefficient `0.04` tells us that for every year of age, trust in journalists increases on average by 0.04 points.\n  * The p-value of `< 0.001` tells us that this effect is statistically significant.\n  * The confidence interval tells us that we are 95% confident that the true effect of `age` on `trust_t1` is between 0.04 and 0.05.\n\nWe also get the $R^2$ value, which tells us how well the model fits the data.\nHere we see that the $R^2$ is 0.281, which means that 28.1% of the variance in `trust_t1` can be explained by the variance in `age`, which is huge.\n\nIn conclusion, this model shows that (in our simulated data) there is a strong effect of `age` on `trust_t1`.\n\n  \n## Multiple regression\n\nIn the previous example we used a single independent variable (`age`) to predict the dependent variable.\nNow we'll add a second numeric independent variable (`news_consumption`) to the model.\nThis is called **multiple regression**, and it allows us to test the effect of multiple variables on the dependent variable at the same time.\nThe formula for using multiple independent variables is: `dependent ~ independent1 + independent2 + ...`.\n\nAs we mentioned earlier, by including both variables in the model we can test the effect of `news_consumption` on `trust` while controlling for `age`, and vice versa.\nTo better investigate the effects of controlling, it is good practise to show multiple versions of the model side by side. \nWith the `tab_model()` function we can do this by simply providing both models as arguments.\nAlso, if we want to compare effect sizes, it helps to include the standardized coefficients in the table, which we can do with `show.std = T` in the `tab_model()` function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 <- lm(trust_t1 ~ news_consumption, data = d)\nm2 <- lm(trust_t1 ~ news_consumption + age, data = d)\ntab_model(m1, m2, show.std = T)\n```\n\n::: {.cell-output-display  html-table-processing=none}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"5\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">trust_t1</th>\n<th colspan=\"5\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">trust_t1</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">std. Beta</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">standardized CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  col7\">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  col8\">std. Beta</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  col9\">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  0\">standardized CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  1\">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.58</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.00</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.31&nbsp;&ndash;&nbsp;3.85</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.08&nbsp;&ndash;&nbsp;0.08</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\">2.17</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col8\">0.00</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col9\">1.87&nbsp;&ndash;&nbsp;2.46</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  0\">&#45;0.07&nbsp;&ndash;&nbsp;0.07</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  1\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">news consumption</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.10</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.00&nbsp;&ndash;&nbsp;0.06</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.02&nbsp;&ndash;&nbsp;0.18</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.020</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\">0.00</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col8\">0.01</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col9\">&#45;0.02&nbsp;&ndash;&nbsp;0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  0\">&#45;0.06&nbsp;&ndash;&nbsp;0.08</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  1\">0.699</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">age</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\">0.04</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col8\">0.53</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col9\">0.03&nbsp;&ndash;&nbsp;0.05</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  0\">0.46&nbsp;&ndash;&nbsp;0.60</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  1\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"5\">600</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"5\">595</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"5\">0.009 / 0.007</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"5\">0.281 / 0.279</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n\n\nIf we look at the first model, we see a significant effect of `news_consumption` on `trust_t1` ($b = 0.03$, $p < 0.05$). \nThe $R^2$ value is very low (0.009): only 0.9% of the variance in `trust_t1` can be explained by `news_consumption`.\nStill, if this was all the information we had, we might have concluded that there is at least a small effect of `news_consumption` on `trust_t1`.\n\nBut when we include the `age` variable in the model, we see that the effect of `news_consumption` is no longer significant ($p = 0.699$).\nAlso note that the $R^2$ value (0.281) is identical to when we only included the `age` variable (in the simple linear regression).\nSo this suggests that the effect of `news_consumption` on `trust_t1` is actually explained by the effect of `age`.\n\nWhat we're witnessing here is a form of **confounding**.\nThere is a relation between the `news_consumption` and `trust_t1` variables, but this relation is likely *spurious*.\n`age` has an effect on both `news_consumption` and `trust_t1`: older people consume more news, and have more trust in journalists.\nBy controlling for `age`, the spurious relation between `news_consumption` and `trust_t1` disappears.\nFor a more detailed explanation of confounding, see the [confounding tutorial](../tests/confounding.qmd).\n\nLet's take a second to **interpret the std. Beta values**.\nThese are the coefficients of the model if all variables are standardized.\nSo the regular coefficient of `age` tells us that for every year increase in `age`, `trust_t1` increases by 0.04 points.\nFrom this value it's hard to say how big this effect is, and how it compares to other effects.\nFor instance, in the first model we saw that `news_consumption` had a coefficient of 0.10, which would seem larger than the coefficient of `age`.\nBut this is not a fair comparison, because the `age` variable is in years, and the `news_consumption` variable is in hours per day.\nBy standardizing the variables, we can compare the effects in terms of standard deviations instead.\nFor every standard deviation increase in `age`, `trust_t1` increases by 0.53 standard deviations.\nFor every standard deviation increase in `news_consumption`, `trust_t1` only increases by 0.10 standard deviations, and after controlling for `age` only by 0.01 standard deviations (which is also no longer significant).\n\nThis is also a good time to talk about the **$R^2$ adjusted** value.\nThis is the $R^2$ value, but corrected for the number of independent variables in the model.\nThis is important, because the $R^2$ value will always increase when you add more independent variables to the model, because this adds an additional parameter that it can use to fit the data (in our case the increase was just so small that it didn't show after rounding).\nThe $R^2$ adjusted value corrects for this, which is why it's actually lower in our model with both `age` and `news_consumption` than in the model with just `age`.\nIn practice, the $R^2$ adjusted value is very similar to the $R^2$ value, and the regular $R^2$ is then often reported.\nBut always check the $R^2$ adjusted value if you're adding multiple independent variables to the model to see if you're not just adding noise.\n\n## Categorical IVs\n\nThe **independent variable** in a regression model can also be categorical.\nThis allows us to include categorical variables in the model, similar to the t-test and ANOVA.\nLet's see what this looks like when we predict `trust_t1` based on the `political_orientation` variable, which has the values `left`, `right` and `center`.\n\nWe cannot directly use the values \"left\", \"right\" and \"center\" in the regression equation, but we create so-called **dummy variables**.\nA dummy variable is a *binary* variable (i.e., it can only take the values `0` and `1`) that represents whether something is the case or not.\nSo if we want to test whether people that lean to the left have more trust in journalists, we can create a dummy variable that is `1` for people with orientation `left`, and `0` for all other people (i.e., `right` and `center`).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd = mutate(d, political_left = if_else(political_orientation == \"left\", 1, 0))\n\nselect(d, political_orientation, political_left) ## check the result\n```\n:::\n\n\n\n\nNow we can use this variable in the regression model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(trust_t1 ~ political_left, data = d)\n\ntab_model(m)\n```\n\n::: {.cell-output-display  html-table-processing=none}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">trust_t1</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.81</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.71&nbsp;&ndash;&nbsp;3.91</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">political left</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.22</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.05&nbsp;&ndash;&nbsp;0.39</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.012</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">600</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.010 / 0.009</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n\n\nThe interpretation of the results is almost identical to the previous example with the `age` variable.\nThe only thing we need to keep in mind is that the `political_left` variable is binary, so it can only take the values `0` and `1`.\nAccordingly, we don't say that the trust in journalists increases by `0.21` points for every unit increase in `political_left`.\nInstead, we just say that people on the political left (`political_left = 1`) have on average a trust score that is `0.21` points higher than people that are not on the political left (`political_left = 0`).\n\n<!-- Note that in this case we can actually interpret the **intercept**.\nThe intercept is the predicted value of the dependent variable when the independent variable is zero.\nSo in this case, the intercept (`3.83`) is the predicted trust in journalists for people that are NOT `political_left`. -->\n\n### Categories with more than two levels\n\nIn the previous example we created the dummy variable ourselves, but we can also let R do this for us.\nThis is especially convenient if we have more than two categories.\nIn addition, this also has the benefit that `lm` *knows* that the variable is categorical, which enables us (among other things) to visualize the model propertly.\n\nWe first need to make sure that our variable is of the `factor` type[^3].\nThe difference between a `character` and a `factor` is that in a `factor` we explicitly tell R what the categories are, and what the *order* of the categories is.\nThe order is important, because the first category will be the **reference category** (more on this later) in the regression model.\nTo transform our `political_orientation` variable to a `factor` we can use the `factor()` function, and provide the `levels` argument to specify the order of the categories.\n\n[^3]: You *can* also use a `character` variable directly, but the `factor` type is more appropriate for categorical variables. It also let's you specify the order of the categories, which is important for regression analysis because it let's you specify the reference category.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- mutate(d, political_orientation = factor(political_orientation, levels=c('center','left','right')))\n```\n:::\n\n\n\n\nNow, when we use the `political_orientation` variable in the regression model, R will automatically create the dummy variables for us.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(trust_t1 ~ political_orientation, data = d)\ntab_model(m)\n```\n\n::: {.cell-output-display  html-table-processing=none}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">trust_t1</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.90</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.76&nbsp;&ndash;&nbsp;4.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">political orientation<br>[left]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.14</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.06&nbsp;&ndash;&nbsp;0.33</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.175</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">political orientation<br>[right]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.17</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.36&nbsp;&ndash;&nbsp;0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.094</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">600</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.015 / 0.012</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n\n\nNotice that there are now two coefficients for the `political_orientation` variable: one for `left` and one for `right`.\nWhy 2, and not 3?\nThis is because when we have a categorical variable with `k` levels, we only need `k-1` coefficients to represent all the levels.\nIf we know that a person is not in the `left` category, and not in the `right` category, then we know that the person must be in the `center` category.\nYou can see how this works in the following table:\n\n| political_orientation | is_left | is_right |\n|-----------------------|---------|----------|\n| left                  | 1       | 0        |\n| right                 | 0       | 1        |\n| center                | 0       | 0        |\n\nThe `center` category is now the **reference category**.\nThat is, the `center` category is the category that all other categories are compared to.\nSo the `political_orientation [left]` coefficient tells us that `trust` is for people in the `left` category is 0.12 points higher than for people in the `center` category, but this difference is not significant (p = 0.220)\nThe `political_orientation [right]` coefficient tells us that `trust` is for people in the `right` category is 0.17 points lower than for people in the `center` category, but this difference is also not significant (p = 0.100).\n\n::: {.callout-note title=\"Determining what reference category to use\" collapse=\"true\"}\n\nIn the current model we cannot directly compare the `left` and `right` categories, because the `center` category is the reference category.\nThis makes it important to choose a reference category that makes sense for your research question.\nTo determine the reference category, you can change the order of the levels in the `factor()` function.\nThe first level will always be the reference category.\nSo in the following example, the `left` category is the reference category.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2 <- mutate(d, political_orientation = factor(political_orientation, levels=c('left','center','right')))\n\nlm(trust_t1 ~ political_orientation, data = d2) |>\n    tab_model()\n```\n\n::: {.cell-output-display  html-table-processing=none}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">trust_t1</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">4.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.89&nbsp;&ndash;&nbsp;4.17</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">political orientation<br>[center]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.14</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.33&nbsp;&ndash;&nbsp;0.06</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.175</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">political orientation<br>[right]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.30</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.50&nbsp;&ndash;&nbsp;-0.11</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.003</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">600</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.015 / 0.012</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n\n:::\n\n## Numeric and categorical IVs\n\nYou've already seen how multiple regression works, AND how to include categorical variables in the model.\nLet's conclude with a model that includes both numeric and categorical variables.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 <- lm(trust_t1 ~ political_orientation, data = d)\nm2 <- lm(trust_t1 ~ political_orientation + age, data = d)\n\ntab_model(m1, m2)\n```\n\n::: {.cell-output-display  html-table-processing=none}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">trust_t1</th>\n<th colspan=\"3\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">trust_t1</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  col7\">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.90</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.76&nbsp;&ndash;&nbsp;4.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">2.02</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.79&nbsp;&ndash;&nbsp;2.26</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">political orientation<br>[left]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.14</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.06&nbsp;&ndash;&nbsp;0.33</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.175</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.27</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.11&nbsp;&ndash;&nbsp;0.43</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">political orientation<br>[right]</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.17</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.36&nbsp;&ndash;&nbsp;0.03</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.094</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.42</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.58&nbsp;&ndash;&nbsp;-0.26</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">age</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.05</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.04&nbsp;&ndash;&nbsp;0.05</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  col7\"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">600</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">595</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.015 / 0.012</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.354 / 0.351</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n\n\nIn the first model, we did not see any effect of having a `left` or `right` political orientation compared to the `center` (reference) category.\nBut once we include the `age` variable in the model, both the `left` and `right` categories do show a significant effect on `trust`!\n\nThis is again a form of **confounding**.\nHowever, this time the confounder `age` **suppresses** the effect.\nYounger people have less trust in journalists, but are also more likely to be on the political left.\nTherefore, if we do not control for `age`, the positive effect of being on the political left is suppressed by the negative effect of being younger.\n\nIf this sounds confusing, don't worry, because you're in good company!\nThinking about how the effect of independent variables *on* the dependent variable depends on the relation *between* the independent variables can make your head spin.\nBut for better or worse, this is how the world works, and this is why we need sophisticated statistical tools to help us understand it.\n\n# When can you use it?\n\nRegression analysis is a very versatile tool that can be used in many different situations.\nHowever, there are some assumptions that need to be met in order to use it correctly.\nThe most important assumptions are:\n\n1. **Linearity**: The relationship between the independent and dependent variable(s) should be linear.\n2. **Normality**: The residuals should be normally distributed.\n3. **No outliers**: Extreme values can have a large effect on the model.\n3. **Homoscedasticity**: The variance of the residuals should be constant.\n4. **Independence**: The observations should be independent of each other.\n5. **No multicollinearity**: In **multiple regression**, the independent variables should not be too highly correlated with each other.\n\nWhether these assumptions are met is generally a grey area.\nIt's not a matter of \"yes\" or \"no\", but rather a matter of \"to what extent\".\nLet's discuss some strategies for checking these assumptions.\n\n### Linearity\n\nRemember that we are trying to fit a straight line through the data.\nIf the relationship between the independent and dependent variable is not linear, then the model will not fit the data well.\nTo check this assumption, you can plot the data and see if a straight line is a good fit.\nWith the `plot_model()` function from the `sjPlot` package you can plot the predicted (`type = \"pred\"`) values for an independent variable (`terms = \"age\"`) on top of the data points (`show.data = T, jitter = T`[^jitter]).\n\n[^jitter]: Jitter is a small amount of noise that is added to the data points to prevent them from overlapping in case their values are the same.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(trust_t1 ~ age, data = d)\n\nplot_model(m, type = \"pred\", terms = \"age\", \n              show.data = T, jitter = T)\n```\n:::\n\n\n\n\nIf you check this plot for our practice data, you'll see that the relationship between `age` and `trust_t1` is indeed linear.\nLet's instead look at an example where the relationship is not linear.\nIn the following plot the relation between `age` and `happiness` is quadratic, following a u-shape.\nYounger and older people tend to be happier than middle-aged people.\nA regular linear model (left) will not be able to capture this relationship well, and will therefore not fit the data well.\nOn the right we fit a regression line to the actual relation. \n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n### Normality\n\nThe residuals of the model should be normally distributed.\nIf you look at the figure of the regression model at the top of this tutorial, you can see how the residuals (the red lines) are distributed around the regression line.\nThis distribution should be normal.\n\nYou can to some extend see whether this is the case using the same plot as above. \nBut there are also special diagnostics plots that can help you with this.\nWith the `plot_model()` function you can plot the residuals (`type = \"diag\"`) of the model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots <- plot_model(m, type = \"diag\")\n```\n:::\n\n\n\n\nThis creates a series of plots that show the residuals in different ways.\nWhat diagnostics plots are produced depends on the type of model you have.\nFor regression we get three plots:\n\n* `diag_plots[[1]]`: Non-normality of residuals and outliers\n* `diag_plots[[2]]`: Non-normality of outliers\n* `diag_plots[[3]]:` Homoscedasticity\n\nLet's first focus on the second plot, which shows just the distribution of the residuals.\nThe coloured area is a smoothed density plot of the residuals, and the blue line shows a proper bell curve for reference.\nIf there is a clear difference between the two, then the residuals are not normally distributed.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots = plot_model(m, type=\"diag\")\ndiag_plots[[2]]\n```\n\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### No outliers in the residuals\n\nOutliers can have a large effect on the model.\nUsually outliers in the residuals are related to outliers in the variables, which can be detected when [inspecting and cleaning](../tests/data-cleaning.qmd) the data.\nFor detecting outliers in the residuals, the **Non-normality of residuals and outliers** plot can be helpful.\nIf there are outliers, they will show up as points that are far away from the line.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots[[1]]\n```\n\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Homoscedasticity\n\nThe residuals should have a constant variance.\nWhat we mean by this is that among the fitted values in the model, the variance of the residuals should not systematically increase or decrease.\nFor instance, if in our model of **age** predicting **trust** the residuals are very small for young people, and very large for old people, then the model is not homoscedastic.\n\nThis can be checked with the **Homoscedasticity** plot.\nIf the residuals are homoscedastic, then the residuals should be randomly distributed around the horizontal line.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag_plots[[3]]\n```\n\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n\nNow let's look at an example where the variance of the residuals systematically increases with the fitted values.\nSo the higher our prediction of `trust_t1`, the larger the residuals.\nWe then say that the residuals are **heteroscedastic**.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](linear-regression_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Independence\n\nIndependence means that the residuals should not be related to each other.\nThere are many ways in which this assumption can be violated.\n\nOne common case is data over time.\nIf you're measuring something over time, like a person's number of social media followers, then it is very common that every observations is highly dependent on the previous observation.\nThis is called **autocorrelation** (the variable correlates with itself).\nIn order to use these data in a regression model, you need to use special techniques that take this autocorrelation into account. \nThis area of statistics is called **time series analysis**.\n\nAnother common case is that your observations are nested within groups.\nFor example, say your measuring the relation between social media use and happiness, and you gathered data from multiple countries.\nObservations nested in countries are not independent, because people within the same country tend to be more similar to each other than people from different countries.\nSome countries are overall happier, and some countries have more social media use.\nCountry therefore could act as a **confounder**.\nTo reduce the dependency between the residuals, you could control for countries using dummy variables, or use more advanced techniques like **multilevel models**.\n\n### No multicolinearity\n\nMulticolinearity is a situation where two or more independent variables are highly correlated with each other.\nThis can cause problems in the model, because the model cannot distinguish between the effects of the two variables.\nTo check for multicolinearity, you can look at the **correlation matrix** of the independent variables (see the [correlation tutorial](../tests/correlation.qmd)).\nA rough rule of thumb is that if the correlation between two variables is higher than 0.5 you should be cautious, and if it's higher than 0.8 you should be very worried.\n\nYou should try to avoid high correlations between independent variables where possible. \nIf you have multiple variables that are highly correlated, they might be measuring a common factor, so you could consider combining them into a single variable (see the [scale construction](../techniques/scale-construction.qmd) tutorial).\n\n::: {.callout-note title=\"The variance inflation factor\" collapse=\"true\"}\n\nA more appropriate way to check for multicolinearity is to use the **variance inflation factor** (VIF).\nLooking just at correlations is actually not good enough.\nThe more accurate definition of multicolinearity is that one independent variable can be predicted by the other independent variables in the model.\nSo looking at bi-variate correlations is not enough, because the prediction of one variable can be spread out over multiple other variables.\n\nIn R we can calculate the VIF of the variables in a model with the `vif()` function from the `car` package.\nThis returns the VIF score for each independent variable.\nIf the VIF is 1 or lower, we don't have to worry about multicolinearity.\nIf it's higher than 5, we very likely have a problem.\nIf it's between 1 and 5, it's a grey area, and you (as always) have to use your judgement.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: carData\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'car'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:purrr':\n\n    some\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    recode\n```\n\n\n:::\n\n```{.r .cell-code}\nm <- lm(trust_t1 ~ age + news_consumption, data = d)\nvif(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             age news_consumption \n        1.024159         1.024159 \n```\n\n\n:::\n:::\n\n\n\n:::",
    "supporting": [
      "linear-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script type=\"ojs-define\">\n{\"contents\":[{\"name\":\"x\",\"value\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]},{\"name\":\"y\",\"value\":[-0.6814,1.3095,7.6761,4.2115,5.3879,11.1452,8.3827,4.2048,6.9394,8.663,14.6722,13.0794,14.2023,14.332,13.3325,21.3607,18.4936,12.1001,21.1041,18.5816]}]}\n</script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}