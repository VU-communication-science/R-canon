{
  "hash": "ac5bcc148f0bec10d1ba4e09db1aacbe",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Textual data\n---\n\n\n\n\n# Introduction\n\nThe goal of this tutorial is to get you acquainted with basic string handling in R. \nA large part of this uses the `stringr` included in the [Tidyverse](https://www.tidyverse.org/). \nSee also chapter 14 of [R for Data Science](http://r4ds.had.co.nz/) and the [stringr cheat sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/strings.pdf)\n\nNote that 'string' is not an official word in R (which uses `character` to denote textual data), \nbut since it's the word used in most documentations I will also use `strings` to refer to objects containing textual data. (AFAIK, the name originates from seeing a text as a `string` or sequence of characters)\n\n\n# String basics\n\nThe package `stringr` has a number of functions for dealing with strings. For example, `str_length` gives the length of the string:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nstr_length(\"john\")\n```\n:::\n\n\nAs usual, these functions can be applied to a vector or column of strings directly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_length(c(\"john\", \"mary\"))\n\ndf = data.frame(name=c(\"john\", \"mary\"))\nstr_length(df$name)\n\nmutate(df, len=str_length(name))\n```\n:::\n\n\nOther useful functions are `str_to_lower`, `str_to_upper` (which mostly mimic built-in `tolower` and `toupper`), and `str_to_title`.\n\n## Combining strings\n\nTo combine two strings, you can use `str_c` (which is equivalent to built-in `paste0`):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_c(\"john\", \"mary\")\nstr_c(\"john\", \"mary\", sep = \" & \")\n```\n:::\n\n\nIt can also work of longer vectors, where shorter vectors are repeated as needed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames = c(\"john\", \"mary\")\nstr_c(\"Hello, \", names)\n```\n:::\n\n\nFinally, you can also ask it to *collapse* longer vectors after the initial pasting:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_c(names, collapse=\" & \")\nstr_c(\"Hello, \", names, collapse=\" and \")\n```\n:::\n\n\n## Subsetting strings\n\nTo take a fixed subset of a string, you can use str_sub. This can be useful, for example, to strip the time part off dates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndates = c(\"2019-04-01T12:00\", \"2012-07-29 01:12\")\nstr_sub(dates, start = 1, end = 10)\n```\n:::\n\n\nYou can also replace a substring, for example to make sure the 'T' notation is used in the dates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_sub(dates, start=11, end=11) = \"T\"\ndates\n```\n:::\n\n\n# Regular expressions\n\nThe example above showed how to extract or replace a fixed part of a string.\nIn many cases, however, we want to find, replace, or extract certain patterns in a string\n(for example, dates, email addresses, or html tags). \n\nFor this purpose, R (like most other languages) use *regular expressions*, a very powerful way to write\ntext patterns. Although a full overview of regular expressions is beyond the scope of this handout \n(there's full books written on the subject!), below are some examples of what you can do.\n\nNote: To find a regular expression in a text, I use the `str_view` command, which is quite useful for designing/debugging expressions:\n(You miught have to install the htmlwidgets package for this to work)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# if needed: install.packages(\"htmlwidgets\")\ntxt = c(\"Hi, I'm Bob\", \"my email address  is  Bob@example.com\", \"A #hashtag for the #millenials\")\nstr_view(txt, \"my\") # literal text\nstr_view(txt, \"m.\") # . matches any character \nstr_view(txt, \"\\\\.\") # \\\\. matches an actual period (.)  \nstr_view(txt, \"\\\\w\") # \\w matches any alphanumeric character (including numbers)\nstr_view(txt, \"\\\\W\") # \\W matches any alphanumeric character \nstr_view(txt, \"[a-z]\") # [..] are character ranges, in this case, all lower caps letters \nstr_view(txt, \"[^a-z]\") # [^..] is a negated character range, in this case, all except lower caps letters \nstr_view(txt, \"\\\\ba\") # \\b matches word boundaries,this matches an a at the beginning of a word\n```\n:::\n\n\nThe different options above all match (a sequence of) individual characters.\nYou can also specify multiples of a character:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_view(txt, \"ad*\")   # * means an a followed by zero or more (d's, in this case)\n                       # and as many as possible (greedy)\nstr_view(txt, \"ad*?\")  # *? means zero or more (d's, in this case), but as few as needed (non-greedy)\nstr_view(txt, \"ad+\")   # + means one or more d's\nstr_view(txt, \"ad+?\")  # + means one or more d's, but as few as needed\nstr_view(txt, \"ad?\")   # ? means zero or one, i.e. an 'optional' match\nstr_view(txt, \"add?\")  # a single d, optionally followed by another d\nstr_view(txt, \"B.*m\")  # a B, followed by zero or more of any character \n                       # (and as many as possible), followed by an m\nstr_view(txt, \"B.*?m\") # a B, followed by zero or more of any character \n                       # (but as few as needed), followed by an m\n```\n:::\n\n\nThese elements can be combined to make fairly powerful patterns, such as for emails or introductions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_view(txt, \"\\\\w+@\\\\w+\\\\.\\\\w+\")  # matches (some) email addresses\nstr_view(txt, \"I'm \\\\w+\\\\b\")       # matches \"I'm XXX\" phrases\nstr_view(txt,  \"#\\\\w+\")            # matches #hashtags\n```\n:::\n\n\nNote, the email address pattern is far from complete, and will match addresses with subdomains, numbers, and many other possibilities. It turns out emails are surprisingly complex to match - but the pattern below should do pretty well for all but the most arcane addresses:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregex_email = regex(\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+\")\nstr_view(txt, regex_email)\n```\n:::\n\n\n\nFor more information, see the [relevant section of R4DS](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions), or one of the many available resources on regular expressions.\n\n## Finding patterns\n\nRegular expressions can be used e.g. to find rows containing a specific pattern. \nFor example, if we had a data frame containing the earlier texts, we can filter for rows containing an email address:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt = tibble(id=1:3, text=txt)\nt\nt |> filter(str_detect(text, regex_email))\n```\n:::\n\n\nYou can also `str_count` to count how many matches of a pattern are found in each text:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt |> mutate(n_hashtags = str_count(text, \"#\\\\w+\"))\n```\n:::\n\n\n## Replacing patterns\n\nYou can also use regular expressions to do find-an-replace. \nFor example, you can remove all punctionation, normalize whitespace, or redact email addresses:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt |> mutate(nopunct = str_replace_all(text, \"[^\\\\w ]\", \"\"),\n             normalized = str_replace_all(text, \"\\\\s+\", \" \"),\n             redacted = str_replace_all(text, \"\\\\w+@\", \"****@\"),\n             text = NULL)\n```\n:::\n\n\nNote the use of setting text to NULL as an alternative way to drop a column. \nIn `textr`, most functions have a `_all` variant which replaces/finds/extracts all matches, rather than just the first. \n\n## Extracting patterns\n\nBesides replacing patterns, it can also be useful to extract elements from a string, for example the email or hashtag:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregex_email = regex(\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+\")\nt |> mutate(email = str_extract(text, regex_email),\n             hashtag = str_extract(text, \"#\\\\w+\"))\n```\n:::\n\n\nNote that for the hashtag, it only extracted the first hit. You can use the `str_extract_all` function,\nbut since it can match zero, once, or more often per text, it returns a list containing all matches per row (or more correctly, per element of the input vector):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_extract_all(t$text, \"#\\\\w+\")\n```\n:::\n\n\nThe best way to deal with this in the context of a data frame is to use `unnest_longer` to turn the list into a long format.\nFirst, use mutate to create a column containing the lists (so this is a column which itself contains complex data)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhashtags = mutate(t, tags = str_extract_all(t$text, \"#\\\\w+\"))\nhashtags\n```\n:::\n\n\nAs you can see, the tags column has 'list' as its type, with the last row containing two elements. \nTo turn this into a more usable dataframe, we 'unnest' it into a 'long' format using `unnest_longer`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhashtags |> unnest_longer(\"tags\")\n```\n:::\n\n\nAs you can see, this creates a new data frame with one row per hash tag.\n\n# Splitting text\n\nSometimes, a single column contains multiple data points, e.g. separated with a comma. For example, suppose we have this data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd = tibble(person=c(\"Liam Jones\", \"Olivia Smith\"), books=c(\"The great Gatsby, To kill a Mockingbird\", \"Pride and Prejudice, 1984, Moby Dick\"))\nd\n```\n:::\n\n\nIf we want to split a column, in this case books, there are two good options. \n\n## Using separate\n\nFirst, if the number of elements is known and you can use multiple you can use `separate` to separate the column into seperate columns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd |> separate(person, into=c(\"firstname\", \"lastname\"), sep=\" \")\n```\n:::\n\n\nAs said, this is mostly useful if a column always contains a fixed number of data points that each have a distinct meaning, e.g. first and last name or city and state. \n\n##  Using str_split\n\nIf there is variable number of data points such as the list of books in the data set above, you can use str_split, which takes a regular expression argument to split the column:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd |> mutate(books = str_split(books, pattern=\",\"))\n```\n:::\n\n\nJust like above, this produces a column of type 'list' which contains multiple values per row. To normalize this, use `unnest_longer` as above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd |> mutate(books = str_split(books, pattern=\",\")) |> unnest_longer(books)\n```\n:::\n\n\nAs you can also see, the spaces around the book titles are not removed. You can fix this in two ways, either by changing the pattern to incluide optional whitespace (`\\\\w` is white space, `*` means the preceding element is optional and can be repeated); or by adding a `trimws` call afterwards:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd |> mutate(books = str_split(books, pattern=\"\\\\s*,\\\\s*\")) |> unnest_longer(books)\nd |> mutate(books = str_split(books, pattern=\",\")) |> unnest_longer(books) |> mutate(books=trimws(books))\n```\n:::\n\n\n\n# A note on Unicode, UTF-8 and Encoding\n\n*NOTE*: this code is not tested on non-western/non-linux computers, so let me know if anything does not work\n\nFinally, a short note about unicode and character encodings.\nAs with regular expressions, a full explanation of this topic is (well) beyond the scope of this tutorial.\nSee this [guide on unicode in R](https://kevinushey.github.io/blog/2018/02/21/string-encoding-and-r/)\nand the classic [What Every Programmer .. Needs To Know About Encodings ..](http://kunststube.net/encoding/) for some very useful information if you need to know more. \n\n## Background\n\nA fairly short version of the story is as follows: when computers were mostly dealing with English text, life was easy, as there are not a lot of different letters and they could easily assign each letter and some punctuation marks to a number below 128, so it could be stored as 7 bits. For example, A is number 65. This encoding was called 'ASCII'. \n\nIt turned out, however, that many people needed more than 26 letters, for example to write accented letters. For this reason, the 7 bits were expanded to 8, and many accented latin letters were added. This representation is called latin-1, also known as ISO-8859-1. \n\nOf course, many languages don't use the latin script, so other 8-bit encodings were invented to deal with Cyrillic, Arabic, and other scripts. Most of these are based on ASCII, meaning that 65 still refers to 'A' in e.g. the Hebrew encoding. However, character 228 could refer to greek δ, cyrillic ф, or hebrew ה. Things get even more complex if you consider Chinese, where you can't fit all characters in 256 numbers, so several larger (multi-byte) encodings were used.  \n\nThis can cause a lot of confusion if you read a text that was encoding in e.g. greek as if it were encoded in Hebrew. A famous example of this confusion is that Microsoft Exchange used the WingDings font and encoding for rendering symbols in emails, amongst others using character 74 as a smiley. For non-exchange users (who didn't have that font), however, it renders as the ASCII character nr 74: \"J\". So, if you see an email from a microsoft user with a J where you expected a smiley, now you know `:)`. \n\nTo end this confusion, *unicode* was invented, which assigns a unique number (called a code point) to each letter. A is still 65 (or \"\\u41\" in hexadecimal R notataion), but δ is now uniquely \"\\u03B4\", and  ф is uniquely \"\\u0444\". There are over 1M possible unicode characters, of which about 100 thousand have been currently assigned. This gives enough room for Chinese, old Nordic runes, and even Klingon to be encoded. \n\nYou can directly use these in an R string:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"Some Unicode letters: \\u41 \\u03B4 \\u0444\"\n```\n:::\n\n\nNow, to be able to write all 1M characters to string, one would need almost 24 bits per character, tripling the storage and memory needed to handle most text. So, more efficient encodings were invented that would normally take only 8 or 16 bits per character, but can take more bits if needed. So, while the problem of defining characters is solved, unfortunately you still need to know the actual encoding of a text.\nFortunately, UTF-8 (which uses 1 byte for latin characters, but more for non-western text) is emerging as a de facto standard for most texts. This is a compromise which is most efficient for latin alphabeters, but is still able to unambiguously express all languages.\n\nIt is still quite common, however, to encounter text in other encodings, so it can be good to understand what problems you can face and how to deal with them\n\n## Text encoding in R\n\nTo show how this works in R, we can use the charToRaw function to see how a character is encoded in R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharToRaw('A')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 41\n```\n\n\n:::\n:::\n\n\nNote that the output of this function depends on your regional settings (called 'locale'). \nOn most computers, this should produce 41 however, as most encodings are based on ASCII. \n\nFor other alphabets it can be more tricky. The Chinese character \"蘭\" (unicode \"\\u862d\") on my computer is expressed in UTF-8, where it takes 3 bytes:\n\n\n## Dealing with encodings\n\nTo convert between encodings, you can use the iconv function. For example, to express the Chinese character above in GB2312 (Chinese national standard) encoding:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharToRaw(iconv('蘭', to='GB2312'))\n```\n:::\n\n\nThe most common way of dealing with encodings is to ignore the problem and hope it goes away. However, outside the English world this is often not an option. Also, due to general unicode ignorance many people will use the wrong encoding, and you will even see things like double-utf8-encoded text. \n\nThe *sane* way to deal with encodings is to make sure that all text stored inside your program is encoded in a standard encoding, presumably UTF-8. This means that whenever you read text from an external source, you need to convert it to UTF-8 if it isn't already in that form. \n\nThis means that when you use `read_csv` (on text data) or `readtext`, you should ideally *always* specify which encoding the text is encoded in:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreadtext::readtext(\"file.txt\", encoding = \"utf-8\")\nread_csv(\"file.csv\", locale=locale(encoding='utf-8'))\n```\n:::\n\n\nIf you don't know what encoding a text is in, you can try utf-8 and the most common local encodings\n(e.g. latin-1 in many western countries), you can inspect the raw bytes, or you can use the guessEncoding function from readr:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nguess_encoding(\"file.txt\")\n```\n:::\n\n\n# A note on characters and factors\n\nIn R, every vector (column) has a data type, which you can inspect using `class(x)`.\nTwo relevant data types are `character` for actual texts,\nand `factor` for labeled nominal values (groups). \n\nIn most cases, you want you texts to be stored as `character`, but many default functions (like `data.frame` and `read.csv`) store text colunms as `factor` -- while the tidyverse equivalents of `tibble` and `read_csv` use `character` by default. \n\nIn most cases, in R you can convert object types using `as.character`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = data.frame(name=c(\"john\", \"mary\"))\nclass(df$name)\nlibrary(tidyverse)\ntib = tibble(name=c(\"john\", \"mary\"))\nclass(tib$name)\n```\n:::\n\n\nBefore R version 4.0, textual columns in data frames were automatically converted to a `factor`. A `factor` looks like a `character` column, but it is intended for groups or nominal values. Internally it is stored as a number, with a label attached to each possible value. \n\nThis makes storing large amounts of data more efficient (numbers are smaller than texts), but sometimes, you get unexpected results when you apply a text-based function to a factor. For example, normally `as.numeric` can be used to convert strings to numbers, setting non-numeric strings to NA:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.numeric(c(\"22\", \"16\", \"x\"))\n```\n:::\n\n\nHowever, if we have this data encoded as a factor, it gives a completely different result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurvey = data.frame(age=c(\"22\", \"16\", \"1\"), stringsAsFactors = TRUE)\nas.numeric(survey$age)\n```\n:::\n\n\nWhat happened here? \nThe trick is to remember that factors are intended as labeled groups, and internally R stores the group index and has a separate list of labels (by default sorted alphabetically). Thus, \"22\" was the second element in the labels, \"16\" the first, etc. To convert the labels into numbers, convert to character first (which uses the labels) before converting to numeric:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.numeric(as.character(survey$age))\n```\n:::\n\n\nSince R version 4.0, these problems are much less common, but it's good to be aware of the difference between factors and strings. Also, when using `ggplot` it's important to set factor data types and levels correctly for nominal values as they control the ordering and visualization of nominal data. For more information on dealing with factors, see the [forcats package](https://forcats.tidyverse.org/) and [cheat sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/factors.pdf)\n\n",
    "supporting": [
      "6_strings_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}