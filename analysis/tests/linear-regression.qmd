
---
title: Simple linear regression
subtitle: "Predicting one variable from another"
---

# What is regression analysis?

In the tutorial on [correlation](../tests/correlation.qmd) we learned how to test if there is a relationship between two numeric variables.
We saw that the correlation does not have a direction, and that it only tells us if there is a relationship between the variables, and how strong it is.
With simple linear regression we can take this one step further, and try to *predict* the value of one variable based on the value of another variable.

In this context, the term *predict* is used in a statistical sense.
It refers to estimating how well one can infer the value of one variable given knowledge of another. 
The goal often is not to generate accurate predictions, but to make inferences about the relationship between the variables.
If one variable can predict another, it may for instance suggest a causal relationship.

For example, imagine that you want to be able to predict for a random person on the street how much trust they have in journalists.
What information would you use to make this prediction?
You could perhaps use their age, their level of education, and their political orientation.
Regression analysis allows you to find a mathematical formula that predicts trust based on a combination of these variables.
This can help you understand complex relationships between multiple variables.

We refer to the variable that we want to predict (trust) as the **dependent variable**, and the variables that we use to make the prediction (age) as the **independent variables**.
In the current tutorial you will first learn about simple linear regression, where we use one independent variable to predict the dependent variable.
As you progress, you will learn about [multiple regression](../tests/multiple-regression.qmd), where we use more than one independent variable at once.


# How to use it

For this tutorial we'll be using our standard practice data.
Regression is a flexible tool, allowing the indepent variable to either be numeric (like in a correlation), binary (like in a t-test), or categorical (like in an ANOVA).
So we'll prepare a bunch of columns to show your an example for each of these cases.

Regression analysis is built into R, but we'll also be using the `tidyverse` package to load and clean the data, and the `sjPlot` package to show the results in a nice table.

```{r, message=F, results='hide'}
library(tidyverse)
library(sjPlot)

d <- read_csv("https://tinyurl.com/R-practice-data") |>
        mutate(age = if_else(age > 100, 2024 - age, age)) |>
        select(age, experiment_group, np_subscription, trust_t1, trust_t2)

d
```


## Regression with numerical independent variable

Let's start with the example of predicting `trust` in journalists based on `age`.
For this we'll be using the `trust_t1`[^1] variable as the dependent variable, and the `age` variable as the independent variable.
We'll use the `lm()` function to run the regression, and the `tab_model()` function from the `sjPlot` package to show the results in a nice table.
Inside the `lm()` function we specify the formula for the regression, which has the format `dependent ~ independent` (just like in the t-test and ANOVA).

[^1]: We use the `trust_t1` variable here, because for the current analysis we are not interested in the effect of the experimental group. Since `trust_t1` is measured before the experiment, it is not influenced by the experiment.

```{r}
m <- lm(trust_t1 ~ age, data = d) 

tab_model(m)
```

The output of the regression analysis gives us values for two **predictors**: the **(intercept)** and the **age** variable.
The **(intercept)** tells us what the average value of the dependent variable (`trust_t1`) is when the independent variable (`age`) is zero.
The **age** predictor tells us how much the dependent variable (`trust_t1`) changes for every unit increase in age. 




For each of these predictors we **estimate**, a **confidence interval**, and a **p-value**.
So why do we get two predictors, when we only specified one variable?
 

This is the information that we need to make a prediction. 
To help you understand what this means, it helps to visualize the *regression line*. 
For this we can use the `plot_model()` function.[^2]

[^2]: The `plot_model()` function is part of the `sjPlot` package, and is a very useful tool for visualizing regression models. When use use `type = "pred"` it visualizes the `pred`iction. Here we also say `show.data = T` to show the actual data points. The `jitter = T` argument randomly moves the data points around a tiny bit, so that if two observations have exactly the same values, they will not overlap. 

```{r}
plot_model(m, type = "pred", show.data = T, jitter=T)
```






The `estimate` for the `age` predictor tells us how much the dependent variable (`trust_t1`) changes for every unit increase in the independent variable (`age`).
The coefficient that is reported under `estimate` is `0.04`, which means that for every year older a person is, their trust in journalists increases by 0.04 points.




Together, the intercept and slope create a regression line that best fits the data.
You can think of this line as the prediction.

```{r}
plot_model(m, type = "pred", show.data = T, jitter=T) 
```

For every value of `age`, it tells us what the average value of `trust_t1` is.
So around the age of 20, the average trust is around 3, whereas for 50 year olds its around 4.2.


By *best fits* we mean that the line minimizes the distance between the predicted values (the line) and the actual values.


To get a better feel for the intercept and slope, we can plot the regression line on a scatterplot of the data.

