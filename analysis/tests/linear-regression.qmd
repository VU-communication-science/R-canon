
---
title: Simple linear regression
subtitle: "Predicting one variable from another"
---

# What is regression analysis?

In the tutorial on [correlation](../tests/correlation.qmd) we learned how to test if there is a relationship between two numeric variables.
We saw that the correlation does not have a direction, and that it only tells us if there is a relationship between the variables, and how strong it is.
With simple linear regression we can take this one step further, and try to *predict* the value of one variable based on the value of another variable.

In this context, the term *predict* is used in a statistical sense.
It refers to estimating how well one can infer the value of one variable given knowledge of another. 
We are often not actually interested in the prediction itself!
Instead, we are interested in the relationship between the variables, and what this relationship can tell us about the world. 
If one variable can predict another, it may for instance suggest a causal relationship.

For example, imagine that you want to be able to predict for a random person how much trust they have in journalists.
What information would you use to make this prediction?
You could perhaps use their age and political orientation, as there is some data that suggests that older people and people that lean to the political left have more trust in journalists.

Regression analysis allows you to find a mathematical formula that predicts trust based on a combination of these variables.
This can help you understand complex relationships between two or more variables.

<!-- We refer to the variable that we want to predict (trust) as the **dependent variable**, and the variables that we use to make the prediction (age) as the **independent variables**. -->
<!-- In the current tutorial you will first learn about simple linear regression, where we use one independent variable to predict the dependent variable.
As you progress, you will learn about [multiple regression](../tests/multiple-regression.qmd), where we use more than one independent variable at once. -->


# How to use it

For this tutorial we'll be using our standard practice data, and a bit of `tidyverse` to load and clean the data.
We'll also be using the `sjPlot` package, which can create nice tables and plots of regression models.

```{r, message=F, results='hide'}
library(tidyverse)
library(sjPlot)

d <- read_csv("https://tinyurl.com/R-practice-data") |>
        mutate(age = if_else(age > 100, 2024 - age, age)) |>
        select(age, political_orientation, trust_t1)

d
```

The **dependent** variable in the regression model has to be numeric.
For our examples we'll use the `trust_t1` variable, which measures a person's trust in journalists on a scale from 1 to 10. 
A regression model can have **one or multiple independent** variables, which can be numeric or categorical.
This makes regression analysis a very flexible tool that can be used in many different situations.
In this example we'll demonstrate this with both a numeric (`age`) and a categorical (`political_orientation`) independent variable.

## Regression with numerical independent variable

Let's start with the example of predicting `trust` in journalists based on `age`.
For this we'll be using the `trust_t1`[^1] variable as the dependent variable, and the `age` variable as the independent variable.
We'll use the `lm()` function to run the regression, and the `tab_model()` function from the `sjPlot` package to show the results in a nice table.
Inside the `lm()` function we specify the formula for the regression, which has the format `dependent ~ independent` (just like in the t-test and ANOVA).

[^1]: We use the `trust_t1` variable here, because for the current analysis we are not interested in the effect of the experimental group. Since `trust_t1` is measured before the experiment, it is not influenced by the experiment.

```{r}
#| html-table-processing: none
m <- lm(trust_t1 ~ age, data = d) 

tab_model(m)
```

The output of the regression analysis gives us values for two **predictors**: the **(intercept)** and the **age** variable.
The **(intercept)** tells us what the average value of the dependent variable (`trust_t1`) is when the independent variable (`age`) is zero.
The **age** predictor tells us how much the dependent variable (`trust_t1`) changes for every unit increase in age. 
For each of these predictors we **estimate**, a **confidence interval**, and a **p-value**.

This could already be all the information we need!
If our hypothesis was that trust in journalists increases with age, we could now report that this hypothesis is supported by the data (b = 0.04, p < 0.001).
But let's not stop there, and see how we can interpret these results in more detail!

### Interpreting the intercept and the effect of age 

So why do we get two predictors, when we only specified one variable?
Why do we need the intercept?
This is because our prediction of `trust_t1` is based on two pieces of information:

* What is the average trust in journalists regardless of age?
* How does trust in journalists change as people get older?

The **intercept** (2.21) tells us the average value of the dependent variable when age is zero.[^2]
The coefficient for the **age** variable (0.04) tells us how much this trust increases for every year of age.
Together, these values allow us to predict the trust in journalists for any age, using the following formula:

[^2]: Often, the intercept doesn't have a meaningful interpretation, because the independent variable can't be zero. For example, there are no people with age zero in our data. We therefore often do not interpret the intercept by itself, but we still need it to make the model work.

$$ trust\_t1 = intercept + slope_{age} \times age $$

If we plug in the values from our model, we get: 

$$ trust\_t1 = 2.21 + 0.04 \times age $$

We can for instance calculate the predicted trust in journalists for a 20 year old:

```{r}
2.21 + 0.04 * 20
```


### Visualizing the intercept and slope

We can also visualize how the predicted trust in journalists changes with age.
The `sjPlot` package has a function `plot_model()` that can do this for us.
Here we visualize the prediction (`type = "pred"`) for the `age` variable (`terms = "age"`). 
To show you how the prediction relates to the actual data, we'll also include a scatterplot of the data points (`show.data = T`), and add a bit of jitter[^3] to the data points so they don't overlap (`jitter = T`)

[^3]: Jitter is a visualization technique that randomly moves the data points a bit, so that they don't overlap. If we don't do this, then data points with identical values are plotted on top of each other, and we can't see how many data points there are.

```{r}
plot_model(m, type = "pred", terms = "age", show.data = T, jitter=T)
```

Here the *regression line* shows the predicted values. 
You can check that at age 20 the predicted trust is indeed 3.01, as we calculated before. 
We can now see very clearly how average trust in journalists increases with age!

This should also clarify why the effect of age is called a **slope**.
Since we have a positive effect, the regression line slopes upwards as age increases.
The stronger the effect, the steeper the slope.
If there was no effect of age, the regression line would be flat (i.e. no slope), because the predicted value would not change with age. 
If the effect was negative, the regression line would slope downwards.
Note that this is identical to the **direction** and **strength** of [covariance and correlation measures](../concepts/covariance-and-correlation.qmd#visualizing-correlation).


::: {.callout-note title="How intercept and slope make the regression line" collapse="true"}

The **intercept** is the value of the dependent variable when the independent variable is zero, which determines at what value the regression line crosses the y-axis.
The coefficient for an independent variable tells us how much the dependent variable changes for every unit increase in the independent variable, which determines the **slope** of the regression line.
The regression model finds the values for the intercept and slope that draws a line through the data points that best fits the data.
To get a feel for this, try changing the values of the intercept and slope in the interactive widget below, and see how the regression line changes.

```{ojs}
//| echo: false
viewof intercept = Inputs.range(
  [-10, 10], 
  {value: -5, step: 0.01, label: "Intercept"}
)
viewof slope = Inputs.range(
  [-2,2], 
  {value: 0.1, step: 0.01, label: "Slope"}
)
```

```{ojs}
//| echo: false

x_values = Array.from({ length: 100 }, (_, i) => i);
y_values = x_values.map(x => slope * x + intercept);

// Now, plot the regression line
Plot.plot({
  marks: [
    Plot.lineY(y_values, {x: x_values}) // Explicitly pass x and y values
  ],
  x: {
    label: "Independent variable (age)",
    domain: [0, 100]
  },
  y: {
    label: "Dependent variable (trust)",
    domain: [-10, 10]
  },
  width: 600,
  height: 400
})

```
:::

## Regression with categorical independent variable

The **independent variable** in a regression model can also be categorical.
This allows us to include categorical variables in the model, similar to the t-test and ANOVA.
Let's see what this looks like when we predict `trust_t1` based on the `np_subscription` variable, which has the values `yes` and `no` to indicate if a person has a newspaper subscription.

We cannot do calculations with the values "yes" and "no", but we can make this possible by creating a so-called **dummy variable**.
In a dummy variable, we assign a value of `1` to one of the categories, and `0` to the other categories.
In our case we'll assign `1` to the category `yes`, and `0` to the category `no`.
R can actually do this for us, but we'll first show you how to do it yourself to understand what's happening.
Here we create a new variable `has_subscription` that has the value `1` if the person has a subscription, and `0` if they don't.

```{r}
d = mutate(d, has_subscription = if_else(np_subscription == "yes", 1, 0))

select(d, np_subscription, has_subscription)
```

Now we can use this variable in the regression model.

```{r}
m <- lm(trust_t1 ~ has_subscription, data = d)

tab_model(m)
```

We now get a **slope** for the `has_subscription` variable.
Before we saw that the slope tells us how much the dependent variable changes for every unit increase in the independent variable.
This is still true!
We only need to remember that the `has_subscription` variable is binary, so it can only take the values `0` and `1`.
So given a coefficient of `0.37`, we can say that people with a subscription have on average a trust score that is `0.37` points higher than people without a subscription.

Now let's do this again, but this time we'll let R create the dummy variable.
This time we'll just directly use the `np_subscription` variable in the regression model.

```{r}
m <- lm(trust_t1 ~ np_subscription, data = d)

tab_model(m)
```

Notice that the results are identical! 
The only difference is that the label for the `np_subscription` variable is `np_subscription [yes]`.
This is R's way of telling us that it created a dummy variable for us in which the value `1` corresponds to the category `yes`.

An additional benefit of letting R create the dummy variable is that it now knows that the variable is categorical. 
If we now use the `plot_model()` function to visualize the prediction, it will 

```{r}
plot_model(m, type = "pred", terms = "np_subscription")
```


Just like before, we get an **intercept** and a **slope**.
The interpretation is mostly the same, but we need to keep in mind that the np_subscription variable is binary.

Notice that the label for the `np_subscription` variable is `np_subscription [yes]`.
This tells us that R 